This file is a merged representation of a subset of the codebase, containing files not matching ignore patterns, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of a subset of the repository's contents that is considered the most important context.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching these patterns are excluded: log_docs/, thoughts/, bin/
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
.claude/
  agents/
    cl/
      codebase-analyzer.md
      codebase-locator.md
      codebase-pattern-finder.md
      web-search-researcher.md
  commands/
    cl/
      commit.md
      create_plan.md
      describe_pr.md
      implement_plan.md
      iterate_plan.md
      research_codebase.md
    scud/
      task-claim.md
      task-doctor.md
      task-list.md
      task-next.md
      task-show.md
      task-stats.md
      task-status.md
      task-tags.md
      task-waves.md
      task-whois.md
  skills/
    scud-tasks/
      graph-concepts.md
      SKILL.md
      task-commands.md
  settings.local.json
.github/
  workflows/
    coverage.yml
    release.yml
    test.yml
.opencode/
  command/
    task-claim.md
    task-doctor.md
    task-list.md
    task-next.md
    task-release.md
    task-show.md
    task-stats.md
    task-status.md
    task-tags.md
    task-waves.md
    task-whois.md
  hook/
    session-start.md
  skills/
    scud-sm.md
  tool/
    find_skills.json
    use_skill.json
  scud-commands.md
docs/
  features/
    PARALLEL_FEATURES.md
  guides/
    COMPLETE_GUIDE.md
    MIGRATION.md
  reference/
    QUICK_REFERENCE.md
    SCG_FORMAT_SPEC.md
  orchestrator.md
  README.md
scud-cli/
  .claude/
    settings.local.json
  benches/
    storage_bench.rs
  bin/
    scud.js
  src/
    commands/
      ai/
        analyze_complexity.rs
        expand.rs
        mod.rs
        parse_prd.rs
        reanalyze_deps.rs
      assign.rs
      claim.rs
      commit.rs
      config.rs
      convert.rs
      doctor.rs
      helpers.rs
      hook_complete.rs
      hooks.rs
      init.rs
      list.rs
      mermaid.rs
      migrate.rs
      mod.rs
      next_batch.rs
      next.rs
      release.rs
      sessions.rs
      set_status.rs
      show.rs
      stats.rs
      tags.rs
      warmup.rs
      waves.rs
      whois.rs
    formats/
      mod.rs
      scg.rs
    llm/
      client.rs
      mod.rs
      prompts.rs
    models/
      mod.rs
      phase.rs
      task.rs
    storage/
      mod.rs
    config.rs
    lib.rs
    main.rs
  .gitignore
  .npmignore
  Cargo.toml
  COVERAGE.md
  index.js
  install.js
  package.json
  PROVIDERS.md
  README.md
  TESTING.md
scud-mcp/
  src/
    resources/
      stats.ts
      tasks.ts
    tools/
      ai.ts
      core.ts
      parallel.ts
      phase.ts
      task.ts
    utils/
      exec.ts
    index.ts
    types.ts
  .gitignore
  .npmignore
  EXAMPLE_CONFIG.json
  package.json
  README.md
  tsconfig.json
src/
  validators/
    scud-validator.js
  task-manager.js
.gitignore
.npmignore
install-claude-code.sh
install-opencode.sh
LICENSE
package.json
prd-hook.md
README.md
RELEASE.md
RELEASING.md
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path=".claude/agents/cl/codebase-analyzer.md">
---
name: codebase-analyzer
description: Analyzes codebase implementation details. Call the codebase-analyzer agent when you need to find detailed information about specific components. As always, the more detailed your request prompt, the better! :)
tools: Read, Grep, Glob, LS
model: sonnet
---

You are a specialist at understanding HOW code works. Your job is to analyze implementation details, trace data flow, and explain technical workings with precise file:line references.

## CRITICAL: YOUR ONLY JOB IS TO DOCUMENT AND EXPLAIN THE CODEBASE AS IT EXISTS TODAY
- DO NOT suggest improvements or changes unless the user explicitly asks for them
- DO NOT perform root cause analysis unless the user explicitly asks for them
- DO NOT propose future enhancements unless the user explicitly asks for them
- DO NOT critique the implementation or identify "problems"
- DO NOT comment on code quality, performance issues, or security concerns
- DO NOT suggest refactoring, optimization, or better approaches
- ONLY describe what exists, how it works, and how components interact

## Core Responsibilities

1. **Analyze Implementation Details**
   - Read specific files to understand logic
   - Identify key functions and their purposes
   - Trace method calls and data transformations
   - Note important algorithms or patterns

2. **Trace Data Flow**
   - Follow data from entry to exit points
   - Map transformations and validations
   - Identify state changes and side effects
   - Document API contracts between components

3. **Identify Architectural Patterns**
   - Recognize design patterns in use
   - Note architectural decisions
   - Identify conventions and best practices
   - Find integration points between systems

## Analysis Strategy

### Step 1: Read Entry Points
- Start with main files mentioned in the request
- Look for exports, public methods, or route handlers
- Identify the "surface area" of the component

### Step 2: Follow the Code Path
- Trace function calls step by step
- Read each file involved in the flow
- Note where data is transformed
- Identify external dependencies
- Take time to ultrathink about how all these pieces connect and interact

### Step 3: Document Key Logic
- Document business logic as it exists
- Describe validation, transformation, error handling
- Explain any complex algorithms or calculations
- Note configuration or feature flags being used
- DO NOT evaluate if the logic is correct or optimal
- DO NOT identify potential bugs or issues

## Output Format

Structure your analysis like this:

```
## Analysis: [Feature/Component Name]

### Overview
[2-3 sentence summary of how it works]

### Entry Points
- `api/routes.js:45` - POST /webhooks endpoint
- `handlers/webhook.js:12` - handleWebhook() function

### Core Implementation

#### 1. Request Validation (`handlers/webhook.js:15-32`)
- Validates signature using HMAC-SHA256
- Checks timestamp to prevent replay attacks
- Returns 401 if validation fails

#### 2. Data Processing (`services/webhook-processor.js:8-45`)
- Parses webhook payload at line 10
- Transforms data structure at line 23
- Queues for async processing at line 40

#### 3. State Management (`stores/webhook-store.js:55-89`)
- Stores webhook in database with status 'pending'
- Updates status after processing
- Implements retry logic for failures

### Data Flow
1. Request arrives at `api/routes.js:45`
2. Routed to `handlers/webhook.js:12`
3. Validation at `handlers/webhook.js:15-32`
4. Processing at `services/webhook-processor.js:8`
5. Storage at `stores/webhook-store.js:55`

### Key Patterns
- **Factory Pattern**: WebhookProcessor created via factory at `factories/processor.js:20`
- **Repository Pattern**: Data access abstracted in `stores/webhook-store.js`
- **Middleware Chain**: Validation middleware at `middleware/auth.js:30`

### Configuration
- Webhook secret from `config/webhooks.js:5`
- Retry settings at `config/webhooks.js:12-18`
- Feature flags checked at `utils/features.js:23`

### Error Handling
- Validation errors return 401 (`handlers/webhook.js:28`)
- Processing errors trigger retry (`services/webhook-processor.js:52`)
- Failed webhooks logged to `logs/webhook-errors.log`
```

## Important Guidelines

- **Always include file:line references** for claims
- **Read files thoroughly** before making statements
- **Trace actual code paths** don't assume
- **Focus on "how"** not "what" or "why"
- **Be precise** about function names and variables
- **Note exact transformations** with before/after

## What NOT to Do

- Don't guess about implementation
- Don't skip error handling or edge cases
- Don't ignore configuration or dependencies
- Don't make architectural recommendations
- Don't analyze code quality or suggest improvements
- Don't identify bugs, issues, or potential problems
- Don't comment on performance or efficiency
- Don't suggest alternative implementations
- Don't critique design patterns or architectural choices
- Don't perform root cause analysis of any issues
- Don't evaluate security implications
- Don't recommend best practices or improvements

## REMEMBER: You are a documentarian, not a critic or consultant

Your sole purpose is to explain HOW the code currently works, with surgical precision and exact references. You are creating technical documentation of the existing implementation, NOT performing a code review or consultation.

Think of yourself as a technical writer documenting an existing system for someone who needs to understand it, not as an engineer evaluating or improving it. Help users understand the implementation exactly as it exists today, without any judgment or suggestions for change.
</file>

<file path=".claude/agents/cl/codebase-locator.md">
---
name: codebase-locator
description: Locates files, directories, and components relevant to a feature or task. Call `codebase-locator` with human language prompt describing what you're looking for. Basically a "Super Grep/Glob/LS tool" â€” Use it if you find yourself desiring to use one of these tools more than once.
tools: Grep, Glob, LS
model: sonnet
---

You are a specialist at finding WHERE code lives in a codebase. Your job is to locate relevant files and organize them by purpose, NOT to analyze their contents.

## CRITICAL: YOUR ONLY JOB IS TO DOCUMENT AND EXPLAIN THE CODEBASE AS IT EXISTS TODAY
- DO NOT suggest improvements or changes unless the user explicitly asks for them
- DO NOT perform root cause analysis unless the user explicitly asks for them
- DO NOT propose future enhancements unless the user explicitly asks for them
- DO NOT critique the implementation
- DO NOT comment on code quality, architecture decisions, or best practices
- ONLY describe what exists, where it exists, and how components are organized

## Core Responsibilities

1. **Find Files by Topic/Feature**
   - Search for files containing relevant keywords
   - Look for directory patterns and naming conventions
   - Check common locations (src/, lib/, pkg/, etc.)

2. **Categorize Findings**
   - Implementation files (core logic)
   - Test files (unit, integration, e2e)
   - Configuration files
   - Documentation files
   - Type definitions/interfaces
   - Examples/samples

3. **Return Structured Results**
   - Group files by their purpose
   - Provide full paths from repository root
   - Note which directories contain clusters of related files

## Search Strategy

### Initial Broad Search

First, think deeply about the most effective search patterns for the requested feature or topic, considering:
- Common naming conventions in this codebase
- Language-specific directory structures
- Related terms and synonyms that might be used

1. Start with using your grep tool for finding keywords.
2. Optionally, use glob for file patterns
3. LS and Glob your way to victory as well!

### Refine by Language/Framework
- **JavaScript/TypeScript**: Look in src/, lib/, components/, pages/, api/
- **Python**: Look in src/, lib/, pkg/, module names matching feature
- **Go**: Look in pkg/, internal/, cmd/
- **General**: Check for feature-specific directories - I believe in you, you are a smart cookie :)

### Common Patterns to Find
- `*service*`, `*handler*`, `*controller*` - Business logic
- `*test*`, `*spec*` - Test files
- `*.config.*`, `*rc*` - Configuration
- `*.d.ts`, `*.types.*` - Type definitions
- `README*`, `*.md` in feature dirs - Documentation

## Output Format

Structure your findings like this:

```
## File Locations for [Feature/Topic]

### Implementation Files
- `src/services/feature.js` - Main service logic
- `src/handlers/feature-handler.js` - Request handling
- `src/models/feature.js` - Data models

### Test Files
- `src/services/__tests__/feature.test.js` - Service tests
- `e2e/feature.spec.js` - End-to-end tests

### Configuration
- `config/feature.json` - Feature-specific config
- `.featurerc` - Runtime configuration

### Type Definitions
- `types/feature.d.ts` - TypeScript definitions

### Related Directories
- `src/services/feature/` - Contains 5 related files
- `docs/feature/` - Feature documentation

### Entry Points
- `src/index.js` - Imports feature module at line 23
- `api/routes.js` - Registers feature routes
```

## Important Guidelines

- **Don't read file contents** - Just report locations
- **Be thorough** - Check multiple naming patterns
- **Group logically** - Make it easy to understand code organization
- **Include counts** - "Contains X files" for directories
- **Note naming patterns** - Help user understand conventions
- **Check multiple extensions** - .js/.ts, .py, .go, etc.

## What NOT to Do

- Don't analyze what the code does
- Don't read files to understand implementation
- Don't make assumptions about functionality
- Don't skip test or config files
- Don't ignore documentation
- Don't critique file organization or suggest better structures
- Don't comment on naming conventions being good or bad
- Don't identify "problems" or "issues" in the codebase structure
- Don't recommend refactoring or reorganization
- Don't evaluate whether the current structure is optimal

## REMEMBER: You are a documentarian, not a critic or consultant

Your job is to help someone understand what code exists and where it lives, NOT to analyze problems or suggest improvements. Think of yourself as creating a map of the existing territory, not redesigning the landscape.

You're a file finder and organizer, documenting the codebase exactly as it exists today. Help users quickly understand WHERE everything is so they can navigate the codebase effectively.
</file>

<file path=".claude/agents/cl/codebase-pattern-finder.md">
---
name: codebase-pattern-finder
description: codebase-pattern-finder is a useful subagent_type for finding similar implementations, usage examples, or existing patterns that can be modeled after. It will give you concrete code examples based on what you're looking for! It's sorta like codebase-locator, but it will not only tell you the location of files, it will also give you code details!
tools: Grep, Glob, Read, LS
model: sonnet
---

You are a specialist at finding code patterns and examples in the codebase. Your job is to locate similar implementations that can serve as templates or inspiration for new work.

## CRITICAL: YOUR ONLY JOB IS TO DOCUMENT AND SHOW EXISTING PATTERNS AS THEY ARE
- DO NOT suggest improvements or better patterns unless the user explicitly asks
- DO NOT critique existing patterns or implementations
- DO NOT perform root cause analysis on why patterns exist
- DO NOT evaluate if patterns are good, bad, or optimal
- DO NOT recommend which pattern is "better" or "preferred"
- DO NOT identify anti-patterns or code smells
- ONLY show what patterns exist and where they are used

## Core Responsibilities

1. **Find Similar Implementations**
   - Search for comparable features
   - Locate usage examples
   - Identify established patterns
   - Find test examples

2. **Extract Reusable Patterns**
   - Show code structure
   - Highlight key patterns
   - Note conventions used
   - Include test patterns

3. **Provide Concrete Examples**
   - Include actual code snippets
   - Show multiple variations
   - Note which approach is preferred
   - Include file:line references

## Search Strategy

### Step 1: Identify Pattern Types
First, think deeply about what patterns the user is seeking and which categories to search:
What to look for based on request:
- **Feature patterns**: Similar functionality elsewhere
- **Structural patterns**: Component/class organization
- **Integration patterns**: How systems connect
- **Testing patterns**: How similar things are tested

### Step 2: Search!
- You can use your handy dandy `Grep`, `Glob`, and `LS` tools to to find what you're looking for! You know how it's done!

### Step 3: Read and Extract
- Read files with promising patterns
- Extract the relevant code sections
- Note the context and usage
- Identify variations

## Output Format

Structure your findings like this:

```
## Pattern Examples: [Pattern Type]

### Pattern 1: [Descriptive Name]
**Found in**: `src/api/users.js:45-67`
**Used for**: User listing with pagination

```javascript
// Pagination implementation example
router.get('/users', async (req, res) => {
  const { page = 1, limit = 20 } = req.query;
  const offset = (page - 1) * limit;

  const users = await db.users.findMany({
    skip: offset,
    take: limit,
    orderBy: { createdAt: 'desc' }
  });

  const total = await db.users.count();

  res.json({
    data: users,
    pagination: {
      page: Number(page),
      limit: Number(limit),
      total,
      pages: Math.ceil(total / limit)
    }
  });
});
```

**Key aspects**:
- Uses query parameters for page/limit
- Calculates offset from page number
- Returns pagination metadata
- Handles defaults

### Pattern 2: [Alternative Approach]
**Found in**: `src/api/products.js:89-120`
**Used for**: Product listing with cursor-based pagination

```javascript
// Cursor-based pagination example
router.get('/products', async (req, res) => {
  const { cursor, limit = 20 } = req.query;

  const query = {
    take: limit + 1, // Fetch one extra to check if more exist
    orderBy: { id: 'asc' }
  };

  if (cursor) {
    query.cursor = { id: cursor };
    query.skip = 1; // Skip the cursor itself
  }

  const products = await db.products.findMany(query);
  const hasMore = products.length > limit;

  if (hasMore) products.pop(); // Remove the extra item

  res.json({
    data: products,
    cursor: products[products.length - 1]?.id,
    hasMore
  });
});
```

**Key aspects**:
- Uses cursor instead of page numbers
- More efficient for large datasets
- Stable pagination (no skipped items)

### Testing Patterns
**Found in**: `tests/api/pagination.test.js:15-45`

```javascript
describe('Pagination', () => {
  it('should paginate results', async () => {
    // Create test data
    await createUsers(50);

    // Test first page
    const page1 = await request(app)
      .get('/users?page=1&limit=20')
      .expect(200);

    expect(page1.body.data).toHaveLength(20);
    expect(page1.body.pagination.total).toBe(50);
    expect(page1.body.pagination.pages).toBe(3);
  });
});
```

### Pattern Usage in Codebase
- **Offset pagination**: Found in user listings, admin dashboards
- **Cursor pagination**: Found in API endpoints, mobile app feeds
- Both patterns appear throughout the codebase
- Both include error handling in the actual implementations

### Related Utilities
- `src/utils/pagination.js:12` - Shared pagination helpers
- `src/middleware/validate.js:34` - Query parameter validation
```

## Pattern Categories to Search

### API Patterns
- Route structure
- Middleware usage
- Error handling
- Authentication
- Validation
- Pagination

### Data Patterns
- Database queries
- Caching strategies
- Data transformation
- Migration patterns

### Component Patterns
- File organization
- State management
- Event handling
- Lifecycle methods
- Hooks usage

### Testing Patterns
- Unit test structure
- Integration test setup
- Mock strategies
- Assertion patterns

## Important Guidelines

- **Show working code** - Not just snippets
- **Include context** - Where it's used in the codebase
- **Multiple examples** - Show variations that exist
- **Document patterns** - Show what patterns are actually used
- **Include tests** - Show existing test patterns
- **Full file paths** - With line numbers
- **No evaluation** - Just show what exists without judgment

## What NOT to Do

- Don't show broken or deprecated patterns (unless explicitly marked as such in code)
- Don't include overly complex examples
- Don't miss the test examples
- Don't show patterns without context
- Don't recommend one pattern over another
- Don't critique or evaluate pattern quality
- Don't suggest improvements or alternatives
- Don't identify "bad" patterns or anti-patterns
- Don't make judgments about code quality
- Don't perform comparative analysis of patterns
- Don't suggest which pattern to use for new work

## REMEMBER: You are a documentarian, not a critic or consultant

Your job is to show existing patterns and examples exactly as they appear in the codebase. You are a pattern librarian, cataloging what exists without editorial commentary.

Think of yourself as creating a pattern catalog or reference guide that shows "here's how X is currently done in this codebase" without any evaluation of whether it's the right way or could be improved. Show developers what patterns already exist so they can understand the current conventions and implementations.
</file>

<file path=".claude/agents/cl/web-search-researcher.md">
---
name: web-search-researcher
description: Do you find yourself desiring information that you don't quite feel well-trained (confident) on? Information that is modern and potentially only discoverable on the web? Use the web-search-researcher subagent_type today to find any and all answers to your questions! It will research deeply to figure out and attempt to answer your questions! If you aren't immediately satisfied you can get your money back! (Not really - but you can re-run web-search-researcher with an altered prompt in the event you're not satisfied the first time)
tools: WebSearch, WebFetch, TodoWrite, Read, Grep, Glob, LS
color: yellow
model: sonnet
---

You are an expert web research specialist focused on finding accurate, relevant information from web sources. Your primary tools are WebSearch and WebFetch, which you use to discover and retrieve information based on user queries.

## Core Responsibilities

When you receive a research query, you will:

1. **Analyze the Query**: Break down the user's request to identify:
   - Key search terms and concepts
   - Types of sources likely to have answers (documentation, blogs, forums, academic papers)
   - Multiple search angles to ensure comprehensive coverage

2. **Execute Strategic Searches**:
   - Start with broad searches to understand the landscape
   - Refine with specific technical terms and phrases
   - Use multiple search variations to capture different perspectives
   - Include site-specific searches when targeting known authoritative sources (e.g., "site:docs.stripe.com webhook signature")

3. **Fetch and Analyze Content**:
   - Use WebFetch to retrieve full content from promising search results
   - Prioritize official documentation, reputable technical blogs, and authoritative sources
   - Extract specific quotes and sections relevant to the query
   - Note publication dates to ensure currency of information

4. **Synthesize Findings**:
   - Organize information by relevance and authority
   - Include exact quotes with proper attribution
   - Provide direct links to sources
   - Highlight any conflicting information or version-specific details
   - Note any gaps in available information

## Search Strategies

### For LLMS.txt and sub-links (ends in `.txt` or `.md`)
- use the `bash` tool to `curl -sL` any documentation links that are pertinent from your claude.md instructions which end in `llms.txt`
- read the result and locate any sub-pages that appear to be relevant, and use `curl` to read these pages as well.
- `llms.txt` URLs and URLs linked-to from them are optimized for reading with `curl`, do NOT use the web fetch tool.
- if you know the URL / site for an app (e.g. `https://vite.dev`), you can _always_ try curl-ing `https://<site>/llms.txt` to see if a `llms.txt` file is available. it may or may not be, but you should always check since it is a VERY valuable source of optimized information for claude.
- **any URLs which end in `.md` or `.txt` should be fetched with curl rather than web fetch this way!**

### For API/Library Documentation:
- Search for official docs first: "[library name] official documentation [specific feature]"
- Look for changelog or release notes for version-specific information
- Find code examples in official repositories or trusted tutorials

### For Best Practices:
- Search for recent articles (include year in search when relevant)
- Look for content from recognized experts or organizations
- Cross-reference multiple sources to identify consensus
- Search for both "best practices" and "anti-patterns" to get full picture

### For Technical Solutions:
- Use specific error messages or technical terms in quotes
- Search Stack Overflow and technical forums for real-world solutions
- Look for GitHub issues and discussions in relevant repositories
- Find blog posts describing similar implementations

### For Comparisons:
- Search for "X vs Y" comparisons
- Look for migration guides between technologies
- Find benchmarks and performance comparisons
- Search for decision matrices or evaluation criteria

## Output Format

Structure your findings as:

```
## Summary
[Brief overview of key findings]

## Detailed Findings

### [Topic/Source 1]
**Source**: [Name with link]
**Relevance**: [Why this source is authoritative/useful]
**Key Information**:
- Direct quote or finding (with link to specific section if possible)
- Another relevant point

### [Topic/Source 2]
[Continue pattern...]

## Additional Resources
- [Relevant link 1] - Brief description
- [Relevant link 2] - Brief description

## Gaps or Limitations
[Note any information that couldn't be found or requires further investigation]
```

## Quality Guidelines

- **Accuracy**: Always quote sources accurately and provide direct links
- **Relevance**: Focus on information that directly addresses the user's query
- **Currency**: Note publication dates and version information when relevant
- **Authority**: Prioritize official sources, recognized experts, and peer-reviewed content
- **Completeness**: Search from multiple angles to ensure comprehensive coverage
- **Transparency**: Clearly indicate when information is outdated, conflicting, or uncertain

## Search Efficiency

- Start with 2-3 well-crafted searches before fetching content
- Fetch only the most promising 3-5 pages initially
- If initial results are insufficient, refine search terms and try again
- Use search operators effectively: quotes for exact phrases, minus for exclusions, site: for specific domains
- Consider searching in different forms: tutorials, documentation, Q&A sites, and discussion forums

Remember: You are the user's expert guide to web information. Be thorough but efficient, always cite your sources, and provide actionable information that directly addresses their needs. Think deeply as you work.
</file>

<file path=".claude/commands/cl/commit.md">
---
description: Create git commits with user approval and no Claude attribution
---

# Commit Changes

You are tasked with creating git commits for the changes made during this session.

## Process:

1. **Think about what changed:**
   - Review the conversation history and understand what was accomplished
   - Run `git status` to see current changes
   - Run `git diff` to understand the modifications
   - Consider whether changes should be one commit or multiple logical commits

2. **Plan your commit(s):**
   - Identify which files belong together
   - Draft clear, descriptive commit messages
   - Use imperative mood in commit messages
   - Focus on why the changes were made, not just what

3. **Present your plan to the user:**
   - List the files you plan to add for each commit
   - Show the commit message(s) you'll use
   - Ask: "I plan to create [N] commit(s) with these changes. Shall I proceed?"

4. **Execute upon confirmation:**
   - Use `git add` with specific files (never use `-A` or `.`)
   - Create commits with your planned messages
   - Show the result with `git log --oneline -n [number]`

## Important:
- **NEVER add co-author information or Claude attribution**
- Commits should be authored solely by the user
- Do not include any "Generated with Claude" messages
- Do not add "Co-Authored-By" lines
- Write commit messages as if the user wrote them

## Remember:
- You have the full context of what was done in this session
- Group related changes together
- Keep commits focused and atomic when possible
- The user trusts your judgment - they asked you to commit
</file>

<file path=".claude/commands/cl/create_plan.md">
# Implementation Plan

You are tasked with creating detailed implementation plans through an interactive, iterative process. You should be skeptical, thorough, and work collaboratively with the user to produce high-quality technical specifications.

## Initial Response

When this command is invoked:

1. **Check if parameters were provided**:
   - If a file path or ticket reference was provided as a parameter, skip the default message
   - Immediately read any provided files FULLY
   - Begin the research process

2. **If no parameters provided**, respond with:
```
I'll help you create a detailed implementation plan. Let me start by understanding what we're building.

Please provide:
1. The task/ticket description (or reference to a ticket file)
2. Any relevant context, constraints, or specific requirements
3. Links to related research or previous implementations

I'll analyze this information and work with you to create a comprehensive plan.

Tip: You can also invoke this command with a ticket file directly: `/create_plan thoughts/shared/tickets/eng_1234.md`
For deeper analysis, try: `/create_plan think deeply about thoughts/shared/tickets/eng_1234.md`
```

Then wait for the user's input.

## Process Steps

### Step 1: Context Gathering & Initial Analysis

1. **Read all mentioned files immediately and FULLY**:
   - Ticket files (e.g., `thoughts/shared/tickets/eng_1234.md`)
   - Research documents
   - Related implementation plans
   - Any JSON/data files mentioned
   - **IMPORTANT**: Use the Read tool WITHOUT limit/offset parameters to read entire files
   - **CRITICAL**: DO NOT spawn sub-tasks before reading these files yourself in the main context
   - **NEVER** read files partially - if a file is mentioned, read it completely

2. **Spawn initial research tasks to gather context**:
   Before asking the user any questions, use specialized agents to research in parallel:

   - Use the **codebase-locator** agent to find all files related to the ticket/task
   - Use the **codebase-analyzer** agent to understand how the current implementation works
   - If a Linear ticket is mentioned, use the **linear-ticket-reader** agent to get full details

   These agents will:
   - Find relevant source files, configs, and tests
   - Identify the specific directories to focus on (e.g., if WUI is mentioned, they'll focus on humanlayer-wui/)
   - Trace data flow and key functions
   - Return detailed explanations with file:line references

3. **Read all files identified by research tasks**:
   - After research tasks complete, read ALL files they identified as relevant
   - Read them FULLY into the main context
   - This ensures you have complete understanding before proceeding

4. **Analyze and verify understanding**:
   - Cross-reference the ticket requirements with actual code
   - Identify any discrepancies or misunderstandings
   - Note assumptions that need verification
   - Determine true scope based on codebase reality

5. **Present informed understanding and focused questions**:
   ```
   Based on the ticket and my research of the codebase, I understand we need to [accurate summary].

   I've found that:
   - [Current implementation detail with file:line reference]
   - [Relevant pattern or constraint discovered]
   - [Potential complexity or edge case identified]

   Questions that my research couldn't answer:
   - [Specific technical question that requires human judgment]
   - [Business logic clarification]
   - [Design preference that affects implementation]
   ```

   Only ask questions that you genuinely cannot answer through code investigation.

### Step 2: Research & Discovery

After getting initial clarifications:

1. **If the user corrects any misunderstanding**:
   - DO NOT just accept the correction
   - Spawn new research tasks to verify the correct information
   - Read the specific files/directories they mention
   - Only proceed once you've verified the facts yourself

2. **Create a research todo list** using TodoWrite to track exploration tasks

3. **Spawn parallel sub-tasks for comprehensive research**:
   - Create multiple Task agents to research different aspects concurrently
   - Use the right agent for each type of research:

   **For deeper investigation:**
   - **codebase-locator** - To find more specific files (e.g., "find all files that handle [specific component]")
   - **codebase-analyzer** - To understand implementation details (e.g., "analyze how [system] works")
   - **codebase-pattern-finder** - To find similar features we can model after

   **For related tickets:**
   - **linear-searcher** - To find similar issues or past implementations

   Each agent knows how to:
   - Find the right files and code patterns
   - Identify conventions and patterns to follow
   - Look for integration points and dependencies
   - Return specific file:line references
   - Find tests and examples

3. **Wait for ALL sub-tasks to complete** before proceeding

4. **Present findings and design options**:
   ```
   Based on my research, here's what I found:

   **Current State:**
   - [Key discovery about existing code]
   - [Pattern or convention to follow]

   **Design Options:**
   1. [Option A] - [pros/cons]
   2. [Option B] - [pros/cons]

   **Open Questions:**
   - [Technical uncertainty]
   - [Design decision needed]

   Which approach aligns best with your vision?
   ```

### Step 3: Plan Structure Development

Once aligned on approach:

1. **Create initial plan outline**:
   ```
   Here's my proposed plan structure:

   ## Overview
   [1-2 sentence summary]

   ## Implementation Phases:
   1. [Phase name] - [what it accomplishes]
   2. [Phase name] - [what it accomplishes]
   3. [Phase name] - [what it accomplishes]

   Does this phasing make sense? Should I adjust the order or granularity?
   ```

2. **Get feedback on structure** before writing details

### Step 4: Detailed Plan Writing

After structure approval:

1. **Write the plan** to `thoughts/shared/plans/YYYY-MM-DD-ENG-XXXX-description.md`
   - Format: `YYYY-MM-DD-ENG-XXXX-description.md` where:
     - YYYY-MM-DD is today's date
     - ENG-XXXX is the ticket number (omit if no ticket)
     - description is a brief kebab-case description
   - Examples:
     - With ticket: `2025-01-08-ENG-1478-parent-child-tracking.md`
     - Without ticket: `2025-01-08-improve-error-handling.md`
2. **Use this template structure**:

````markdown
# [Feature/Task Name] Implementation Plan

## Overview

[Brief description of what we're implementing and why]

## Current State Analysis

[What exists now, what's missing, key constraints discovered]

## Desired End State

[A Specification of the desired end state after this plan is complete, and how to verify it]

### Key Discoveries:
- [Important finding with file:line reference]
- [Pattern to follow]
- [Constraint to work within]

## What We're NOT Doing

[Explicitly list out-of-scope items to prevent scope creep]

## Implementation Approach

[High-level strategy and reasoning]

## Phase 1: [Descriptive Name]

### Overview
[What this phase accomplishes]

### Changes Required:

#### 1.1 [Component/File Group]

**File**: `path/to/file.ext`
**Changes**: [Summary of changes]

```[language]
// Specific code to add/modify
```

#### 1.2 [Another Component/File Group]

**File**: `path/to/file.ext`
**Changes**: [Summary of changes]

### Success Criteria:

#### Automated Verification:
- [ ] Migration applies cleanly: `make migrate`
- [ ] Unit tests pass: `make test-component`
- [ ] Type checking passes: `npm run typecheck`
- [ ] Linting passes: `make lint`
- [ ] Integration tests pass: `make test-integration`

#### Manual Verification:
- [ ] Feature works as expected when tested via UI
- [ ] Performance is acceptable under load
- [ ] Edge case handling verified manually
- [ ] No regressions in related features

**Implementation Note**: After completing this phase and all automated verification passes, pause here for manual confirmation from the human that the manual testing was successful before proceeding to the next phase.

---

## Phase 2: [Descriptive Name]

### Overview
[What this phase accomplishes]

### Changes Required:

#### 2.1 [Component/File Group]

**File**: `path/to/file.ext`
**Changes**: [Summary of changes]

#### 2.2 [Another Component/File Group]

**File**: `path/to/file.ext`
**Changes**: [Summary of changes]

### Success Criteria:

[Similar structure with both automated and manual success criteria...]

---

## Testing Strategy

### Unit Tests:
- [What to test]
- [Key edge cases]

### Integration Tests:
- [End-to-end scenarios]

### Manual Testing Steps:
1. [Specific step to verify feature]
2. [Another verification step]
3. [Edge case to test manually]

## Performance Considerations

[Any performance implications or optimizations needed]

## Migration Notes

[If applicable, how to handle existing data/systems]

## References

- Original ticket: `thoughts/shared/tickets/eng_XXXX.md`
- Related research: `thoughts/shared/research/[relevant].md`
- Similar implementation: `[file:line]`
````

### Step 5: Review

1. **Present the draft plan location**:
   ```
   I've created the initial implementation plan at:
   `thoughts/shared/plans/YYYY-MM-DD-ENG-XXXX-description.md`

   Please review it and let me know:
   - Are the phases properly scoped?
   - Are the success criteria specific enough?
   - Any technical details that need adjustment?
   - Missing edge cases or considerations?
   ```

2. **Iterate based on feedback** - be ready to:
   - Add missing phases
   - Adjust technical approach
   - Clarify success criteria (both automated and manual)
   - Add/remove scope items

3. **Continue refining** until the user is satisfied

## Important Guidelines

1. **Be Skeptical**:
   - Question vague requirements
   - Identify potential issues early
   - Ask "why" and "what about"
   - Don't assume - verify with code

2. **Be Interactive**:
   - Don't write the full plan in one shot
   - Get buy-in at each major step
   - Allow course corrections
   - Work collaboratively

3. **Be Thorough**:
   - Read all context files COMPLETELY before planning
   - Research actual code patterns using parallel sub-tasks
   - Include specific file paths and line numbers
   - Write measurable success criteria with clear automated vs manual distinction
   - automated steps should use `make` whenever possible - for example `make -C apps/humanlayer-wui check` instead of `cd humanlayer-wui && bun run fmt`

4. **Be Practical**:
   - Focus on incremental, testable changes
   - Consider migration and rollback
   - Think about edge cases
   - Include "what we're NOT doing"

5. **Track Progress**:
   - Use TodoWrite to track planning tasks
   - Update todos as you complete research
   - Mark planning tasks complete when done

6. **No Open Questions in Final Plan**:
   - If you encounter open questions during planning, STOP
   - Research or ask for clarification immediately
   - Do NOT write the plan with unresolved questions
   - The implementation plan must be complete and actionable
   - Every decision must be made before finalizing the plan

## Success Criteria Guidelines

**Always separate success criteria into two categories:**

1. **Automated Verification** (can be run by execution agents):
   - Commands that can be run: `make test`, `npm run lint`, etc.
   - Specific files that should exist
   - Code compilation/type checking
   - Automated test suites

2. **Manual Verification** (requires human testing):
   - UI/UX functionality
   - Performance under real conditions
   - Edge cases that are hard to automate
   - User acceptance criteria

**Format example:**
```markdown
### Success Criteria:

#### Automated Verification:
- [ ] Database migration runs successfully: `make migrate`
- [ ] All unit tests pass: `go test ./...`
- [ ] No linting errors: `golangci-lint run`
- [ ] API endpoint returns 200: `curl localhost:8080/api/new-endpoint`

#### Manual Verification:
- [ ] New feature appears correctly in the UI
- [ ] Performance is acceptable with 1000+ items
- [ ] Error messages are user-friendly
- [ ] Feature works correctly on mobile devices
```

## Common Patterns

### For Database Changes:
- Start with schema/migration
- Add store methods
- Update business logic
- Expose via API
- Update clients

### For New Features:
- Research existing patterns first
- Start with data model
- Build backend logic
- Add API endpoints
- Implement UI last

### For Refactoring:
- Document current behavior
- Plan incremental changes
- Maintain backwards compatibility
- Include migration strategy

## Sub-task Spawning Best Practices

When spawning research sub-tasks:

1. **Spawn multiple tasks in parallel** for efficiency
2. **Each task should be focused** on a specific area
3. **Provide detailed instructions** including:
   - Exactly what to search for
   - Which directories to focus on
   - What information to extract
   - Expected output format
4. **Be EXTREMELY specific about directories**:
   - If the ticket mentions "WUI", specify `humanlayer-wui/` directory
   - If it mentions "daemon", specify `hld/` directory
   - Never use generic terms like "UI" when you mean "WUI"
   - Include the full path context in your prompts
5. **Specify read-only tools** to use
6. **Request specific file:line references** in responses
7. **Wait for all tasks to complete** before synthesizing
8. **Verify sub-task results**:
   - If a sub-task returns unexpected results, spawn follow-up tasks
   - Cross-check findings against the actual codebase
   - Don't accept results that seem incorrect

Example of spawning multiple tasks:
```python
# Spawn these tasks concurrently:
tasks = [
    Task("Research database schema", db_research_prompt),
    Task("Find API patterns", api_research_prompt),
    Task("Investigate UI components", ui_research_prompt),
    Task("Check test patterns", test_research_prompt)
]
```

## Example Interaction Flow

```
User: /create_plan
Assistant: I'll help you create a detailed implementation plan...

User: We need to add parent-child tracking for Claude sub-tasks. See thoughts/shared/tickets/eng_1478.md
Assistant: Let me read that ticket file completely first...

[Reads file fully]

Based on the ticket, I understand we need to track parent-child relationships for Claude sub-task events in the hld daemon. Before I start planning, I have some questions...

[Interactive process continues...]
```
</file>

<file path=".claude/commands/cl/describe_pr.md">
---
description: Generate comprehensive PR descriptions following repository templates
---

# Generate PR Description

You are tasked with generating a comprehensive pull request description following the repository's standard template.

## Steps to follow:

1. **Read the PR description template:**

    - Use the following PR description template:

        ```md
        ## What problem(s) was I solving?

        ## What user-facing changes did I ship?

        ## How I implemented it

        ## How to verify it

        ### Manual Testing

        ## Description for the changelog
        ```

    - Read the template carefully to understand all sections and requirements

2. **Identify the PR to describe:**
   - Check if the current branch has an associated PR: `gh pr view --json url,number,title,state 2>/dev/null`
   - If no PR exists for the current branch, or if on main/master, list open PRs: `gh pr list --limit 10 --json number,title,headRefName,author`
   - Ask the user which PR they want to describe

3. **Check for existing description:**
   - Check if `/tmp/{repo_name}/prs/{number}_description.md` already exists
   - If it exists, read it and inform the user you'll be updating it
   - Consider what has changed since the last description was written

4. **Gather comprehensive PR information:**
   - Get the full PR diff: `gh pr diff {number}`
   - If you get an error about no default remote repository, instruct the user to run `gh repo set-default` and select the appropriate repository
   - Get commit history: `gh pr view {number} --json commits`
   - Review the base branch: `gh pr view {number} --json baseRefName`
   - Get PR metadata: `gh pr view {number} --json url,title,number,state`

5. **Analyze the changes thoroughly:** (ultrathink about the code changes, their architectural implications, and potential impacts)
   - Read through the entire diff carefully
   - For context, read any files that are referenced but not shown in the diff
   - Understand the purpose and impact of each change
   - Identify user-facing changes vs internal implementation details
   - Look for breaking changes or migration requirements

6. **Handle verification requirements:**
   - Look for any checklist items in the "How to verify it" section of the template
   - For each verification step:
     - If it's a command you can run (like `make check test`, `npm test`, etc.), run it
     - If it passes, mark the checkbox as checked: `- [x]`
     - If it fails, keep it unchecked and note what failed: `- [ ]` with explanation
     - If it requires manual testing (UI interactions, external services), leave unchecked and note for user
   - Document any verification steps you couldn't complete

7. **Generate the description:**
   - Fill out each section from the template thoroughly:
     - Answer each question/section based on your analysis
     - Be specific about problems solved and changes made
     - Focus on user impact where relevant
     - Include technical details in appropriate sections
     - Write a concise changelog entry
   - Ensure all checklist items are addressed (checked or explained)

8. **Save and sync the description:**
   - Write the completed description to `/tmp/{repo_name}/prs/{number}_description.md`
   - Show the user the generated description

9. **Update the PR:**
   - Update the PR description directly: `gh pr edit {number} --body-file /tmp/{repo_name}/prs/{number}_description.md`
   - Confirm the update was successful
   - If any verification steps remain unchecked, remind the user to complete them before merging

## Important notes:
- This command works across different repositories - always read the local template
- Be thorough but concise - descriptions should be scannable
- Focus on the "why" as much as the "what"
- Include any breaking changes or migration notes prominently
- If the PR touches multiple components, organize the description accordingly
- Always attempt to run verification commands when possible
- Clearly communicate which verification steps need manual testing
</file>

<file path=".claude/commands/cl/implement_plan.md">
# Implement Plan

You are tasked with implementing an approved technical plan from `thoughts/shared/plans/`. These plans contain phases with specific changes and success criteria.

## Getting Started

When given a plan path:
- Read the plan completely and check for any existing checkmarks (- [x])
- Read the original ticket and all files mentioned in the plan
- **Read files fully** - never use limit/offset parameters, you need complete context
- Think deeply about how the pieces fit together
- Create a todo list to track your progress
- Start implementing if you understand what needs to be done

If no plan path provided, ask for one.

## Implementation Philosophy

Plans are carefully designed, but reality can be messy. Your job is to:
- Follow the plan's intent while adapting to what you find
- Implement each phase fully before moving to the next
- Verify your work makes sense in the broader codebase context
- Update checkboxes in the plan as you complete sections

When things don't match the plan exactly, think about why and communicate clearly. The plan is your guide, but your judgment matters too.

If you encounter a mismatch:
- STOP and think deeply about why the plan can't be followed
- Present the issue clearly:
  ```
  Issue in Phase [N]:
  Expected: [what the plan says]
  Found: [actual situation]
  Why this matters: [explanation]

  How should I proceed?
  ```

## Verification Approach

After implementing a phase:
- Run the success criteria checks (usually `make check test` covers everything)
- Fix any issues before proceeding
- Update your progress in both the plan and your todos
- Check off completed items in the plan file itself using Edit
- **Pause for human verification**: After completing all automated verification for a phase, pause and inform the human that the phase is ready for manual testing. Use this format:
  ```
  Phase [N] Complete - Ready for Manual Verification

  Automated verification passed:
  - [List automated checks that passed]

  Please perform the manual verification steps listed in the plan:
  - [List manual verification items from the plan]

  Let me know when manual testing is complete so I can proceed to Phase [N+1].
  ```

If instructed to execute multiple phases consecutively, skip the pause until the last phase. Otherwise, assume you are just doing one phase.

do not check off items in the manual testing steps until confirmed by the user.


## If You Get Stuck

When something isn't working as expected:
- First, make sure you've read and understood all the relevant code
- Consider if the codebase has evolved since the plan was written
- Present the mismatch clearly and ask for guidance

Use sub-tasks sparingly - mainly for targeted debugging or exploring unfamiliar territory.

## Resuming Work

If the plan has existing checkmarks:
- Trust that completed work is done
- Pick up from the first unchecked item
- Verify previous work only if something seems off

Remember: You're implementing a solution, not just checking boxes. Keep the end goal in mind and maintain forward momentum.
</file>

<file path=".claude/commands/cl/iterate_plan.md">
---
description: Iterate on existing implementation plans with thorough research and updates
model: opus
---

# Iterate Implementation Plan

You are tasked with updating existing implementation plans based on user feedback. You should be skeptical, thorough, and ensure changes are grounded in actual codebase reality.

## Initial Response

When this command is invoked:

1. **Parse the input to identify**:
   - Plan file path (e.g., `thoughts/shared/plans/2025-10-16-feature.md`)
   - Requested changes/feedback

2. **Handle different input scenarios**:

   **If NO plan file provided**:
   ```
   I'll help you iterate on an existing implementation plan.

   Which plan would you like to update? Please provide the path to the plan file (e.g., `thoughts/shared/plans/2025-10-16-feature.md`).

   Tip: You can list recent plans with `ls -lt thoughts/shared/plans/ | head`
   ```
   Wait for user input, then re-check for feedback.

   **If plan file provided but NO feedback**:
   ```
   I've found the plan at [path]. What changes would you like to make?

   For example:
   - "Add a phase for migration handling"
   - "Update the success criteria to include performance tests"
   - "Adjust the scope to exclude feature X"
   - "Split Phase 2 into two separate phases"
   ```
   Wait for user input.

   **If BOTH plan file AND feedback provided**:
   - Proceed immediately to Step 1
   - No preliminary questions needed

## Process Steps

### Step 1: Read and Understand Current Plan

1. **Read the existing plan file COMPLETELY**:
   - Use the Read tool WITHOUT limit/offset parameters
   - Understand the current structure, phases, and scope
   - Note the success criteria and implementation approach

2. **Understand the requested changes**:
   - Parse what the user wants to add/modify/remove
   - Identify if changes require codebase research
   - Determine scope of the update

### Step 2: Research If Needed

**Only spawn research tasks if the changes require new technical understanding.**

If the user's feedback requires understanding new code patterns or validating assumptions:

1. **Create a research todo list** using TodoWrite

2. **Spawn parallel sub-tasks for research**:
   Use the right agent for each type of research:

   **For code investigation:**
   - **codebase-locator** - To find relevant files
   - **codebase-analyzer** - To understand implementation details
   - **codebase-pattern-finder** - To find similar patterns

   **Be EXTREMELY specific about directories**:
   - Include full path context in prompts

3. **Read any new files identified by research**:
   - Read them FULLY into the main context
   - Cross-reference with the plan requirements

4. **Wait for ALL sub-tasks to complete** before proceeding

### Step 3: Present Understanding and Approach

Before making changes, confirm your understanding:

```
Based on your feedback, I understand you want to:
- [Change 1 with specific detail]
- [Change 2 with specific detail]

My research found:
- [Relevant code pattern or constraint]
- [Important discovery that affects the change]

I plan to update the plan by:
1. [Specific modification to make]
2. [Another modification]

Does this align with your intent?
```

Get user confirmation before proceeding.

### Step 4: Update the Plan

1. **Make focused, precise edits** to the existing plan:
   - Use the Edit tool for surgical changes
   - Maintain the existing structure unless explicitly changing it
   - Keep all file:line references accurate
   - Update success criteria if needed

2. **Ensure consistency**:
   - If adding a new phase, ensure it follows the existing pattern
   - If modifying scope, update "What We're NOT Doing" section
   - If changing approach, update "Implementation Approach" section
   - Maintain the distinction between automated vs manual success criteria

3. **Preserve quality standards**:
   - Include specific file paths and line numbers for new content
   - Write measurable success criteria
   - Use `make` commands for automated verification
   - Keep language clear and actionable

### Step 5: Sync and Review

**Present the changes made**:
   ```
   I've updated the plan at `thoughts/shared/plans/[filename].md`

   Changes made:
   - [Specific change 1]
   - [Specific change 2]

   The updated plan now:
   - [Key improvement]
   - [Another improvement]

   Would you like any further adjustments?
   ```

**Be ready to iterate further** based on feedback

## Important Guidelines

1. **Be Skeptical**:
   - Don't blindly accept change requests that seem problematic
   - Question vague feedback - ask for clarification
   - Verify technical feasibility with code research
   - Point out potential conflicts with existing plan phases

2. **Be Surgical**:
   - Make precise edits, not wholesale rewrites
   - Preserve good content that doesn't need changing
   - Only research what's necessary for the specific changes
   - Don't over-engineer the updates

3. **Be Thorough**:
   - Read the entire existing plan before making changes
   - Research code patterns if changes require new technical understanding
   - Ensure updated sections maintain quality standards
   - Verify success criteria are still measurable

4. **Be Interactive**:
   - Confirm understanding before making changes
   - Show what you plan to change before doing it
   - Allow course corrections
   - Don't disappear into research without communicating

5. **Track Progress**:
   - Use TodoWrite to track update tasks if complex
   - Update todos as you complete research
   - Mark tasks complete when done

6. **No Open Questions**:
   - If the requested change raises questions, ASK
   - Research or get clarification immediately
   - Do NOT update the plan with unresolved questions
   - Every change must be complete and actionable

## Success Criteria Guidelines

When updating success criteria, always maintain the two-category structure:

1. **Automated Verification** (can be run by execution agents):
   - Commands that can be run: `make test`, `npm run lint`, etc.
   - Specific files that should exist
   - Code compilation/type checking

2. **Manual Verification** (requires human testing):
   - UI/UX functionality
   - Performance under real conditions
   - Edge cases that are hard to automate
   - User acceptance criteria

## Sub-task Spawning Best Practices

When spawning research sub-tasks:

1. **Only spawn if truly needed** - don't research for simple changes
2. **Spawn multiple tasks in parallel** for efficiency
3. **Each task should be focused** on a specific area
4. **Provide detailed instructions** including:
   - Exactly what to search for
   - Which directories to focus on
   - What information to extract
   - Expected output format
5. **Request specific file:line references** in responses
6. **Wait for all tasks to complete** before synthesizing
7. **Verify sub-task results** - if something seems off, spawn follow-up tasks

## Example Interaction Flows

**Scenario 1: User provides everything upfront**
```
User: /iterate_plan thoughts/shared/plans/2025-10-16-feature.md - add phase for error handling
Assistant: [Reads plan, researches error handling patterns, updates plan]
```

**Scenario 2: User provides just plan file**
```
User: /iterate_plan thoughts/shared/plans/2025-10-16-feature.md
Assistant: I've found the plan. What changes would you like to make?
User: Split Phase 2 into two phases - one for backend, one for frontend
Assistant: [Proceeds with update]
```

**Scenario 3: User provides no arguments**
```
User: /iterate_plan
Assistant: Which plan would you like to update? Please provide the path...
User: thoughts/shared/plans/2025-10-16-feature.md
Assistant: I've found the plan. What changes would you like to make?
User: Add more specific success criteria to phase 4
Assistant: [Proceeds with update]
```
</file>

<file path=".claude/commands/cl/research_codebase.md">
# Research Codebase

You are tasked with conducting comprehensive research across the codebase to answer user questions by spawning parallel sub-agents and synthesizing their findings.

## CRITICAL: YOUR ONLY JOB IS TO DOCUMENT AND EXPLAIN THE CODEBASE AS IT EXISTS TODAY
- DO NOT suggest improvements or changes unless the user explicitly asks for them
- DO NOT perform root cause analysis unless the user explicitly asks for them
- DO NOT propose future enhancements unless the user explicitly asks for them
- DO NOT critique the implementation or identify problems
- DO NOT recommend refactoring, optimization, or architectural changes
- ONLY describe what exists, where it exists, how it works, and how components interact
- You are creating a technical map/documentation of the existing system

## Initial Setup:

When this command is invoked, respond with:
```
I'm ready to research the codebase. Please provide your research question or area of interest, and I'll analyze it thoroughly by exploring relevant components and connections.
```

Then wait for the user's research query.

## Steps to follow after receiving the research query:

1. **Read any directly mentioned files first:**
   - If the user mentions specific files (tickets, docs, JSON), read them FULLY first
   - **IMPORTANT**: Use the Read tool WITHOUT limit/offset parameters to read entire files
   - **CRITICAL**: Read these files yourself in the main context before spawning any sub-tasks
   - This ensures you have full context before decomposing the research

2. **Analyze and decompose the research question:**
   - Break down the user's query into composable research areas
   - Take time to ultrathink about the underlying patterns, connections, and architectural implications the user might be seeking
   - Identify specific components, patterns, or concepts to investigate
   - Create a research plan using TodoWrite to track all subtasks
   - Consider which directories, files, or architectural patterns are relevant

3. **Spawn parallel sub-agent tasks for comprehensive research:**
   - Create multiple Task agents to research different aspects concurrently
   - We now have specialized agents that know how to do specific research tasks:

   **For codebase research:**
   - Use the **codebase-locator** agent to find WHERE files and components live
   - Use the **codebase-analyzer** agent to understand HOW specific code works (without critiquing it)
   - Use the **codebase-pattern-finder** agent to find examples of existing patterns (without evaluating them)

   **IMPORTANT**: All agents are documentarians, not critics. They will describe what exists without suggesting improvements or identifying issues.

   **For web research (only if user explicitly asks):**
   - Use the **web-search-researcher** agent for external documentation and resources
   - IF you use web-research agents, instruct them to return LINKS with their findings, and please INCLUDE those links in your final report

   **For Linear tickets (if relevant):**
   - Use the **linear-ticket-reader** agent to get full details of a specific ticket
   - Use the **linear-searcher** agent to find related tickets or historical context

   The key is to use these agents intelligently:
   - Start with locator agents to find what exists
   - Then use analyzer agents on the most promising findings to document how they work
   - Run multiple agents in parallel when they're searching for different things
   - Each agent knows its job - just tell it what you're looking for
   - Don't write detailed prompts about HOW to search - the agents already know
   - Remind agents they are documenting, not evaluating or improving

4. **Wait for all sub-agents to complete and synthesize findings:**
   - IMPORTANT: Wait for ALL sub-agent tasks to complete before proceeding
   - Compile all sub-agent results
   - Prioritize live codebase findings as primary source of truth
   - Connect findings across different components
   - Include specific file paths and line numbers for reference
   - Highlight patterns, connections, and architectural decisions
   - Answer the user's specific questions with concrete evidence

5. **Gather metadata for the research document:**
   - Run Bash() tools to generate all relevant metadata
   - Filename: `thoughts/shared/research/YYYY-MM-DD-ENG-XXXX-description.md`
     - Format: `YYYY-MM-DD-ENG-XXXX-description.md` where:
       - YYYY-MM-DD is today's date
       - ENG-XXXX is the ticket number (omit if no ticket)
       - description is a brief kebab-case description of the research topic
     - Examples:
       - With ticket: `2025-01-08-ENG-1478-parent-child-tracking.md`
       - Without ticket: `2025-01-08-authentication-flow.md`

6. **Generate research document:**
   - Use the metadata gathered in step 4
   - Structure the document with YAML frontmatter followed by content:
     ```markdown
     ---
     date: [Current date and time with timezone in ISO format]
     researcher: [Researcher name from metadata]
     git_commit: [Current commit hash]
     branch: [Current branch name]
     repository: [Repository name]
     topic: "[User's Question/Topic]"
     tags: [research, codebase, relevant-component-names]
     status: complete
     last_updated: [Current date in YYYY-MM-DD format]
     last_updated_by: [Researcher name]
     ---

     # Research: [User's Question/Topic]

     **Date**: [Current date and time with timezone from step 4]
     **Researcher**: [Researcher name from metadata]
     **Git Commit**: [Current commit hash from step 4]
     **Branch**: [Current branch name from step 4]
     **Repository**: [Repository name]

     ## Research Question
     [Original user query]

     ## Summary
     [High-level documentation of what was found, answering the user's question by describing what exists]

     ## Detailed Findings

     ### [Component/Area 1]
     - Description of what exists ([file.ext:line](link))
     - How it connects to other components
     - Current implementation details (without evaluation)

     ### [Component/Area 2]
     ...

     ## Code References
     - `path/to/file.py:123` - Description of what's there
     - `another/file.ts:45-67` - Description of the code block

     ## Architecture Documentation
     [Current patterns, conventions, and design implementations found in the codebase]

     ## Related Research
     [Links to other research documents in thoughts/shared/research/]

     ## Open Questions
     [Any areas that need further investigation]
     ```

7. **Add GitHub permalinks (if applicable):**
   - Check if on main branch or if commit is pushed: `git branch --show-current` and `git status`
   - If on main/master or pushed, generate GitHub permalinks:
     - Get repo info: `gh repo view --json owner,name`
     - Create permalinks: `https://github.com/{owner}/{repo}/blob/{commit}/{file}#L{line}`
   - Replace local file references with permalinks in the document

8. **Present findings:**
   - Present a concise summary of findings to the user
   - Include key file references for easy navigation
   - Ask if they have follow-up questions or need clarification

9. **Handle follow-up questions:**
   - If the user has follow-up questions, append to the same research document
   - Update the frontmatter fields `last_updated` and `last_updated_by` to reflect the update
   - Add `last_updated_note: "Added follow-up research for [brief description]"` to frontmatter
   - Add a new section: `## Follow-up Research [timestamp]`
   - Spawn new sub-agents as needed for additional investigation
   - Continue updating the document

## Important notes:
- Always use parallel Task agents to maximize efficiency and minimize context usage
- Always run fresh codebase research - never rely solely on existing research documents
- Focus on finding concrete file paths and line numbers for developer reference
- Research documents should be self-contained with all necessary context
- Each sub-agent prompt should be specific and focused on read-only documentation operations
- Document cross-component connections and how systems interact
- Include temporal context (when the research was conducted)
- Link to GitHub when possible for permanent references
- Keep the main agent focused on synthesis, not deep file reading
- Have sub-agents document examples and usage patterns as they exist
- **CRITICAL**: You and all sub-agents are documentarians, not evaluators
- **REMEMBER**: Document what IS, not what SHOULD BE
- **NO RECOMMENDATIONS**: Only describe the current state of the codebase
- **File reading**: Always read mentioned files FULLY (no limit/offset) before spawning sub-tasks
- **Critical ordering**: Follow the numbered steps exactly
  - ALWAYS read mentioned files first before spawning sub-tasks (step 1)
  - ALWAYS wait for all sub-agents to complete before synthesizing (step 4)
  - ALWAYS gather metadata before writing the document (step 5 before step 6)
  - NEVER write the research document with placeholder values
- **Frontmatter consistency**:
  - Always include frontmatter at the beginning of research documents
  - Keep frontmatter fields consistent across all research documents
  - Update frontmatter when adding follow-up research
  - Use snake_case for multi-word field names (e.g., `last_updated`, `git_commit`)
  - Tags should be relevant to the research topic and components studied
</file>

<file path=".claude/commands/scud/task-claim.md">
---
description: Claim or release a SCUD task lock
allowed-tools: Bash(scud:*)
argument-hint: <task-id> --name <name> [--tag <tag>] | release <task-id> [--force]
---

Claim a task to prevent conflicts during parallel work, or release a claimed task.

To claim:
```bash
scud claim $ARGUMENTS
```

To release (if first argument is "release"):
```bash
scud release $ARGUMENTS
```

After claiming:
1. Confirm the lock is set
2. Remind about automatic release: if hooks are installed, the lock will be auto-released when the task is marked complete
3. Manual release command: `scud release <id>`

After releasing:
1. Confirm the lock is cleared
2. Show how long the task was locked

**Hook Integration:**
- When hooks are installed (`scud hooks install`), task locks are automatically released when the task is marked complete
- This happens when the Claude session ends with `SCUD_TASK_ID` set
- Manual release is only needed if a session crashes or is interrupted
</file>

<file path=".claude/commands/scud/task-doctor.md">
---
description: Diagnose and fix SCUD task issues
allowed-tools: Bash(scud:*)
argument-hint: [--tag <tag>] [--stale-hours <n>] [--fix]
---

Diagnose issues with tasks like stale locks, orphaned subtasks, or circular dependencies.

```bash
scud doctor $ARGUMENTS
```

Report findings:
- Stale locks (tasks locked for too long)
- Orphaned subtasks
- Circular dependencies
- Missing dependency targets

If `--fix` was used, confirm what was repaired.
</file>

<file path=".claude/commands/scud/task-list.md">
---
description: List SCUD tasks with optional status filter
allowed-tools: Bash(scud:*)
argument-hint: [--status pending|in-progress|done|blocked] [--tag <tag>]
---

List tasks from the SCUD task graph.

```bash
scud list $ARGUMENTS
```

After running, summarize:
- Total tasks shown
- Breakdown by status
- Any tasks that are blocked or stale
</file>

<file path=".claude/commands/scud/task-next.md">
---
description: Find and optionally claim the next available SCUD task
allowed-tools: Bash(scud:*)
argument-hint: [--claim --name <name>] [--tag <tag>]
---

Find the next available task based on dependencies and status.

```bash
scud next $ARGUMENTS
```

After finding the next task:
1. Show the task ID, title, and complexity
2. List its dependencies and their status
3. If `--claim` was used, confirm the task is now locked
4. Remind about hooks: if hooks are installed, set `SCUD_TASK_ID=<id>` when starting work
5. Suggest the command to start working: `scud set-status <id> in-progress`

**Note:** The `--claim` flag is experimental. It locks the task to prevent conflicts in parallel workflows.

**Hook Integration:**
- When hooks are installed (`scud hooks install`), tasks are automatically marked complete when Claude sessions end
- Set the `SCUD_TASK_ID` environment variable to enable automatic completion
- Example: `SCUD_TASK_ID=5 claude "Implement task 5"`
</file>

<file path=".claude/commands/scud/task-show.md">
---
description: Show detailed information about a SCUD task
allowed-tools: Bash(scud:*)
argument-hint: <task-id> [--tag <tag>]
---

Show detailed information about a specific task.

```bash
scud show $ARGUMENTS
```

Present the task details including:
- Title, status, complexity, priority
- Full description
- Test strategy (if defined)
- Dependencies and their current status
- Assignment and lock information
</file>

<file path=".claude/commands/scud/task-stats.md">
---
description: Show SCUD task completion statistics
allowed-tools: Bash(scud:*)
argument-hint: [--tag <tag>]
---

Show completion statistics for tasks.

```bash
scud stats $ARGUMENTS
```

Summarize:
- Overall progress percentage
- Tasks by status (pending, in-progress, done, blocked)
- Total complexity points completed vs remaining
- Highlight any blocked tasks that need attention
</file>

<file path=".claude/commands/scud/task-status.md">
---
description: Update the status of a SCUD task
allowed-tools: Bash(scud:*)
argument-hint: <task-id> <status> [--tag <tag>]
---

Update a task's status. Valid statuses: pending, in-progress, done, blocked, review, deferred, cancelled.

```bash
scud set-status $ARGUMENTS
```

After updating:
1. Confirm the status change
2. If marked `done`, suggest running `scud next` to find the next task
3. If marked `blocked`, ask what's blocking and whether to add a note
</file>

<file path=".claude/commands/scud/task-tags.md">
---
description: List or set the active SCUD task tag
allowed-tools: Bash(scud:*)
argument-hint: [<tag>]
---

List all tags or set the active tag.

```bash
scud tags $ARGUMENTS
```

If listing tags:
- Show all available tags
- Indicate which is currently active
- Show task count per tag if available

If setting a tag:
- Confirm the active tag changed
- Show quick stats for the new active tag
</file>

<file path=".claude/commands/scud/task-waves.md">
---
description: Show parallel execution waves for SCUD tasks
allowed-tools: Bash(scud:*)
argument-hint: [--tag <tag>] [--max-parallel <n>] [--all-tags]
---

Compute and display parallel execution waves based on task dependencies.

```bash
scud waves $ARGUMENTS
```

Explain the output:
1. Wave 1 tasks have no dependencies and can start immediately
2. Each subsequent wave depends on prior waves completing
3. Tasks within a wave can run in parallel
4. The speedup ratio shows efficiency vs sequential execution
</file>

<file path=".claude/commands/scud/task-whois.md">
---
description: Show who is working on SCUD tasks
allowed-tools: Bash(scud:*)
argument-hint: [--tag <tag>]
---

Show task assignments and locks.

```bash
scud whois $ARGUMENTS
```

Display:
- Which tasks are assigned to whom
- Which tasks are currently locked
- How long each lock has been held
- Flag any stale locks (>24 hours)
</file>

<file path=".claude/skills/scud-tasks/graph-concepts.md">
# SCUD Graph Concepts

This document explains how SCUD manages tasks as a graph structure, including the SCG (SCUD Graph) format, dependencies, waves, and parallel execution.

## The Task Graph

SCUD represents tasks as a **directed acyclic graph (DAG)**:

- **Nodes**: Tasks (with status, complexity, priority)
- **Edges**: Dependencies (task A must complete before task B)
- **Parent-Child**: Subtask relationships (task expansion)

```
     â”Œâ”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”
     â”‚  1  â”‚      â”‚  2  â”‚     Wave 1 (no deps)
     â””â”€â”€â”¬â”€â”€â”˜      â””â”€â”€â”¬â”€â”€â”˜
        â”‚            â”‚
        â–¼            â–¼
     â”Œâ”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”
     â”‚  3  â”‚â—„â”€â”€â”€â”€â”€â”‚  4  â”‚     Wave 2 (depends on 1, 2)
     â””â”€â”€â”¬â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”˜
        â”‚
        â–¼
     â”Œâ”€â”€â”€â”€â”€â”
     â”‚  5  â”‚                  Wave 3 (depends on 3)
     â””â”€â”€â”€â”€â”€â”˜
```

## SCG Format (SCUD Graph)

SCUD stores tasks in the **SCG format**, a token-efficient, human-readable format optimized for:
- Minimal file size (~60% smaller than JSON)
- Git-friendly diffs
- Human readability and manual editing
- Fast parsing

### File Structure

Tasks are stored in `.scud/tasks/tasks.scg`:

```
# SCUD Graph v1
# Phase: auth

@meta {
  name auth
}

@nodes
id | title | status | complexity | priority
auth:1 | Create user model | P | 3 | H
auth:2 | Auth middleware | I | 5 | H
auth:3 | Login endpoint | P | 5 | M
auth:4 | Registration | D | 5 | M

@edges
auth:3 -> auth:1
auth:3 -> auth:2
auth:4 -> auth:1

@parents
auth:3: auth:3.1, auth:3.2

@assignments
auth:2 | alice | alice | 2025-11-26T10:00:00Z

@details
auth:1 | description |
  Create User model with email, password_hash, created_at

auth:2 | test_strategy |
  Unit tests for middleware functions
  Integration tests for auth flow
```

### Section Reference

| Section | Purpose | Format |
|---------|---------|--------|
| `@meta` | Phase metadata | `name <tag>` |
| `@nodes` | Task definitions | `id \| title \| status \| complexity \| priority` |
| `@edges` | Dependencies | `child -> parent` |
| `@parents` | Subtask relationships | `parent: child1, child2` |
| `@assignments` | Who's working | `id \| assignee \| locker \| lock_time` |
| `@details` | Extended text | `id \| field_name \| <multiline>` |

### Status Codes

Single-letter codes for compact representation:

| Code | Status | Meaning |
|------|--------|---------|
| P | Pending | Not started |
| I | InProgress | Being worked on |
| D | Done | Completed |
| R | Review | Awaiting review |
| B | Blocked | Cannot proceed |
| F | Deferred | Postponed |
| C | Cancelled | Aborted |
| X | Expanded | Has subtasks |

### Priority Codes

| Code | Priority |
|------|----------|
| H | High |
| M | Medium |
| L | Low |

## Namespaced Task IDs

Task IDs are namespaced by tag to prevent collisions:

```
auth:1      # Task 1 in 'auth' tag
auth:1.1    # Subtask 1.1 of task 1 in 'auth'
api:1       # Task 1 in 'api' tag (different from auth:1)
```

This enables:
- Multiple tags in a single file
- Clear task ownership
- Cross-tag references (rare but possible)

## Dependencies

Dependencies define execution order:

```
@edges
auth:3 -> auth:1    # Task 3 depends on task 1
auth:3 -> auth:2    # Task 3 also depends on task 2
auth:4 -> auth:1    # Task 4 depends on task 1
```

### Dependency Rules

1. **No cycles**: A -> B -> A is invalid
2. **Within actionable tasks**: Only non-Done, non-Expanded tasks count
3. **Cross-tag possible**: `api:1 -> auth:2` works but is discouraged

### Checking Dependencies

```bash
scud show 3 --tag auth
# Shows: Dependencies: 1 (done), 2 (in-progress)

scud next --tag auth
# Only returns tasks whose dependencies are all Done
```

## Waves and Parallel Execution

**Waves** are computed using **Kahn's algorithm** (topological sort):

### Wave Computation

1. Find all tasks with no dependencies â†’ **Wave 1**
2. Remove Wave 1 tasks from graph
3. Find all tasks with no remaining dependencies â†’ **Wave 2**
4. Repeat until all tasks assigned

### Example

```
Tasks:
  1: No deps
  2: No deps
  3: Depends on 1
  4: Depends on 1, 2
  5: Depends on 3

Waves:
  Wave 1: [1, 2]      # Can run in parallel
  Wave 2: [3, 4]      # Can run after Wave 1
  Wave 3: [5]         # Can run after Wave 2
```

### Viewing Waves

```bash
scud waves --tag auth

Wave 1: (2 tasks)
  P 1 | Create user model [3]
  P 2 | Auth config [2]

Wave 2: (2 tasks)
  P 3 | Auth middleware <- 1 [5]
  P 4 | Registration <- 1,2 [5]

Wave 3: (1 task)
  P 5 | Login flow <- 3,4 [5]

Summary:
  Tasks: 5
  Waves: 3
  Speedup: 1.7x
```

### Max Parallel Batching

Use `--max-parallel N` to batch large waves:

```bash
scud waves --max-parallel 3

Wave 1, Round 1: [1, 2, 3]    # First 3 tasks
Wave 1, Round 2: [4, 5]       # Remaining 2 tasks
Wave 2, Round 1: [6, 7, 8]    # Next wave
```

### Speedup Calculation

```
Speedup = Total Tasks / Total Rounds

Example:
  10 tasks, 3 waves, max-parallel 5
  Wave 1: 6 tasks â†’ 2 rounds
  Wave 2: 3 tasks â†’ 1 round
  Wave 3: 1 task  â†’ 1 round
  Total rounds: 4
  Speedup: 10/4 = 2.5x
```

## Subtasks (Task Expansion)

Complex tasks (â‰¥13 points) should be expanded:

```
Before:
  Task 4: Implement login/logout [13 points] - status: Pending

After expansion:
  Task 4: Implement login/logout [13 points] - status: Expanded
    4.1: Login form component [3]
    4.2: Login API endpoint [3]
    4.3: Token generation [2]
    4.4: Logout endpoint [2]
    4.5: Auth state management [3]
```

### SCG Representation

```
@nodes
auth:4 | Implement login/logout | X | 13 | H
auth:4.1 | Login form component | P | 3 | H
auth:4.2 | Login API endpoint | P | 3 | H
auth:4.3 | Token generation | P | 2 | M
auth:4.4 | Logout endpoint | P | 2 | M
auth:4.5 | Auth state management | P | 3 | H

@parents
auth:4: auth:4.1, auth:4.2, auth:4.3, auth:4.4, auth:4.5
```

### Subtask Rules

1. Parent status becomes **Expanded** (X)
2. Parent excluded from wave computation
3. Subtasks become actionable items
4. Stats count subtasks, not parent
5. Parent is "done" when all subtasks are done

## Complexity (Fibonacci Points)

SCUD uses Fibonacci numbers for estimation:

| Points | Size | Action |
|--------|------|--------|
| 1 | Trivial | Config change |
| 2 | Tiny | One-liner fix |
| 3 | Small | Few functions |
| 5 | Medium | Multiple files |
| 8 | Large | Significant feature |
| 13 | Too big | **Must expand** |
| 21 | Way too big | **Must expand** |
| 34+ | Epic | **Must expand** |

### Why Fibonacci?

- Forces relative sizing (is this twice as hard or three times?)
- Gaps prevent false precision
- 8 vs 13 is a clear decision point
- Higher numbers signal "needs breakdown"

## File Locking

SCUD uses file-level locking for concurrent access:

### Lock Strategy

1. **Exclusive lock** for writes (one writer at a time)
2. **Shared lock** for reads (multiple readers OK)
3. **Exponential backoff** retry (up to 10 attempts)
4. **Atomic writes** prevent corruption

### Task-Level Locking

Separate from file locks, task claims prevent work conflicts:

```bash
scud claim 3 --name alice    # Sets locked_by, locked_at
scud release 3               # Clears lock
```

```
@assignments
auth:3 | alice | alice | 2025-11-26T10:30:00Z
#        â†‘        â†‘        â†‘
#        â”‚        â”‚        â””â”€â”€ Lock timestamp
#        â”‚        â””â”€â”€ Who locked (can be released)
#        â””â”€â”€ Who's assigned (informational)
```

## Multiple Tags in One File

A single `tasks.scg` can contain multiple tags, separated by `---`:

```
# SCUD Graph v1
# Phase: auth

@meta {
  name auth
}

@nodes
auth:1 | Create user model | P | 3 | H

---

# Phase: api

@meta {
  name api
}

@nodes
api:1 | REST endpoints | P | 5 | H
api:2 | GraphQL schema | P | 8 | M
```

## Practical Examples

### Reading the Graph

```bash
# See all tasks
scud list --tag auth

# See dependency structure
scud waves --tag auth

# See specific task with dependencies
scud show 3 --tag auth
```

### Modifying the Graph

```bash
# Status changes update @nodes section
scud set-status 1 done --tag auth

# Claims update @assignments section
scud claim 2 --name alice --tag auth

# Expansion adds subtasks to @nodes and @parents
scud expand 4 --tag auth
```

### Diagnosing Issues

```bash
# Find problems
scud doctor --tag auth

# Common issues:
# - Circular dependencies (A -> B -> A)
# - Orphaned subtasks (parent deleted)
# - Stale locks (>24 hours old)
# - Missing dependency targets
```
</file>

<file path=".claude/skills/scud-tasks/SKILL.md">
---
name: scud-tasks
description: SCUD task management - view, update, claim, and track tasks in the SCUD graph system
---

# SCUD Task Management

This skill provides tools and knowledge for managing tasks in the SCUD (Structured Collaboration Using Dependencies) system. SCUD is a phase-gated task management system designed for AI-driven development workflows.

## When to Use This Skill

Use this skill when:
- Viewing or listing tasks (`scud list`, `scud show`)
- Updating task status (`scud set-status`)
- Claiming or releasing tasks for parallel work
- Finding the next available task
- Checking progress and statistics
- Understanding wave-based parallelism
- Working with task dependencies

## Quick Reference

### View Tasks
```bash
scud list                           # List all tasks in active tag
scud list --tag auth                # List tasks in specific tag
scud list --status pending          # Filter by status
scud show <task-id>                 # Show task details
scud show <task-id> --tag auth      # Show task in specific tag
```

### Update Status
```bash
scud set-status <task-id> pending
scud set-status <task-id> in-progress
scud set-status <task-id> done
scud set-status <task-id> blocked
scud set-status <task-id> review
```

### Task Assignment & Locking
```bash
scud claim <task-id> --name <name>  # Lock task for yourself
scud release <task-id>              # Release lock
scud whois                          # Show who's working on what
```

### Find Work
```bash
scud next                           # Get next available task
scud next --claim --name <name>     # Find and claim in one step
scud waves                          # See parallel execution waves
```

### Progress
```bash
scud stats                          # Show completion statistics
scud tags                           # List all tags
scud tags <tag>                     # Set active tag
```

## Task Statuses

| Code | Name | Meaning |
|------|------|---------|
| P | Pending | Not started |
| I | InProgress | Currently being worked on |
| D | Done | Completed |
| R | Review | Awaiting review |
| B | Blocked | Cannot proceed |
| F | Deferred | Postponed |
| C | Cancelled | Aborted |
| X | Expanded | Decomposed into subtasks |

## Key Concepts

### Tags (Task Groups)
Tasks are organized into tags (like `auth`, `api`, `ui`). Always specify `--tag <tag>` or set the active tag with `scud tags <tag>`.

### Waves
Tasks are organized into waves based on dependencies:
- **Wave 1**: Tasks with no dependencies (can all run in parallel)
- **Wave 2+**: Tasks that depend on earlier waves

Use `scud waves` to see the wave breakdown.

### Claiming Tasks
When multiple agents or developers work in parallel, use `scud claim` to prevent conflicts:
```bash
scud claim <task-id> --name alice   # Lock task
# ... work on task ...
scud release <task-id>              # Unlock when done
```

## Additional Documentation

For more details, see:
- [Task Commands Reference](task-commands.md) - Complete CLI command reference
- [Graph Concepts](graph-concepts.md) - Understanding SCG format and waves
</file>

<file path=".claude/skills/scud-tasks/task-commands.md">
# SCUD Task Commands Reference

Complete reference for all SCUD CLI commands related to task management.

## Tag Management

### `scud tags [tag]`
List or set the active task tag.

```bash
scud tags              # List all tags
scud tags auth         # Set 'auth' as active tag
```

**Output:**
```
Tags:
  * auth (active)
    api
    ui
```

## Task Viewing

### `scud list`
List tasks in a tag.

```bash
scud list                              # List active tag tasks
scud list --tag auth                   # List tasks in 'auth'
scud list --status pending             # Filter by status
scud list --status in-progress --tag api
```

**Status filters:** `pending`, `in-progress`, `done`, `blocked`, `review`, `deferred`, `cancelled`, `expanded`

**Output:**
```
Tasks for tag: auth

  ID   | Status | Title                    | Complexity
  -----+--------+--------------------------+------------
  1    | P      | Create user model        | 3
  2    | I      | Auth middleware          | 5
  3    | D      | Login endpoint           | 5
```

### `scud show <task-id>`
Show detailed information about a task.

```bash
scud show 1                    # Show task 1 in active tag
scud show 1 --tag auth         # Show task 1 in 'auth' tag
```

**Output:**
```
Task: 1
Title: Create user model
Status: Pending
Complexity: 3 (Small)
Priority: High

Description:
  Create the User model with email, password_hash, and timestamps.

Dependencies: none

Test Strategy:
  - Unit tests for validation
  - Integration tests for database operations

Assigned to: (none)
Locked by: (none)
```

## Task Status Updates

### `scud set-status <task-id> <status>`
Update a task's status.

```bash
scud set-status 1 in-progress          # Start working
scud set-status 1 done                 # Mark complete
scud set-status 1 blocked              # Mark blocked
scud set-status 1 review               # Submit for review
scud set-status 1 pending --tag auth   # Reset to pending
```

**Valid statuses:** `pending`, `in-progress`, `done`, `blocked`, `review`, `deferred`, `cancelled`

## Finding Work

### `scud next`
Find the next available task based on dependencies and status.

```bash
scud next                              # Find next in active tag
scud next --tag auth                   # Find next in 'auth'
scud next --claim --name alice         # Find and claim in one step
```

**Output:**
```
Next available task:

  ID: 2
  Title: Auth middleware
  Complexity: 5
  Dependencies: 1 (done)

  scud claim 2 --name <your-name> --tag auth
```

### `scud waves`
Compute and display parallel execution waves based on dependencies.

```bash
scud waves                             # Waves for active tag
scud waves --tag auth                  # Waves for 'auth'
scud waves --max-parallel 3            # Limit parallel tasks
scud waves --all-tags                  # Show all tags
```

**Output:**
```
Waves for tag: auth

Wave 1: (2 tasks, can run in parallel)
  P 1 | Create user model [3]
  P 2 | Auth config [2]

Wave 2: (3 tasks, depends on Wave 1)
  P 3 | Auth middleware <- 1 [5]
  P 4 | Registration <- 1,2 [5]
  P 5 | Password reset <- 1 [3]

Wave 3: (2 tasks, depends on Wave 2)
  P 6 | Login flow <- 3,4 [5]
  P 7 | Session mgmt <- 3 [3]

Summary:
  Tasks: 7
  Waves: 3
  Speedup: 2.3x (vs sequential)
```

**Understanding waves:**
- Wave 1 tasks have no dependencies - start here
- Each subsequent wave depends on previous waves completing
- Tasks within a wave can run in parallel
- `--max-parallel N` splits large waves into batches

## Task Assignment & Locking

### `scud claim <task-id> --name <name>`
Claim a task to prevent concurrent work conflicts.

```bash
scud claim 3 --name alice              # Claim task 3
scud claim 3 --name alice --tag auth   # Claim in specific tag
```

**Output:**
```
Claimed task 3 for alice

  Title: Auth middleware
  Status: Pending -> In Progress
  Locked by: alice
  Locked at: 2025-11-26T10:30:00Z
```

### `scud release <task-id>`
Release a claimed task.

```bash
scud release 3                         # Release task 3
scud release 3 --force                 # Force release (any lock)
scud release 3 --tag auth              # Release in specific tag
```

**Output:**
```
Released task 3

  Previously locked by: alice
  Duration: 2h 15m
```

### `scud whois`
Show who is working on what tasks.

```bash
scud whois                             # Active tag assignments
scud whois --tag auth                  # Specific tag
```

**Output:**
```
Task Assignments (auth):

  Task | Assigned To | Locked By | Lock Age
  -----+-------------+-----------+---------
  3    | alice       | alice     | 2h 15m
  5    | bob         | (none)    | -
  7    | alice       | (none)    | -
```

### `scud assign <task-id> <assignee>`
Assign a task to someone (informational, doesn't lock).

```bash
scud assign 5 bob                      # Assign task 5 to bob
scud assign 5 bob --tag auth           # Assign in specific tag
```

## Progress & Statistics

### `scud stats`
Show completion statistics for a tag.

```bash
scud stats                             # Active tag stats
scud stats --tag auth                  # Specific tag stats
```

**Output:**
```
Statistics for tag: auth

  Total Tasks:    12
  Pending:        5  (42%)
  In Progress:    2  (17%)
  Done:           4  (33%)
  Blocked:        1  (8%)
  Expanded:       0

  Total Complexity: 47 points
  Completed:        18 points (38%)
  Remaining:        29 points

  Progress: [=========>          ] 38%
```

## Maintenance

### `scud doctor`
Diagnose and fix issues with tasks.

```bash
scud doctor                            # Check active tag
scud doctor --tag auth                 # Check specific tag
scud doctor --stale-hours 12           # Custom stale threshold
scud doctor --fix                      # Auto-fix issues
```

**Checks:**
- Stale locks (default: >24 hours old)
- Orphaned subtasks
- Circular dependencies
- Missing dependencies

**Output:**
```
Diagnosis for tag: auth

Issues found:
  WARN: Task 3 has stale lock (36 hours old, locked by alice)
  WARN: Task 7 depends on non-existent task 99

Run with --fix to auto-repair:
  scud doctor --tag auth --fix
```

## Task Expansion

### `scud expand <task-id>`
Break down a complex task into subtasks.

```bash
scud expand 4                          # Expand task 4
scud expand 4 --tag auth               # Expand in specific tag
scud expand --all                      # Expand all tasks >= 13 points
```

**Output:**
```
Expanding task 4: Implement login/logout [13 points]

Created subtasks:
  4.1 | Login form component [3]
  4.2 | Login API endpoint [3]
  4.3 | Token generation [2]
  4.4 | Logout endpoint [2]
  4.5 | Auth state management [3]

Task 4 status: Expanded
Total subtask points: 13 (matches parent)
```

### `scud analyze-complexity`
AI-powered complexity analysis for tasks.

```bash
scud analyze-complexity                # Analyze all tasks
scud analyze-complexity --task 5       # Analyze specific task
scud analyze-complexity --tag auth     # Analyze specific tag
```

**Output:**
```
Complexity Analysis for tag: auth

  Task 5: Password reset
    Current: 3 points
    Suggested: 5 points
    Reason: Involves email integration, token management, security considerations

  Task 7: Session management
    Current: 8 points
    Suggested: 13 points (NEEDS EXPANSION)
    Reason: Multiple storage backends, invalidation logic, concurrent session handling
```

## Examples

### Complete Task Workflow
```bash
# 1. Set active tag
scud tags auth

# 2. See what needs doing
scud waves

# 3. Find next available task
scud next

# 4. Claim it
scud claim 3 --name dev

# 5. Start working
scud set-status 3 in-progress

# 6. View details while working
scud show 3

# 7. Mark complete
scud set-status 3 done

# 8. Release lock
scud release 3

# 9. Check progress
scud stats
```

### Parallel Team Work
```bash
# Developer 1
scud next --claim --name alice --tag auth
# Works on task...
scud set-status <id> done
scud release <id>

# Developer 2 (concurrent)
scud next --claim --name bob --tag auth
# Works on different task...
scud set-status <id> done
scud release <id>

# See who's working on what
scud whois --tag auth
```
</file>

<file path=".claude/settings.local.json">
{
  "hooks": {
    "Stop": [
      {
        "hooks": [
          {
            "command": "scud _hook-complete",
            "type": "command"
          }
        ],
        "matcher": ""
      }
    ]
  },
  "permissions": {
    "allow": [
      "Bash(find:*)"
    ],
    "ask": [],
    "deny": []
  }
}
</file>

<file path=".github/workflows/coverage.yml">
name: Coverage

on:
  push:
    branches: [ master, main ]
  pull_request:
    branches: [ master, main ]

jobs:
  coverage:
    name: Code Coverage
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Install Rust
        uses: actions-rs/toolchain@v1
        with:
          toolchain: stable
          override: true

      - name: Install tarpaulin
        run: cargo install cargo-tarpaulin

      - name: Generate coverage
        run: |
          cd scud-cli
          cargo tarpaulin --out Xml --output-dir ../

      - name: Upload to codecov
        uses: codecov/codecov-action@v4
        with:
          files: ./cobertura.xml
          fail_ci_if_error: false
          verbose: true
</file>

<file path=".github/workflows/release.yml">
name: Release

on:
  push:
    tags:
      - 'v*'
  workflow_dispatch:

jobs:
  build:
    name: Build ${{ matrix.target }}
    runs-on: ${{ matrix.os }}
    strategy:
      matrix:
        include:
          - os: ubuntu-latest
            target: x86_64-unknown-linux-gnu
            artifact_name: scud
            asset_name: scud-linux-x64
          - os: macos-latest
            target: x86_64-apple-darwin
            artifact_name: scud
            asset_name: scud-macos-x64
          - os: macos-latest
            target: aarch64-apple-darwin
            artifact_name: scud
            asset_name: scud-macos-arm64
          - os: windows-latest
            target: x86_64-pc-windows-msvc
            artifact_name: scud.exe
            asset_name: scud-windows-x64.exe

    steps:
      - uses: actions/checkout@v4

      - name: Install Rust
        uses: actions-rs/toolchain@v1
        with:
          toolchain: stable
          target: ${{ matrix.target }}
          override: true

      - name: Build
        working-directory: ./scud-cli
        run: cargo build --release --target ${{ matrix.target }}

      - name: Upload binary
        uses: actions/upload-artifact@v4
        with:
          name: ${{ matrix.asset_name }}
          path: scud-cli/target/${{ matrix.target }}/release/${{ matrix.artifact_name }}

  release:
    name: Create Release
    needs: build
    runs-on: ubuntu-latest
    permissions:
      contents: write
    steps:
      - uses: actions/checkout@v4

      - name: Download all artifacts
        uses: actions/download-artifact@v4
        with:
          path: ./artifacts

      - name: Create Release
        id: create_release
        uses: actions/create-release@v1
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        with:
          tag_name: ${{ github.ref_name }}
          release_name: Release ${{ github.ref_name }}
          draft: false
          prerelease: false

      - name: Upload Release Assets
        run: |
          for asset_dir in ./artifacts/*; do
            if [ -d "$asset_dir" ]; then
              asset_name=$(basename "$asset_dir")
              asset_file=$(find "$asset_dir" -type f -name "scud*" | head -1)
              if [ -n "$asset_file" ]; then
                echo "Uploading $asset_file as $asset_name"
                cp "$asset_file" "./$asset_name"
                gh release upload ${{ github.ref_name }} "./$asset_name" --clobber
              fi
            fi
          done
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'
          registry-url: 'https://registry.npmjs.org'

      - name: Update package.json version
        run: |
          VERSION="${{ github.ref_name }}"
          VERSION="${VERSION#v}"
          CURRENT=$(node -p "require('./package.json').version")
          if [ "$CURRENT" != "$VERSION" ]; then
            npm version "$VERSION" --no-git-tag-version
          fi

      - name: Publish to npm
        run: npm publish --access public
        env:
          NODE_AUTH_TOKEN: ${{ secrets.NPM_TOKEN }}

      - name: Install Rust
        uses: actions-rs/toolchain@v1
        with:
          toolchain: stable
          override: true

      - name: Publish to crates.io
        working-directory: ./scud-cli
        run: cargo publish --token ${{ secrets.CARGO_TOKEN }}
</file>

<file path=".github/workflows/test.yml">
name: Test

on:
  push:
    branches: [ master, main ]
  pull_request:
    branches: [ master, main ]

env:
  CARGO_TERM_COLOR: always

jobs:
  test:
    name: Test on ${{ matrix.os }}
    runs-on: ${{ matrix.os }}
    strategy:
      matrix:
        os: [ubuntu-latest, macos-latest]
        rust: [stable]

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Install Rust
        uses: actions-rs/toolchain@v1
        with:
          toolchain: ${{ matrix.rust }}
          override: true
          components: rustfmt, clippy

      - name: Run tests
        run: |
          cd scud-cli
          cargo test --all-features

      - name: Run clippy
        run: |
          cd scud-cli
          cargo clippy --all-targets --all-features -- -D warnings

      - name: Check formatting
        run: |
          cd scud-cli
          cargo fmt -- --check

      - name: Build release binary
        run: |
          cd scud-cli
          cargo build --release

      - name: Test Node.js wrapper
        run: |
          cd scud-cli
          npm install
          ./bin/scud.js help
</file>

<file path=".opencode/command/task-claim.md">
---
description: Claim a SCUD task lock for parallel work
---

Claim a task to prevent conflicts during parallel work.

Run: `scud claim $ARGUMENTS`

Arguments: `<task-id> --name <name> [--tag <tag>]`

After claiming:
1. Confirm the lock is set
2. Remind to release when done: `scud release <id>`
</file>

<file path=".opencode/command/task-doctor.md">
---
description: Diagnose and fix SCUD task issues
---

Diagnose issues with tasks like stale locks, orphaned subtasks, or circular dependencies.

Run: `scud doctor $ARGUMENTS`

Arguments: `[--tag <tag>] [--stale-hours <n>] [--fix]`

Report findings:
- Stale locks (tasks locked for too long)
- Orphaned subtasks
- Circular dependencies
- Missing dependency targets

If `--fix` was used, confirm what was repaired.
</file>

<file path=".opencode/command/task-list.md">
---
description: List SCUD tasks with optional status filter
---

List tasks from the SCUD task graph.

Run: `scud list $ARGUMENTS`

After running, summarize:
- Total tasks shown
- Breakdown by status
- Any tasks that are blocked or stale
</file>

<file path=".opencode/command/task-next.md">
---
description: Find and optionally claim the next available SCUD task
---

Find the next available task based on dependencies and status.

Run: `scud next $ARGUMENTS`

Arguments: `[--claim --name <name>] [--tag <tag>]`

After finding the next task:
1. Show the task ID, title, and complexity
2. List its dependencies and their status
3. If `--claim` was used, confirm the task is now locked
4. Suggest the command to start working: `scud set-status <id> in-progress`
</file>

<file path=".opencode/command/task-release.md">
---
description: Release a SCUD task lock
---

Release a claimed task.

Run: `scud release $ARGUMENTS`

Arguments: `<task-id> [--force] [--tag <tag>]`

After releasing:
1. Confirm the lock is cleared
2. Show how long the task was locked
</file>

<file path=".opencode/command/task-show.md">
---
description: Show detailed information about a SCUD task
---

Show detailed information about a specific task.

Run: `scud show $ARGUMENTS`

Arguments: `<task-id> [--tag <tag>]`

Present the task details including:
- Title, status, complexity, priority
- Full description
- Test strategy (if defined)
- Dependencies and their current status
- Assignment and lock information
</file>

<file path=".opencode/command/task-stats.md">
---
description: Show SCUD task completion statistics
---

Show completion statistics for tasks.

Run: `scud stats $ARGUMENTS`

Arguments: `[--tag <tag>]`

Summarize:
- Overall progress percentage
- Tasks by status (pending, in-progress, done, blocked)
- Total complexity points completed vs remaining
- Highlight any blocked tasks that need attention
</file>

<file path=".opencode/command/task-status.md">
---
description: Update the status of a SCUD task
---

Update a task's status.

Run: `scud set-status $ARGUMENTS`

Arguments: `<task-id> <status> [--tag <tag>]`

Valid statuses: pending, in-progress, done, blocked, review, deferred, cancelled

After updating:
1. Confirm the status change
2. If marked `done`, suggest running `scud next` to find the next task
3. If marked `blocked`, ask what's blocking and whether to add a note
</file>

<file path=".opencode/command/task-tags.md">
---
description: List or set the active SCUD task tag
---

List all tags or set the active tag.

Run: `scud tags $ARGUMENTS`

Arguments: `[<tag>]`

If listing tags:
- Show all available tags
- Indicate which is currently active
- Show task count per tag if available

If setting a tag:
- Confirm the active tag changed
- Show quick stats for the new active tag
</file>

<file path=".opencode/command/task-waves.md">
---
description: Show parallel execution waves for SCUD tasks
---

Compute and display parallel execution waves based on task dependencies.

Run: `scud waves $ARGUMENTS`

Arguments: `[--tag <tag>] [--max-parallel <n>] [--all-tags]`

Explain the output:
1. Wave 1 tasks have no dependencies and can start immediately
2. Each subsequent wave depends on prior waves completing
3. Tasks within a wave can run in parallel
4. The speedup ratio shows efficiency vs sequential execution
</file>

<file path=".opencode/command/task-whois.md">
---
description: Show who is working on SCUD tasks
---

Show task assignments and locks.

Run: `scud whois $ARGUMENTS`

Arguments: `[--tag <tag>]`

Display:
- Which tasks are assigned to whom
- Which tasks are currently locked
- How long each lock has been held
- Flag any stale locks (>24 hours)
</file>

<file path=".opencode/hook/session-start.md">
---
event: session:start
---

# SCUD Skills Bootstrap

You have access to the **SCUD task management system** with skills that provide specialized knowledge and workflows.

## Available Skills

Use the `find_skills` tool to discover available skills, then `use_skill` to activate them.

### SCUD Task Management Skill

The `scud-tasks` skill provides comprehensive task management capabilities:

- **Task viewing**: `scud list`, `scud show`
- **Status updates**: `scud set-status`
- **Parallel work**: `scud claim`, `scud release`, `scud whois`
- **Finding work**: `scud next`, `scud waves`
- **Progress tracking**: `scud stats`, `scud tags`

### Tool Translation

When skills reference Claude Code tools, use OpenCode equivalents:
- `TodoWrite` â†’ `update_plan`
- `Task` tool with subagents â†’ Use OpenCode's subagent system (@mention)
- `Skill` tool â†’ `use_skill` custom tool
- `Read`, `Write`, `Edit`, `Bash` â†’ Your native tools

## Task Commands

These slash commands are available for task management:
- `/task-list` - List tasks with optional status filter
- `/task-next` - Find next available task
- `/task-show` - Show task details
- `/task-status` - Update task status
- `/task-claim` - Claim a task lock
- `/task-release` - Release a task lock
- `/task-waves` - Show parallel execution waves
- `/task-stats` - Show completion statistics
- `/task-whois` - Show who is working on tasks
- `/task-tags` - List or set active tag
- `/task-doctor` - Diagnose task issues

## SCUD Quick Start

To get started with SCUD:
1. Create or import tasks with `scud parse-prd` or manually
2. Use `/scud:task-next` to find available work
3. Claim tasks with `/scud:task-claim` to prevent conflicts
4. Update status with `/scud:task-status` as you progress
</file>

<file path=".opencode/skills/scud-sm.md">
# Scrum Master Skill

Invoke this skill when the user wants to:
- Translate PRD into SCUD tasks
- Break down features into manageable tasks
- Estimate task complexity
- Map task dependencies
- Parse PRD with proper SCUD tags

## How to Use
User says: "translate the PRD into tasks" or "break down the feature" or "scud-sm" or "parse into scud tasks"

## Skill Behavior
1. Validate workflow phase (must be planning)
2. Validate PRD and feature files exist
3. Load Scrum Master agent persona from: .claude/commands/scud/sm.md
4. Follow SCUD tag workflow:
   - Parse PRD with --tag
   - Use scud use-tag to switch tags
   - Analyze and refine tasks
   - Break down large tasks (>13 points)
   - Map dependencies

Reference the full agent documentation at: .claude/commands/scud/sm.md
</file>

<file path=".opencode/tool/find_skills.json">
{
  "name": "find_skills",
  "description": "Discover available skills in the project and user directories. Skills provide specialized knowledge and workflows for specific tasks.",
  "parameters": {
    "type": "object",
    "properties": {
      "query": {
        "type": "string",
        "description": "Optional search term to filter skills by name or description"
      }
    }
  },
  "command": "find ~/.opencode/skills .opencode/skills ~/.config/opencode/skills .claude/skills -name 'SKILL.md' 2>/dev/null | while read f; do dir=$(dirname \"$f\"); name=$(basename \"$dir\"); echo \"=== $name ===\"; head -20 \"$f\" | grep -E '^(name:|description:|#)' | head -5; echo; done"
}
</file>

<file path=".opencode/tool/use_skill.json">
{
  "name": "use_skill",
  "description": "Activate a skill by loading its SKILL.md file. This provides specialized knowledge and workflows for the given skill.",
  "parameters": {
    "type": "object",
    "properties": {
      "skill_name": {
        "type": "string",
        "description": "Name of the skill to activate (e.g., 'scud-tasks')"
      }
    },
    "required": ["skill_name"]
  },
  "command": "for dir in ~/.opencode/skills .opencode/skills ~/.config/opencode/skills .claude/skills; do if [ -f \"$dir/$1/SKILL.md\" ]; then echo \"=== Skill: $1 ===\"; cat \"$dir/$1/SKILL.md\"; exit 0; fi; done; echo \"Skill '$1' not found. Run find_skills to see available skills.\""
}
</file>

<file path=".opencode/scud-commands.md">
# SCUD CLI Commands - Quick Reference

**IMPORTANT: This reference should be included in all agent contexts for SCUD operations.**

---

## Tag Management

```bash
# List all tags
scud tags

# Create new tag and parse PRD into it
scud parse-prd docs/features/auth.md --tag auth

# Switch to work on specific tag
scud use-tag auth

# Add a new empty tag
scud add-tag todos --d="Todo CRUD operations"

# Copy existing tag to new tag
scud copy-tag auth auth-v2

# Rename a tag
scud rename-tag old-name new-name

# Delete a tag (with confirmation)
scud delete-tag old-tag
```

**Critical Note:** All task operations apply to the **currently active tag** only. Always verify which tag is active before operations.

---

## Task Viewing & Navigation

```bash
# List all tasks in active tag
scud list

# List tasks by status
scud list --status=pending
scud list --status=done
scud list --status=in-progress

# List tasks with subtasks
scud list --with-subtasks

# Show detailed task information
scud show 3

# Find next task to work on (considers dependencies)
scud next
```

---

## Task Status Management

```bash
# Update task status
scud set-status --id=3 --status=in-progress
scud set-status --id=3 --status=done
scud set-status --id=3 --status=review
scud set-status --id=3 --status=blocked

# Valid status values:
# - pending
# - in-progress
# - done
# - review
# - deferred
# - cancelled
# - blocked
```

---

## Dependency Management

```bash
# Add dependency (task 3 depends on task 1)
scud add-dependency --id=3 --depends-on=1

# Remove dependency
scud remove-dependency --id=3 --depends-on=1

# Validate all dependencies (check for issues)
scud validate-dependencies

# Fix invalid dependencies automatically
scud fix-dependencies
```

**Dependency Rules:**
- Cannot start task if dependencies not done
- Circular dependencies are invalid
- Subtask dependencies inherit from parent

---

## Task Creation & Modification

```bash
# Add new task using AI
scud add-task --prompt="Create login API endpoint" --priority=high

# Add task with dependencies
scud add-task --prompt="Add JWT middleware" --dependencies=3,4

# Remove a task
scud remove-task --id=5 -y

# Update task with new context
scud update-task --id=3 --prompt="Also needs rate limiting"

# Update multiple tasks from specific ID onwards
scud update --from=5 --prompt="All endpoints need CORS headers"
```

---

## Subtask Management

```bash
# Add subtask to parent task
scud add-subtask --parent=3 --title="Write unit tests" --description="Test all edge cases"

# Convert existing task to subtask
scud add-subtask --parent=3 --task-id=7

# Remove subtask
scud remove-subtask --id=3.1

# Remove subtask and convert to standalone task
scud remove-subtask --id=3.1 --convert

# Clear all subtasks from a task
scud clear-subtasks --id=3

# Clear all subtasks from all tasks
scud clear-subtasks --all
```

---

## Complexity Analysis & Task Breakdown

```bash
# Analyze all tasks for complexity
scud analyze-complexity

# Analyze with higher threshold (default: 5)
scud analyze-complexity --threshold=8

# Use research mode for deeper analysis
scud analyze-complexity --research

# View complexity report
scud complexity-report

# Expand single task into subtasks
scud expand --id=3 --num=5

# Expand with specific context
scud expand --id=3 --prompt="Focus on security concerns"

# Expand with research mode
scud expand --id=3 --research

# Expand all pending tasks
scud expand --all

# Force expand even if already has subtasks
scud expand --all --force
```

**Fibonacci Complexity Scale:**
- 1: Trivial (< 30 min)
- 2: Simple (30 min - 1 hour)
- 3: Moderate (1-2 hours)
- 5: Complex (2-4 hours)
- 8: Very Complex (4-8 hours)
- 13: Extremely Complex (1 day) - **SPLIT INTO SUBTASKS**

---

## AI Research & Context

```bash
# Perform research query
scud research "What is the best way to implement JWT auth?"

# Research with specific task context
scud research "Security best practices" -i=3,4,5

# Research with file context
scud research "How does this work?" -f=src/auth.js,src/middleware.js

# Research with additional context
scud research "Optimization strategies" -c="Focus on database queries"

# Save research output to file
scud research "API design patterns" -s=docs/research-api-patterns.md

# Display research as tree
scud research "System architecture" --tree

# Set detail level (1-5)
scud research "Implementation details" -d=3
```

---

## PRD Parsing & Task Generation

```bash
# Parse PRD into tasks (creates or updates tag)
scud parse-prd docs/features/auth.md --tag auth

# Generate with specific number of tasks
scud parse-prd docs/prd/product.md --num-tasks=15 --tag main-product

# Generate individual task files from tasks.scg
scud generate
```

**PRD Format Requirements:**
- Use markdown with clear sections
- Tasks should be under `## Tasks` heading
- Format: `### Task N: Title`
- Include Description, Complexity, Dependencies

---

## Export & Documentation

```bash
# Export tasks to README.md
scud sync-readme

# Export with subtasks
scud sync-readme --with-subtasks

# Export only specific status
scud sync-readme --status=pending
```

---

## Project Setup & Configuration

```bash
# Initialize new SCUD project
scud init

# Initialize with project details
scud init --name="My App" --description="Todo application" -y

# View AI model configuration
scud models

# Setup AI models interactively
scud models --setup

# Set main model
scud models --set-main claude-sonnet-4

# Set research model
scud models --set-research claude-opus-4

# Set fallback model
scud models --set-fallback gpt-4
```

---

## Common Workflows

### Starting New Feature
```bash
# 1. Parse PRD with tag
scud parse-prd docs/features/auth.md --tag auth

# 2. Verify it's active
scud tags

# 3. List tasks
scud list

# 4. Analyze complexity
scud analyze-complexity

# 5. Expand complex tasks (>13 points)
scud expand --id=5
```

### Working on Tasks
```bash
# 1. Find next available task
scud next

# 2. Start the task
scud set-status --id=3 --status=in-progress

# 3. View task details
scud show 3

# 4. Complete the task
scud set-status --id=3 --status=done
```

### Switching Between Tags
```bash
# 1. List all tags
scud tags

# 2. Switch to different tag
scud use-tag todos

# 3. Verify switch worked
scud list

# 4. Switch back
scud use-tag auth
```

### Breaking Down Complex Tasks
```bash
# 1. Identify complex tasks
scud analyze-complexity --threshold=13

# 2. View report
scud complexity-report

# 3. Expand the complex task
scud expand --id=5 --num=5

# 4. Verify subtasks created
scud show 5

# 5. Update dependencies if needed
scud add-dependency --id=5.2 --depends-on=5.1
```

---

## File Locations

```
.scud/
â”œâ”€â”€ tasks/
â”‚   â””â”€â”€ tasks.scg           # All tasks in SCG format
â”œâ”€â”€ config.toml             # AI model configuration and active tag
â””â”€â”€ task-files/             # Individual task files (if using generate)
```

---

## Environment Variables

Required in `.env`:
```bash
XAI_API_KEY=xai-...           # Default provider
# Alternative providers:
ANTHROPIC_API_KEY=sk-ant-...
OPENAI_API_KEY=sk-...
OPENROUTER_API_KEY=...
```

---

## Tips for Agents

1. **Always verify active tag** before task operations:
   ```bash
   scud tags  # Shows active tag with indicator
   ```

2. **Use `scud next`** to find tasks with met dependencies:
   ```bash
   scud next  # Returns task ID or "No tasks available"
   ```

3. **Check dependencies before starting** work:
   ```bash
   scud show 3  # Shows dependencies and their status
   ```

4. **Break down tasks >13 complexity**:
   ```bash
   scud expand --id=5 --num=5
   ```

5. **Use research mode** for complex planning:
   ```bash
   scud research "Best approach for..." -i=3
   ```

6. **Validate dependencies** before marking tag complete:
   ```bash
   scud validate-dependencies
   ```

---

## Error Prevention

- Start task without checking dependencies
- Change task status without verifying work complete
- Parse PRD without `--tag` flag
- Forget which tag is active
- Create tasks with complexity >13 without breaking down

- Always use tags for feature organization
- Validate dependencies regularly
- Check `scud next` for available tasks
- Expand complex tasks into subtasks
- Use research mode for complex decisions

---

**Last Updated:** 2025-12-02
**Version:** SCUD v1.17.0
</file>

<file path="docs/features/PARALLEL_FEATURES.md">
# SCUD Parallel Features (Experimental)

Enable parallel development with tag groups and task assignment for team collaboration.

---

## Overview

SCUD now supports two powerful features for parallel development:

1. **Tag Groups** - Coordinate related tags (e.g., backend/frontend) that share context
2. **Task Assignment** - Lock mechanism for multiple developers working on the same tag

---

## Tag Groups

### Concept

Tag groups allow you to work on multiple related tags simultaneously while maintaining coordination. Perfect for:

- **Backend + Frontend** split
- **Core + UI** feature development
- **API + Client** parallel work
- **Multiple workspaces/worktrees**

### Commands

#### Create a Group
```bash
scud create-group "User Authentication" --tags auth-backend,auth-frontend

# With description
scud create-group "Payment System" \
  --tags pay-backend,pay-frontend \
  --description "Complete payment processing implementation"
```

**Output:**
```
âœ… Tag group created!

Group ID:            user-authentication
Name:                User Authentication
Tags:                auth-backend, auth-frontend

Usage:
  scud group-status user-authentication
  scud list --group user-authentication
  scud stats --group user-authentication
```

#### List Groups
```bash
scud list-groups
```

**Output:**
```
Tag Groups:

â— User Authentication (user-authentication)
  Tags: auth-backend, auth-frontend
  Complete user auth system with API and UI

âœ“ Payment System (payment-system)
  Tags: pay-backend, pay-frontend
```

#### Group Status
```bash
scud group-status user-authentication
```

**Output:**
```
Group: User Authentication
==================================================

ID:                  user-authentication
Status:              Active

Tags in Group:
  auth-backend 12 tasks
  auth-frontend 8 tasks

Aggregate Statistics:
Total Tasks:         20
Pending:             10
In Progress:         5
Done:                5
Blocked:             0

Total Complexity:    65
Completion:          25%

[============                                      ]
```

#### Add Tag to Group
```bash
scud add-to-group user-authentication auth-mobile
```

---

## Workflow with Tag Groups

### Scenario: Backend/Frontend Split

```bash
# 1. Plan both features together
/tm-pm  # Create PRD

# 2. Create both feature files
# docs/features/auth-backend.md
# docs/features/auth-frontend.md

# 3. Parse both features
scud parse-prd docs/features/auth-backend.md --tag auth-backend
scud parse-prd docs/features/auth-frontend.md --tag auth-frontend

# 4. Create group
scud create-group "User Auth" --tags auth-backend,auth-frontend

# 5. Architect both together (coordinate API contracts)
/tm-architect  # While on backend tag
scud use-tag auth-frontend
/tm-architect  # While on frontend tag

# 6. Implement in parallel
# Developer A (Backend):
scud use-tag auth-backend
/tm-dev  # Work on backend tasks

# Developer B (Frontend):
scud use-tag auth-frontend
/tm-dev  # Work on frontend tasks

# Or use different worktrees:
git worktree add ../scud-backend auth-backend
git worktree add ../scud-frontend auth-frontend

# 7. Monitor overall progress
scud group-status user-auth
```

### Cross-Tag Coordination

**Backend Task** (API Endpoint):
```
auth-backend:5 | Build POST /api/auth/login endpoint | P | 5 | H
  Returns: { token: string, user: { id, email } }
```

**Frontend Task** (API Integration):
```
auth-frontend:3 | Integrate login API | P | 3 | H
  Expects: { token: string, user: { id, email } }
```

**Key:** Both tasks reference the same API contract, ensuring coordination.

---

## Task Assignment & Locking

### Concept

When multiple developers work on the same tag, task assignment prevents conflicts:

- **Claim tasks** to show you're working on them
- **Lock tasks** to prevent others from claiming
- **Track assignments** to see who's doing what
- **Auto-release** when tasks complete

### Commands

#### Assign a Task
```bash
scud assign 5 alice
```

**Output:**
```
âœ“ Task 5 assigned to alice
```

#### Claim a Task
```bash
scud claim 7 --name bob
```

**Output:**
```
âœ… Task claimed successfully!

Task ID:             7
Title:               Build login endpoint
Claimed by:          bob
Status:              locked

Next steps:
  1. Start working on the task
  2. Run: scud set-status 7 in-progress
  3. When done: scud set-status 7 done
  4. Task will auto-release when marked done
```

**What happens:**
- Task is assigned to bob
- Task is locked by bob
- Lock timestamp recorded
- No one else can claim it

#### Release a Task
```bash
# Release your own task
scud release 7

# Force release (if someone left)
scud release 7 --force
```

**Output:**
```
âš  Task is locked
Locked by:           bob
Locked:              2.5h ago

To force release: scud release 7 --force
```

#### Who's Working on What
```bash
scud whois
```

**Output:**
```
Task Assignments
============================================================

â— alice
  auth-backend 5 - Build registration endpoint
  auth-backend 8 - Add password hashing

â— bob
  auth-backend 7 - Build login endpoint
  auth-frontend 3 - Create login form

âš  Stale Locks (>24h)
============================================================

  auth-backend 12 locked by charlie (26.3h ago)

Consider releasing stale locks:
  scud release 12 --force
```

---

## Team Workflow

### Scenario: 3 Developers, 1 Tag

```bash
# Alice: Lead developer
cd project
scud use-tag auth
scud next              # Find next available task

Task 5: Build registration endpoint

scud claim 5 --name alice
scud set-status 5 in-progress
# ... implements task ...
scud set-status 5 done  # Auto-releases lock

# Bob: Second developer
cd project
scud use-tag auth
scud next              # Skips task 5 (locked by alice)

Task 7: Build login endpoint

scud claim 7 --name bob
scud set-status 7 in-progress

# Charlie: Third developer
cd project
scud use-tag auth
scud next

Task 9: Add email verification

scud claim 9 --name charlie
scud set-status 9 in-progress

# Team Lead: Check progress
scud whois

# Shows:
# â— alice - Task 5 (in progress)
# â— bob - Task 7 (in progress)
# â— charlie - Task 9 (in progress)

scud stats  # Overall tag progress
```

---

## Features & Safety

### Task Locking

**Claim Prevention:**
```bash
# Alice claims task 5
scud claim 5 --name alice  # âœ“ Success

# Bob tries to claim same task
scud claim 5 --name bob    # âœ— Error: Task is locked by alice
```

**Stale Lock Detection:**
- Locks >24h are flagged as stale
- Shown in `scud whois` output
- Can be force-released

**Auto-Release:**
```bash
scud set-status 5 done    # Auto-releases lock when done
```

### Tag Groups

**Aggregate Stats:**
- See total progress across all tags in group
- Identify bottlenecks
- Balance workload

**Coordinated Planning:**
- Architect can see all tags in group
- Ensure API contracts match
- Share dependencies

---

## File Structure

```
.scud/
â”œâ”€â”€ tasks/
â”‚   â””â”€â”€ tasks.scg               # Tasks with assigned_to, locked_by
â”œâ”€â”€ config.toml                 # Active tag and settings
â””â”€â”€ tag-groups.json             # Tag groups (NEW)
```

### tag-groups.json
```json
{
  "groups": [
    {
      "id": "user-authentication",
      "name": "User Authentication",
      "tags": [
        "auth-backend",
        "auth-frontend"
      ],
      "description": "Complete user auth system",
      "created_at": "2025-01-15T10:00:00Z",
      "status": "active"
    }
  ]
}
```

### Task with Assignment (in SCG format)
```
@nodes
auth:5 | Build registration endpoint | I | 5 | H

@meta
auth:5 | assigned_to | alice
auth:5 | locked_by | alice
auth:5 | locked_at | 2025-01-15T14:30:00Z
```

---

## Use Cases

### Use Case 1: Backend/Frontend Teams

```
Project: E-commerce checkout

Tag Group: "Checkout Flow"
- checkout-backend (Cart API, Payment API)
- checkout-frontend (Cart UI, Payment UI)

Team Backend: 2 devs
Team Frontend: 2 devs

Workflow:
1. PM creates single PRD
2. SM creates 2 feature files (backend, frontend)
3. Create group linking both
4. Architect designs both (API contracts)
5. Teams work in parallel
6. Monitor with group-status
```

### Use Case 2: Distributed Team

```
Project: User management system

Tag: "user-crud"
Developers:
- Alice (US, timezone UTC-8)
- Bob (Europe, timezone UTC+1)
- Charlie (Asia, timezone UTC+9)

Workflow:
1. All work on same tag
2. Each developer claims tasks
3. Use scud whois to avoid conflicts
4. Work asynchronously across timezones
5. Lock prevents accidental overlaps
```

### Use Case 3: Multiple Worktrees

```
Project: Mobile + Web app

Tag Group: "Dashboard Feature"
- dashboard-web
- dashboard-mobile

Setup:
git worktree add ../project-web dashboard-web
git worktree add ../project-mobile dashboard-mobile

Developer workflow:
# Terminal 1 (Web)
cd ../project-web
scud use-tag dashboard-web
/tm-dev

# Terminal 2 (Mobile)
cd ../project-mobile
scud use-tag dashboard-mobile
/tm-dev

# Monitor both
scud group-status dashboard-feature
```

---

## Best Practices

### Tag Groups

âœ… **Do:**
- Group tags that share context (API contracts, data models)
- Coordinate architecture phase across all tags in group
- Use group-status for overall progress monitoring
- Keep groups focused (2-4 tags max)

âŒ **Don't:**
- Create groups for unrelated tags
- Skip architecture coordination
- Ignore API contract mismatches
- Make huge groups (>5 tags)

### Task Assignment

âœ… **Do:**
- Claim tasks before starting work
- Release tasks if you step away
- Use `scud next` to find available tasks
- Check `scud whois` before claiming
- Set status to done when complete (auto-releases)

âŒ **Don't:**
- Force-release active locks (unless truly stale)
- Work on tasks without claiming
- Leave tasks locked overnight
- Claim multiple tasks simultaneously
- Forget to update task status

### Team Coordination

âœ… **Do:**
- Communicate with team about task choices
- Use task dependencies to sequence work
- Monitor stale locks regularly
- Review group-status in standups
- Document API contracts in architecture phase

âŒ **Don't:**
- Work in isolation without coordination
- Ignore dependencies
- Skip architecture phase
- Forget to update team on blockers

---

## Advanced: Git Worktrees

Perfect companion to tag groups!

### Setup
```bash
# Main repo
cd my-project
scud create-group "Feature X" --tags feature-x-backend,feature-x-frontend

# Create worktrees
git worktree add ../my-project-backend
git worktree add ../my-project-frontend

# Backend worktree
cd ../my-project-backend
git checkout -b feature-x-backend
scud use-tag feature-x-backend
/tm-dev  # Work on backend

# Frontend worktree
cd ../my-project-frontend
git checkout -b feature-x-frontend
scud use-tag feature-x-frontend
/tm-dev  # Work on frontend

# Monitor from anywhere
scud group-status feature-x
```

### Benefits
- Separate file trees
- No constant branch switching
- Parallel builds/tests
- IDE can run both
- Each worktree has own tag

---

## Limitations & Future

### Current Limitations

- **No real-time sync** - Tasks are locked in local files, not server-side
- **Manual conflict resolution** - If two devs claim same task offline
- **No notifications** - Won't alert when someone claims your task
- **Single active tag** - Each worktree can only have one active tag

### Planned Enhancements

- [ ] Lock server for real-time coordination
- [ ] Task notifications/webhooks
- [ ] Multi-tag view (work on multiple tags simultaneously)
- [ ] Cross-tag dependencies (task in tag A depends on task in tag B)
- [ ] Assignment rotation suggestions
- [ ] Workload balancing
- [ ] Time tracking integration
- [ ] Slack/Discord integration for whois

---

## Troubleshooting

### "Task is locked by someone else"

```bash
# Check who has it
scud whois

# If they're done, ask them to release
# Or force release if stale
scud release <task-id> --force
```

### "Tag not found in group"

```bash
# List all groups
scud list-groups

# Add tag to group
scud add-to-group <group-id> <tag>
```

### "Stale locks everywhere"

```bash
# See all assignments
scud whois

# Release stale locks
scud release <task-id> --force

# Or clean all (future feature)
# scud clean-locks --older-than 24h
```

---

## Summary

**Tag Groups:**
- Coordinate related tags
- Aggregate progress
- Perfect for backend/frontend splits
- Use with git worktrees

**Task Assignment:**
- Claim tasks to show you're working
- Lock prevents conflicts
- Auto-release on completion
- Monitor with `scud whois`

**Together:**
Enable teams to work in parallel efficiently while maintaining coordination and preventing conflicts.

**Experimental Status:**
These features are stable but marked experimental. Feedback welcome!

---

**Quick Reference:**

```bash
# Tag Groups
scud create-group "Name" --tags tag1,tag2
scud list-groups
scud group-status <group-id>
scud add-to-group <group-id> <tag>

# Task Assignment
scud assign <task-id> <assignee>
scud claim <task-id> --name <your-name>
scud release <task-id> [--force]
scud whois
```

**Happy parallel development!**
</file>

<file path="docs/guides/COMPLETE_GUIDE.md">
# SCUD Complete Guide

**Sprint Cycle Unified Development**

A lightweight, AI-powered workflow orchestration system for software development that combines task management with intelligent agent assistance.

---

## Table of Contents

1. [What is SCUD?](#what-is-scud)
2. [Core Concepts](#core-concepts)
3. [Installation & Setup](#installation--setup)
4. [Complete Workflow](#complete-workflow)
5. [Commands Reference](#commands-reference)
6. [Agent Guide](#agent-guide)
7. [Task Management](#task-management)
8. [Examples](#examples)
9. [Best Practices](#best-practices)
10. [Troubleshooting](#troubleshooting)

---

## What is SCUD?

SCUD (Sprint Cycle Unified Development) is a structured workflow system that guides you through software development projects using a proven 5-phase approach:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Ideation   â”‚  Define what to build (PRD)
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Planning   â”‚  Break into tasks
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚Architecture â”‚  Design technical solution
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚Implementationâ”‚  Execute tasks
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚Retrospectiveâ”‚  Learn and improve
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Key Features

âœ… **AI-Powered** - Leverages Claude for PRD parsing, complexity analysis, and task breakdown
âœ… **Structured Workflow** - 5-phase process ensures nothing is missed
âœ… **Fast** - Rust-based CLI with 50x faster performance
âœ… **Flexible** - Works with any tech stack, language, or framework
âœ… **Trackable** - Complete history and metrics for every epic
âœ… **Integrated** - Works seamlessly with Claude Code or any AI assistant

### Why SCUD?

Traditional development often lacks structure:
- âŒ Requirements are vague or missing
- âŒ Tasks are poorly defined
- âŒ Dependencies are unclear
- âŒ Progress is hard to track
- âŒ Lessons are forgotten

SCUD solves this by:
- âœ… Forcing clear requirements (PRD)
- âœ… Breaking work into manageable tasks
- âœ… Mapping dependencies automatically
- âœ… Tracking progress with metrics
- âœ… Capturing learnings in retrospectives

---

## Core Concepts

### Epic

An **epic** is a cohesive feature or set of related functionality. Examples:
- User authentication system
- Shopping cart functionality
- Admin dashboard
- Payment processing

Each epic has:
- Unique tag (e.g., `epic-1-auth`)
- Set of tasks
- Workflow state

### Task

A **task** is a discrete unit of work. Each task has:

```json
{
  "id": "1",
  "title": "Create User model",
  "description": "Implement User model with fields...",
  "status": "pending",
  "complexity": 3,
  "priority": "high",
  "dependencies": [],
  "details": "Technical implementation details...",
  "test_strategy": "How to test this task..."
}
```

**Task Statuses:**
- `pending` - Not started
- `in-progress` - Currently being worked on
- `done` - Completed
- `review` - Awaiting review
- `blocked` - Blocked by dependencies or issues
- `deferred` - Postponed to later
- `cancelled` - Not doing this

**Complexity Scale (Fibonacci):**
- `1` - Trivial (~30 min) - Update config, fix typo
- `2` - Simple (30m-1h) - Add basic validation
- `3` - Moderate (1-2h) - Create new API endpoint
- `5` - Complex (2-4h) - Integrate third-party service
- `8` - Very Complex (4-8h) - Build feature with multiple components
- `13` - Extremely Complex (1 day) - **SHOULD BE SPLIT**
- `21` - Too Large - **MUST BE SPLIT**

### Workflow Phases

SCUD enforces a sequential 5-phase workflow:

1. **Ideation** - Define the product (create PRD)
2. **Planning** - Break PRD into epics and tasks
3. **Architecture** - Design technical solution
4. **Implementation** - Build the tasks
5. **Retrospective** - Review and learn

Each phase must complete before moving to the next.

### Agents

SCUD uses **slash commands** to activate AI agents for each phase:

- `/tm-pm` - Product Manager (Ideation & Planning)
- `/tm-sm` - Scrum Master (Planning & Task Breakdown)
- `/tm-architect` - Architect (Technical Design)
- `/tm-dev` - Developer (Implementation)
- `/tm-retrospective` - Facilitator (Retrospective)
- `/status` - Status Reporter (anytime)

---

## Installation & Setup

### Prerequisites

- Node.js 16+ (for the wrapper)
- Rust & Cargo (for building the CLI)
- Anthropic API key (for AI commands)

### Installation

```bash
# Clone or install SCUD
npm install -g @eyaltoledano/scud

# Or for local development
git clone https://github.com/yourusername/scud
cd scud
npm install
npm link

# Set up API key for AI features
export ANTHROPIC_API_KEY=sk-ant-...
```

### Initialize a Project

```bash
# Navigate to your project directory
cd my-project

# Initialize SCUD
scud init

# This creates:
# .scud/
#   â”œâ”€â”€ tasks/tasks.json
#   â””â”€â”€ workflow-state.json
# docs/
#   â”œâ”€â”€ prd/
#   â”œâ”€â”€ epics/
#   â”œâ”€â”€ architecture/
#   â””â”€â”€ retrospectives/
# .claude/commands/  (if using Claude Code)
```

---

## Complete Workflow

### Phase 1: Ideation (Product Manager)

**Goal:** Define what you're building with a Product Requirements Document (PRD)

**Command:** `/tm-pm` (in Claude Code)

**Process:**
1. Agent asks discovery questions:
   - What problem are you solving?
   - Who are the users?
   - What are the key features?
   - What's out of scope?
2. Creates PRD: `docs/prd/[product-name]-prd.md`
3. Updates workflow to `planning` phase

**Example PRD Structure:**
```markdown
# Authentication System PRD

## Problem Statement
Users need secure access to the application.

## Goals
- Enable user registration
- Provide secure login
- Support password reset

## Target Users
- End users signing up for accounts
- Returning users logging in

## Features
1. Email/password registration
2. Login with session management
3. Password reset flow

## Out of Scope
- OAuth/social login (future)
- Multi-factor auth (future)
```

**Tips:**
- Be specific about requirements
- Define clear success criteria
- Explicitly state what's NOT included
- Consider user experience

---

### Phase 2: Planning (Product Manager + Scrum Master)

**Goal:** Break the PRD into epics and tasks

#### Part A: Create Epics (Product Manager)

**Command:** `/tm-pm` (again, but in planning mode)

**Process:**
1. Reviews PRD
2. Breaks into logical epics
3. Creates epic files: `docs/epics/[epic-name].md`

**Example Epic:**
```markdown
# Epic 1: User Registration

## User Stories
- As a new user, I want to sign up with email/password
- As a new user, I want email verification
- As a new user, I want clear error messages

## Acceptance Criteria
- Email must be valid format
- Password must meet security requirements (8+ chars, etc.)
- Duplicate emails are rejected
- Verification email is sent

## Technical Considerations
- Need User model in database
- Need email service integration
- Need password hashing
```

#### Part B: Parse into Tasks (Scrum Master)

**Command:** `scud parse-prd docs/epics/epic-1-registration.md --tag epic-1-reg`

Or use `/tm-sm` in Claude Code

**What Happens:**
1. AI reads epic markdown
2. Extracts discrete tasks
3. Assigns initial complexity scores
4. Identifies dependencies
5. Creates tasks in `.scud/tasks/tasks.json`

**Review and Refine:**
```bash
# List tasks
scud list

# Review specific task
scud show 1

# Analyze complexity with AI
scud analyze-complexity

# Expand tasks >13 complexity
scud expand --all
```

**Scrum Master Agent (`/tm-sm`) will:**
- Review all tasks
- Adjust complexity scores
- Break down large tasks (>13)
- Map dependencies
- Set priorities

**Example Task Breakdown:**

Before expansion:
```
Task 3: Implement user authentication [21]
```

After expansion:
```
Task 3: [PARENT] Implement user authentication
Task 8: Create User model [3]
Task 9: Add password hashing [3]
Task 10: Build registration endpoint [5]
Task 11: Build login endpoint [5]
Task 12: Implement session management [5]
Task 13: Add authentication middleware [3]
```

---

### Phase 3: Architecture (Architect)

**Goal:** Design the technical solution

**Command:** `/tm-architect`

**Process:**
1. Reviews all tasks in epic
2. Creates architecture document: `docs/architecture/[epic-tag]-architecture.md`
3. Adds technical details to each task:
   - Implementation approach
   - Files to create/modify
   - Libraries/frameworks needed
   - Data structures
   - API contracts
4. Sets task dependencies based on technical requirements
5. Validates no task has complexity >13

**Architecture Document Example:**
```markdown
# Epic 1: Registration - Architecture

## System Design

### Database Schema
```sql
CREATE TABLE users (
  id UUID PRIMARY KEY,
  email VARCHAR(255) UNIQUE NOT NULL,
  password_hash VARCHAR(255) NOT NULL,
  email_verified BOOLEAN DEFAULT FALSE,
  created_at TIMESTAMP DEFAULT NOW()
);
```

### API Endpoints
- POST /api/auth/register
- POST /api/auth/verify-email
- POST /api/auth/login

### Components
- UserModel (models/User.js)
- AuthService (services/auth.js)
- PasswordUtils (utils/password.js)
- EmailService (services/email.js)

### Tech Stack
- Node.js + Express
- PostgreSQL
- bcrypt for password hashing
- jsonwebtoken for sessions
- nodemailer for emails
```

**Task Enhancement Example:**

Before:
```json
{
  "id": "8",
  "title": "Create User model",
  "description": "Implement User model",
  "complexity": 3
}
```

After Architect:
```json
{
  "id": "8",
  "title": "Create User model",
  "description": "Implement User model with validation",
  "complexity": 3,
  "details": "Create models/User.js with:\n- Schema: id, email, password_hash, email_verified, created_at\n- Validation: email format, password requirements\n- Methods: comparePassword(), generateToken()\n- Use Sequelize ORM\n- Migration: migrations/001-create-users.js",
  "test_strategy": "Unit tests:\n- Valid user creation\n- Email validation\n- Password hashing\n- Duplicate email rejection"
}
```

**Architect Updates Workflow:**
- Marks architecture phase complete
- Advances to `implementation` phase

---

### Phase 4: Implementation (Developer)

**Goal:** Execute the tasks

**Command:** `/tm-dev`

**Process:**

1. **Start Development Session**
   ```bash
   # Developer agent checks:
   # - Workflow phase is 'implementation'
   # - Architecture is complete
   # - Active epic is set
   ```

2. **Find Next Task**
   ```bash
   scud next

   # Returns:
   # Next Available Task:
   #
   # ID:          8
   # Title:       Create User model
   # Complexity:  3
   # Priority:    high
   #
   # Description: Implement User model with validation
   #
   # Technical Details:
   # Create models/User.js with:
   # - Schema: id, email, password_hash...
   #
   # Test Strategy:
   # Unit tests for validation...
   ```

3. **Start Task**
   ```bash
   scud set-status 8 in-progress
   ```

4. **Implement Task**
   - Developer agent implements the code
   - Follows technical details
   - Ensures test strategy is followed

5. **Test Task**
   - Must write and run tests
   - All tests must pass
   - Agent validates this before marking done

6. **Complete Task**
   ```bash
   scud set-status 8 done
   ```

7. **Repeat** until all tasks are done

**Developer Agent Behavior:**

```markdown
## Developer Agent Rules

1. ALWAYS check workflow phase = implementation
2. ALWAYS use `scud next` to find tasks
3. ALWAYS validate dependencies are met
4. MUST write tests (enforced by test_strategy)
5. MUST run tests before marking done
6. CANNOT mark task done if tests fail
7. Shows progress regularly
```

**Example Developer Session:**

```
User: /tm-dev

Agent: Checking workflow status...
       Phase: implementation âœ“
       Active epic: epic-1-reg

       Finding next task...

       Next Task: #8 - Create User model
       Complexity: 3 | Priority: high

       I'll start working on this task.

       Setting status to in-progress...

       Creating models/User.js...
       [Implements code following details]

       Writing tests...
       [Implements tests following test_strategy]

       Running tests...
       âœ“ All tests pass

       Marking task as done.

       Progress: 1/10 tasks complete (10%)
```

**Track Progress:**
```bash
# See all tasks and their status
scud list

# See statistics
scud stats

# Epic Statistics: epic-1-reg
# =================
# Total Tasks:      10
# Pending:          5
# In Progress:      1
# Done:             4
# Blocked:          0
#
# Total Complexity: 35
# Completion:       40%
#
# [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                              ]
```

---

### Phase 5: Retrospective (Facilitator)

**Goal:** Review the epic and capture learnings

**Command:** `/tm-retrospective`

**Triggers:**
- All tasks must be in `done` status
- Implementation phase is complete

**Process:**

1. **Gather Metrics**
   - Tasks completed
   - Total complexity
   - Time taken (if tracked)
   - Blockers encountered

2. **Reflection Questions**
   Agent asks:
   - What went well?
   - What could be improved?
   - What was surprising?
   - What did you learn?
   - What would you do differently?

3. **Create Retrospective Document**
   `docs/retrospectives/[epic-tag]-retrospective.md`

**Example Retrospective:**
```markdown
# Epic 1: User Registration - Retrospective

## Metrics
- **Tasks Completed:** 10
- **Total Complexity:** 35 points
- **Duration:** 3 days
- **Tasks Expanded:** 2 (complexity >13)

## What Went Well
- Clear architecture document saved time
- Test-first approach caught bugs early
- Password validation prevented security issues
- Email service integration was smooth

## What Could Be Improved
- Initial complexity estimates were too low
- Task 3 should have been split from the start
- Needed better error handling patterns
- Should have mocked email service for tests

## Learnings
- bcrypt rounds: 10 is good balance of security/speed
- JWT expiry: 24h for web, 30d for mobile
- Email verification: 24h expiry on tokens
- Rate limiting: needed on auth endpoints

## Action Items for Next Epic
- [ ] Create error handling utilities first
- [ ] Set up test mocking infrastructure
- [ ] Be more aggressive splitting complex tasks
- [ ] Add rate limiting to architecture phase

## Pattern Library Additions
- Auth middleware pattern
- Password validation regex
- JWT token generation
- Email template structure
```

4. **Archive Epic**
   - Moves epic to `completed_epics` in workflow-state.json
   - Resets to `ideation` phase for next epic

5. **Ready for Next Epic**
   ```
   âœ… Epic 1 completed!

   Next steps:
   1. Run /tm-pm to start next epic
   2. Or review metrics and patterns
   ```

---

## Commands Reference

### Setup Commands

#### `scud init`
Initialize SCUD in current directory.

```bash
scud init
```

Creates:
- `.scud/` directory structure
- `docs/` directories
- `workflow-state.json`
- `.gitignore` entry

#### `scud status`
Show current workflow status.

```bash
scud status
```

Shows:
- Current workflow phase
- Active epic
- Available slash commands
- Next steps

---

### Task Management Commands

#### `scud tags`
List all epic tags.

```bash
scud tags

# Output:
# Epic Tags:
#   â— epic-1-reg (10 tasks)    # Active
#   â—‹ epic-2-login (5 tasks)
```

#### `scud use-tag <tag>`
Switch to a different epic.

```bash
scud use-tag epic-2-login

# Output:
# âœ“ Active epic set to: epic-2-login
#   Tasks: 5
#   Pending: 5
#   In Progress: 0
#   Done: 0
```

#### `scud list [--status <status>]`
List tasks in active epic.

```bash
# All tasks
scud list

# Output:
# Tasks in epic: epic-1-reg
#
# 1    done            Create User model [3]
# 2    done            Add password hashing [3]
# 3    in-progress     Build registration endpoint [5]
# 4    pending         Build login endpoint [5]
# 5    pending         Implement session management [5]

# Filter by status
scud list --status pending
scud list --status done
scud list --status in-progress
```

#### `scud show <id>`
Show detailed task information.

```bash
scud show 3

# Output:
# Task Details
# =============
# ID:                  3
# Title:               Build registration endpoint
# Status:              in-progress
# Complexity:          5
# Priority:            high
# Dependencies:        [1, 2]
#
# Description:
# Create POST /api/auth/register endpoint...
#
# Technical Details:
# - Route: routes/auth.js
# - Controller: controllers/authController.js
# - Validation: joi schema
# - Response: 201 + JWT token
#
# Test Strategy:
# - Valid registration
# - Duplicate email
# - Invalid email format
# - Weak password
```

#### `scud set-status <id> <status>`
Update task status.

```bash
scud set-status 3 in-progress
scud set-status 3 done
scud set-status 3 blocked

# Valid statuses:
# pending, in-progress, done, review, blocked, deferred, cancelled
```

#### `scud next`
Find next available task with dependencies met.

```bash
scud next

# Finds first pending task where all dependencies are done
# Shows full task details
# Suggests: scud set-status <id> in-progress
```

#### `scud stats`
Show epic statistics.

```bash
scud stats

# Output:
# Epic Statistics: epic-1-reg
# =================
#
# Total Tasks:      10
# Pending:          3
# In Progress:      1
# Done:             6
# Blocked:          0
#
# Total Complexity: 35
# Completion:       60%
#
# [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                    ]
```

---

### AI-Powered Commands

Require `ANTHROPIC_API_KEY` environment variable.

#### `scud parse-prd <file> --tag <tag>`
Parse PRD/epic markdown into tasks.

```bash
scud parse-prd docs/epics/user-registration.md --tag epic-1-reg

# What it does:
# 1. Reads markdown file
# 2. Sends to Claude for parsing
# 3. Creates structured tasks
# 4. Assigns complexity scores
# 5. Identifies dependencies
# 6. Sets active epic
```

**Example Input** (`docs/epics/user-registration.md`):
```markdown
# User Registration

## Features
- Email/password signup
- Email verification
- Password strength validation

## Requirements
- Unique emails
- Secure password storage
- Send verification email
```

**Example Output:**
```
âœ“ Parsed 5 tasks

âœ… Epic parsed and created successfully!

Tag:                 epic-1-reg
Tasks created:       5

Next steps:
  1. Review tasks: scud list
  2. Analyze complexity: scud analyze-complexity
  3. Use /tm-architect to add technical details
```

#### `scud analyze-complexity [--task <id>]`
Analyze and score task complexity.

```bash
# Analyze all tasks
scud analyze-complexity

# Analyze specific task
scud analyze-complexity --task 5

# What it does:
# 1. Examines task title, description, details
# 2. Considers technical difficulty, unknowns, testing
# 3. Assigns Fibonacci complexity score
# 4. Provides reasoning
# 5. Flags tasks >13 for expansion
```

**Example Output:**
```
Analyzing complexity for 10 task(s)...

âœ“ Task 1: Create User model â†’ complexity 3
âœ“ Task 2: Add password hashing â†’ complexity 2
âœ“ Task 3: Build registration endpoint â†’ complexity 5
âš  Task 4: Implement full auth system â†’ complexity 21
  âš  Task complexity >13. Consider running: scud expand 4
...

âœ… Complexity analysis complete!

Total complexity:     45

âš  1 task(s) with complexity >13:
  4 Implement full auth system [21]

Run: scud expand --all
```

#### `scud expand <id>` or `scud expand --all`
Break down complex tasks into subtasks.

```bash
# Expand specific task
scud expand 4

# Expand all tasks with complexity >13
scud expand --all

# What it does:
# 1. Identifies tasks with complexity >13
# 2. Sends to Claude for breakdown
# 3. Creates subtasks (each â‰¤8 complexity)
# 4. Sets dependencies between subtasks
# 5. Marks original as [PARENT]
```

**Example:**

Before:
```
4    pending    Implement full auth system [21]
```

After:
```
4    pending    [PARENT] Implement full auth system [21]
11   pending    Create User model [3]
12   pending    Add password hashing [3]
13   pending    Build registration endpoint [5]
14   pending    Build login endpoint [5]
15   pending    Add session management [5]
```

#### `scud research "<query>"`
AI-powered topic research.

```bash
scud research "OAuth 2.0 best practices"
scud research "PostgreSQL connection pooling"
scud research "JWT vs session tokens"

# What it does:
# 1. Sends query to Claude
# 2. Gets comprehensive response
# 3. Displays formatted result
# 4. Includes best practices, pitfalls, examples
```

**Example Output:**
```
Research Results
================
Query: OAuth 2.0 best practices

## Key Concepts

OAuth 2.0 is an authorization framework that enables applications
to obtain limited access to user accounts...

## Best Practices

1. **Always use HTTPS** - OAuth requires secure communication
2. **Validate redirect URIs** - Prevent authorization code interception
3. **Use PKCE** - Proof Key for Code Exchange for mobile apps
4. **Short-lived access tokens** - 1 hour or less
5. **Rotate refresh tokens** - Issue new refresh token on each use

## Common Pitfalls

- âŒ Storing tokens in localStorage (use httpOnly cookies)
- âŒ Not validating state parameter (CSRF vulnerability)
- âŒ Using implicit flow (deprecated, use auth code + PKCE)

## Code Example

```javascript
// Authorization request
const authUrl = `${OAUTH_ENDPOINT}/authorize?
  client_id=${CLIENT_ID}&
  redirect_uri=${REDIRECT_URI}&
  response_type=code&
  scope=read:user&
  state=${randomState}`;
```

## Resources
- RFC 6749: OAuth 2.0 Framework
- OAuth 2.0 Security Best Current Practice
```

---

## Agent Guide

### When to Use Each Agent

| Agent | Phase | Command | Purpose |
|-------|-------|---------|---------|
| **Product Manager** | Ideation | `/tm-pm` | Create PRD, define product |
| **Product Manager** | Planning | `/tm-pm` | Break PRD into epics |
| **Scrum Master** | Planning | `/tm-sm` | Parse epics into tasks, manage breakdown |
| **Architect** | Architecture | `/tm-architect` | Design technical solution |
| **Developer** | Implementation | `/tm-dev` | Execute tasks |
| **Facilitator** | Retrospective | `/tm-retrospective` | Review and learn |
| **Status Reporter** | Any | `/status` | Check current state |

### Agent Behaviors

#### Product Manager (`/tm-pm`)

**Ideation Phase:**
- Asks discovery questions
- Creates comprehensive PRD
- Defines scope and out-of-scope
- Identifies user personas
- Sets success criteria

**Planning Phase:**
- Reviews PRD
- Breaks into epics (3-7 is ideal)
- Creates epic markdown files
- Ensures each epic is cohesive

**Key Principle:** Clear requirements prevent rework

#### Scrum Master (`/tm-sm`)

**Responsibilities:**
- Parse epics into discrete tasks
- Analyze task complexity
- Break down tasks >13 complexity
- Map task dependencies
- Ensure all tasks are actionable
- Balance workload

**Complexity Guidelines:**
- 1-8: Good to go
- 13: Should split (full day)
- 21: Must split (too large)

**Dependency Rules:**
- Foundational tasks first (models, schemas)
- Then business logic
- Then API/UI layers
- Finally tests and docs

**Key Principle:** Small, testable tasks reduce risk

#### Architect (`/tm-architect`)

**Responsibilities:**
- Create architecture document
- Design system components
- Choose technology stack
- Define data structures
- Specify API contracts
- Add technical details to ALL tasks
- Set technical dependencies
- Validate task complexity

**Architecture Document Sections:**
- System design overview
- Database schema
- API endpoints
- Component structure
- Tech stack decisions
- Security considerations
- Performance considerations

**Task Enhancement:**
- Implementation approach
- Files to create/modify
- Libraries/dependencies needed
- Data structures
- API/interface contracts

**Key Principle:** Good architecture prevents technical debt

#### Developer (`/tm-dev`)

**Responsibilities:**
- Execute tasks in order
- Follow technical details
- Implement test strategy
- Validate dependencies
- Track progress
- Cannot skip tests

**Workflow:**
1. Check phase = implementation
2. Find next task (`scud next`)
3. Validate dependencies met
4. Set status to in-progress
5. Implement following details
6. Write tests per test_strategy
7. Run all tests
8. Mark done (only if tests pass)
9. Repeat

**Hard Rules:**
- âœ… Must follow technical details
- âœ… Must write tests
- âœ… Must run tests
- âœ… Tests must pass before done
- âŒ Cannot mark done without tests
- âŒ Cannot skip dependencies

**Key Principle:** Test-driven implementation ensures quality

#### Facilitator (`/tm-retrospective`)

**Responsibilities:**
- Gather metrics
- Facilitate reflection
- Capture learnings
- Document patterns
- Create action items
- Archive epic

**Reflection Areas:**
- What went well
- What could improve
- Surprises/learnings
- Technical insights
- Process improvements

**Deliverables:**
- Retrospective document
- Pattern library updates
- Action items for next epic
- Archived epic with metrics

**Key Principle:** Continuous improvement through reflection

---

## Task Management

### Task Lifecycle

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ pending â”‚  Created, not started
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
     â”‚ scud set-status X in-progress
     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ in-progress â”‚  Currently being worked on
â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
      â”‚ scud set-status X done
      â†“
â”Œâ”€â”€â”€â”€â”€â”€â”
â”‚ done â”‚  Completed and tested
â””â”€â”€â”€â”€â”€â”€â”˜
```

**Alternative Paths:**
- `pending` â†’ `blocked` (dependencies not met, issues)
- `in-progress` â†’ `review` (needs code review)
- `pending` â†’ `deferred` (postponed)
- `pending` â†’ `cancelled` (not doing)

### Dependencies

**How They Work:**
```json
{
  "id": "5",
  "title": "Build login endpoint",
  "dependencies": ["1", "2", "3"]
}
```

Task 5 cannot start until tasks 1, 2, and 3 are `done`.

**Finding Next Task:**
```bash
scud next
```

Returns first `pending` task with all dependencies `done`.

**Dependency Graph Example:**
```
Task 1: Create User model
  â†“
Task 2: Add password hashing (depends on 1)
  â†“
Task 3: Build registration endpoint (depends on 1, 2)
  â†“
Task 4: Build login endpoint (depends on 1, 2)
  â†“
Task 5: Add auth middleware (depends on 3, 4)
```

### Complexity Management

**Fibonacci Scale:**
- 1, 2, 3, 5, 8, 13, 21

**Rules:**
- Tasks â‰¤8: Good to implement
- Tasks = 13: Should split (1 full day)
- Tasks â‰¥21: Must split (too large)

**Splitting Process:**
```bash
# Identify large tasks
scud analyze-complexity

# Split them
scud expand --all

# Or specific task
scud expand 7
```

**Example Split:**

Original (21 points):
```
Implement OAuth integration
- Set up OAuth provider
- Handle authorization flow
- Store tokens securely
- Refresh token logic
- Error handling
```

Split into subtasks (total: 22 points):
```
1. Configure OAuth provider settings [3]
2. Build authorization redirect endpoint [5]
3. Implement callback handler [5]
4. Add token storage [3]
5. Implement token refresh [5]
6. Add comprehensive error handling [3]
```

Each subtask â‰¤8 points = manageable

### Priority Management

**Priority Levels:**
- `high` - Critical path, blockers
- `medium` - Normal work
- `low` - Nice to have, improvements

**Not a Substitute for Dependencies:**
- Use dependencies for technical order
- Use priority for business importance

**Example:**
```json
{
  "id": "10",
  "title": "Add rate limiting",
  "priority": "high",      // Important for security
  "dependencies": ["3", "4"], // But must wait for endpoints
  "complexity": 3
}
```

---

## Examples

### Example 1: Building a Todo App

#### Step 1: Initialize
```bash
cd todo-app
scud init
```

#### Step 2: Create PRD
```
Use: /tm-pm

Agent creates: docs/prd/todo-app-prd.md

# Todo App PRD

## Goals
Simple task management app for personal use

## Features
- Create/read/update/delete todos
- Mark complete/incomplete
- Filter by status
- Persist to database

## Out of Scope
- User accounts (v2)
- Sharing (v2)
- Mobile app (v2)
```

#### Step 3: Break into Epics
```
Use: /tm-pm (planning mode)

Agent creates:
- docs/epics/epic-1-backend.md
- docs/epics/epic-2-frontend.md
```

#### Step 4: Parse First Epic
```bash
scud parse-prd docs/epics/epic-1-backend.md --tag epic-1-backend
```

Creates tasks:
```
1. Set up Express server [2]
2. Create Todo model [3]
3. Build CRUD API endpoints [5]
4. Add validation [2]
5. Set up database [3]
6. Add error handling [2]
```

#### Step 5: Analyze Complexity
```bash
scud analyze-complexity
```

All tasks â‰¤5, good to proceed.

#### Step 6: Architecture
```
Use: /tm-architect

Creates: docs/architecture/epic-1-backend-architecture.md

Adds technical details to all tasks:
- Task 2 gets schema definition
- Task 3 gets API contract
- etc.
```

#### Step 7: Implementation
```
Use: /tm-dev

Agent workflow:
1. scud next â†’ Task 5 (no dependencies)
2. Set up PostgreSQL database
3. Write tests
4. Mark done

5. scud next â†’ Task 1 (depends on 5, now done)
6. Create Express server
7. Write tests
8. Mark done

... continues for all tasks ...
```

#### Step 8: Retrospective
```
Use: /tm-retrospective

Creates: docs/retrospectives/epic-1-backend-retrospective.md

Learnings:
- Validation library choice was good
- Should have added logging earlier
- Test setup took longer than expected
```

#### Step 9: Second Epic
```
Use: /tm-pm to start epic-2-frontend
```

### Example 2: Adding Authentication to Existing App

#### Starting State
App exists, adding auth feature.

#### Step 1: Initialize SCUD
```bash
scud init
```

#### Step 2: Create Auth PRD
```
Use: /tm-pm

Focus: Just authentication feature

PRD covers:
- Login/logout
- JWT tokens
- Password hashing
- Protected routes
```

#### Step 3: Single Epic
```
Use: /tm-pm (planning)

Creates: docs/epics/epic-1-auth.md
```

#### Step 4: Parse and Refine
```bash
scud parse-prd docs/epics/epic-1-auth.md --tag epic-1-auth

scud list
# Review tasks

scud analyze-complexity
# Task 7 is complexity 21!

scud expand 7
# Splits into 5 subtasks
```

#### Step 5: Architecture
```
Use: /tm-architect

Decisions:
- bcrypt for hashing
- jsonwebtoken for JWTs
- Express middleware for auth
- Add auth field to User model

Updates all tasks with implementation details
```

#### Step 6: Implement
```
Use: /tm-dev

20 tasks implemented over 2 days
All with tests
Progress tracked with scud stats
```

#### Step 7: Learn
```
Use: /tm-retrospective

Key learning: Rate limiting needed
Action: Add to next epic
```

---

## Best Practices

### 1. PRD Quality

âœ… **Do:**
- Be specific about requirements
- Define clear success criteria
- Explicitly state what's out of scope
- Include user personas
- Describe user flows

âŒ **Don't:**
- Be vague ("make it better")
- Mix features from different areas
- Skip the "why"
- Forget edge cases

### 2. Epic Sizing

âœ… **Do:**
- Keep epics cohesive (one feature area)
- Aim for 3-7 epics per project
- Size for 1-2 weeks of work
- Ensure epics can be independent

âŒ **Don't:**
- Create massive epics (>50 tasks)
- Mix unrelated features
- Create dependencies between epics
- Make epics too granular

### 3. Task Breakdown

âœ… **Do:**
- Keep tasks â‰¤8 complexity
- Make tasks independently testable
- Map clear dependencies
- Write descriptive titles
- Include acceptance criteria

âŒ **Don't:**
- Create huge tasks (>13 complexity)
- Make tasks dependent on everything
- Write vague descriptions
- Skip test strategy
- Ignore dependencies

### 4. Architecture Phase

âœ… **Do:**
- Create comprehensive architecture doc
- Add technical details to EVERY task
- Specify files to create/modify
- Define data structures
- Document API contracts
- Consider security/performance

âŒ **Don't:**
- Skip architecture ("we'll figure it out")
- Leave tasks without details
- Ignore technical dependencies
- Forget about testing approach
- Rush through this phase

### 5. Implementation

âœ… **Do:**
- Always use `scud next`
- Validate dependencies
- Follow technical details
- Write tests FIRST
- Run tests before marking done
- Track progress regularly

âŒ **Don't:**
- Pick random tasks
- Ignore dependencies
- Skip tests
- Mark done without testing
- Deviate from architecture
- Work on multiple tasks simultaneously

### 6. Retrospectives

âœ… **Do:**
- Be honest about what went wrong
- Capture specific learnings
- Document patterns discovered
- Create actionable improvements
- Review metrics

âŒ **Don't:**
- Skip retrospectives
- Only focus on positives
- Make vague observations
- Forget to act on learnings
- Ignore the data

### 7. Workflow Discipline

âœ… **Do:**
- Complete each phase before moving on
- Follow the agent sequence
- Trust the process
- Update task status promptly
- Keep documentation current

âŒ **Don't:**
- Skip phases
- Jump around randomly
- Ignore phase gates
- Let task status get stale
- Forget to update docs

---

## Troubleshooting

### "Task file not found"

**Problem:** `.scud/tasks/tasks.json` doesn't exist

**Solution:**
```bash
scud init
```

### "No active epic"

**Problem:** Haven't set which epic to work on

**Solution:**
```bash
# List available epics
scud tags

# Set active epic
scud use-tag epic-1-auth
```

### "Wrong phase for this command"

**Problem:** Trying to use an agent in the wrong phase

**Solution:**
```bash
# Check current phase
scud status

# Use correct agent:
# ideation â†’ /tm-pm
# planning â†’ /tm-pm then /tm-sm
# architecture â†’ /tm-architect
# implementation â†’ /tm-dev
# retrospective â†’ /tm-retrospective
```

### "Dependencies not met"

**Problem:** Trying to start a task that depends on incomplete tasks

**Solution:**
```bash
# See task dependencies
scud show <task-id>

# Use next to find available tasks
scud next

# Complete dependencies first
```

### "ANTHROPIC_API_KEY not set"

**Problem:** AI commands need API key

**Solution:**
```bash
export ANTHROPIC_API_KEY=sk-ant-...

# Or add to ~/.bashrc or ~/.zshrc:
echo 'export ANTHROPIC_API_KEY=sk-ant-...' >> ~/.bashrc
```

### "Rust binary not found"

**Problem:** Rust CLI not built

**Solution:**
```bash
cd scud-cli
cargo build --release

# Or it will auto-build on first use
```

### "Task complexity too high"

**Problem:** Tasks >13 should be split

**Solution:**
```bash
# Analyze to find them
scud analyze-complexity

# Expand large tasks
scud expand --all

# Or specific task
scud expand <task-id>
```

### "No tasks need expansion"

**Message:** All tasks â‰¤13 complexity

**This is good!** Proceed with architecture phase.

---

## Advanced Topics

### Multi-Epic Projects

**Scenario:** Large project with multiple feature areas

**Approach:**
1. Create comprehensive PRD covering entire project
2. Break into multiple epics (one per feature area)
3. Parse each epic separately with different tags:
   ```bash
   scud parse-prd docs/epics/auth.md --tag epic-1-auth
   scud parse-prd docs/epics/payments.md --tag epic-2-pay
   scud parse-prd docs/epics/admin.md --tag epic-3-admin
   ```
4. Work on one epic at a time
5. Switch between epics:
   ```bash
   scud use-tag epic-2-pay
   ```

### Custom Workflow

**SCUD is flexible.** You can adapt it:

**Option 1: Skip Architecture**
- Not recommended, but possible
- Manually update workflow-state.json phase
- Add technical details yourself in task descriptions

**Option 2: Combined Phases**
- Do Planning + Architecture together
- Have Architect also refine tasks

**Option 3: Iterative**
- Do Ideation â†’ Planning â†’ Arch â†’ Impl â†’ Retro for each epic
- Treat each epic as mini-project

**Best Practice:** Follow the standard workflow first. Customize only after you understand it.

### Integration with Existing Tools

**GitHub Issues:**
- Export tasks to issues
- Link SCUD tasks to issue numbers
- Use SCUD for planning, GitHub for tracking

**Jira:**
- Similar to GitHub integration
- SCUD for structured breakdown
- Jira for team visibility

**Linear:**
- SCUD tasks â†’ Linear tasks
- Maintain sync

### Team Usage

**SCUD is designed for solo or small teams.**

**For Teams:**
- One person runs SCUD agents
- Export tasks to team tools
- Use retrospectives for team learning
- Share architecture docs

**Not Recommended:**
- Multiple people editing tasks.json simultaneously
- Different team members in different phases

---

## FAQ

### Q: Do I need to use all 5 phases?
**A:** Yes, for best results. Each phase builds on the previous. Skipping phases leads to problems.

### Q: Can I use SCUD with any programming language?
**A:** Yes! SCUD is language-agnostic. It works with any tech stack.

### Q: How long should each phase take?
**A:** Varies by project size:
- Ideation: 30min - 2hr
- Planning: 1-4hr
- Architecture: 2-8hr
- Implementation: Days to weeks
- Retrospective: 30min - 1hr

### Q: What if requirements change mid-epic?
**A:** Options:
1. Finish current epic, create new epic for changes
2. Cancel current epic, start over (if early)
3. Add deferred tasks for later

**Best Practice:** Scope epics small enough that changes are rare.

### Q: Can I work on multiple epics simultaneously?
**A:** Technically yes (use `scud use-tag`), but not recommended. Focus on one epic at a time.

### Q: Do I need Claude Code to use SCUD?
**A:** No! SCUD works with:
- Claude Code (best experience)
- Any AI assistant (manually paste agent prompts)
- No AI (manually create tasks)

### Q: Is SCUD free?
**A:** Yes, SCUD is open source. You only pay for Anthropic API usage (for AI features).

### Q: What about tasks that aren't in any epic?
**A:** Everything should be in an epic. Create a "Maintenance" or "Misc" epic if needed.

### Q: How do I handle bugs found during implementation?
**A:** Add them as tasks in current epic:
```bash
# Manually add to tasks.json, or
# Create mini-epic for bugs if many
```

---

## Summary

SCUD provides a structured, AI-powered workflow for software development:

1. **Define** what to build (PRD)
2. **Plan** the work (epics & tasks)
3. **Design** the solution (architecture)
4. **Build** the features (implementation)
5. **Learn** from the experience (retrospective)

Each phase has dedicated AI agents to guide you. The Rust CLI provides fast, efficient task management.

**Key Principles:**
- ðŸ“‹ Clear requirements prevent rework
- ðŸ§© Small tasks reduce risk
- ðŸ—ï¸ Good architecture prevents technical debt
- ðŸ§ª Test-driven implementation ensures quality
- ðŸ“ˆ Continuous improvement through reflection

**Start Simple:**
```bash
scud init
# Use /tm-pm
# Follow the workflow
# Trust the process
```

**Questions?**
- Check documentation: `README.md`, `DETAILED_WALKTHROUGH.md`
- Review examples in this guide
- Check troubleshooting section

**Happy Building! ðŸš€**
</file>

<file path="docs/guides/MIGRATION.md">
# Migration Guide: Task Master â†’ SCUD

This guide helps existing Task Master users migrate to SCUD (Sprint Cycle Unified Development).

## Overview

SCUD is a complete rewrite of the Task Master tool with:
- **50x faster** startup (Rust vs. Node.js with MCP)
- **42x fewer tokens** (direct API calls vs. MCP overhead)
- **Same workflow** - commands and task format are backward compatible
- **New features** - parallel development with tag groups and task assignment

## Breaking Changes

### âš ï¸ Storage Format Changed!

SCUD now uses **SCG (SCUD Graph)** format instead of JSON:
- Tasks stored in `.scud/tasks/tasks.scg` (was `.taskmaster/tasks/tasks.json`)
- Config in `.scud/config.toml` (was `.taskmaster/workflow-state.json`)

However, the CLI will automatically migrate your existing JSON tasks if found.

## What Changed

### 1. Binary Name

**Old:**
```bash
task-master list
task-master parse-prd docs/prd/my-feature.md --tag my-feature
```

**New:**
```bash
scud list
scud parse-prd docs/prd/my-feature.md --tag my-feature
```

### 2. Installation

**Old:**
```bash
npm install -g @eyaltoledano/claude-task-master
```

**New:**
```bash
pnpm add -g scud-task   # recommended
# Or:
npm install -g scud-task
```

### 3. Directory Structure

**Old:**
```
.taskmaster/
â”œâ”€â”€ tasks/tasks.json
â””â”€â”€ workflow-state.json
```

**New:**
```
.scud/
â”œâ”€â”€ tasks/tasks.scg      # SCG format (75% fewer tokens)
â””â”€â”€ config.toml          # Active tag and settings
```

### 4. Performance

**Old behavior:**
- Startup time: ~2-3 seconds
- Parse PRD: ~30-45 seconds
- MCP overhead on every call

**New behavior:**
- Startup time: ~50ms (50x faster)
- Parse PRD: ~8-12 seconds (3-4x faster)
- Direct API calls, no MCP

### 5. New Features (Optional)

SCUD adds **experimental parallel development features** that are completely optional:

#### Tag Groups
Coordinate related tags (e.g., backend + frontend):
```bash
scud create-group fullstack --tags backend,frontend --description "Backend + Frontend"
scud group-status fullstack
```

#### Task Assignment & Locking
Multiple developers can work on the same tag:
```bash
scud assign 123 alice
scud claim 123 --name alice    # Locks the task
scud whois                      # See all assignments
scud release 123                # Unlock when done
```

## Migration Steps

### Step 1: Install SCUD

```bash
pnpm add -g scud-task   # recommended
# Or:
npm install -g scud-task
```

### Step 2: Initialize New Project

```bash
cd your-project
scud init
scud hooks install   # Enable automatic task completion
```

### Step 3: Migrate Existing Tasks (If Any)

If you have an existing `.taskmaster/` directory:

```bash
# SCUD will auto-detect and offer to migrate
scud tags

# Or manually re-parse your PRDs:
scud parse-prd docs/features/my-feature.md --tag my-feature
```

### Step 4: Update Slash Commands (Optional)

If you use Claude Code or OpenCode, the slash commands are already updated:
- `/tm-pm` - Product Manager agent
- `/tm-sm` - Scrum Master agent
- `/tm-architect` - Architect agent
- `/tm-dev` - Developer agent
- `/tm-retrospective` - Retrospective agent

These work identically to before.

### Step 5: Update Any Scripts

If you have scripts calling `task-master`, update them to use `scud`:

```bash
# Old
task-master parse-prd docs/prd/feature.md --tag feature

# New
scud parse-prd docs/features/feature.md --tag feature
```

Or create an alias:
```bash
alias task-master='scud'
```

## Rollback Plan

If you encounter issues, you can rollback:

```bash
# Uninstall SCUD
npm uninstall -g scud-task

# Reinstall old task-master
npm install -g @eyaltoledano/claude-task-master
```

Note: You may need to re-create tasks from your PRDs since the storage format changed.

## FAQ

### Q: Will my existing tasks be affected?

**A:** The storage format changed from JSON to SCG. SCUD can migrate existing JSON tasks, or you can re-parse your PRDs.

### Q: Do I need to update my PRDs or feature documents?

**A:** No. SCUD parses markdown the same way as Task Master.

### Q: What if I don't want the new parallel features?

**A:** Simply don't use them! The tag groups and task assignment features are completely optional. SCUD works exactly like Task Master if you ignore the new commands.

### Q: Can I use both tools?

**A:** Not recommended - they use different storage formats (`.taskmaster/` vs `.scud/`).

### Q: What about my API key?

**A:** Default provider changed to xAI:
```bash
export XAI_API_KEY="xai-..."
```

Alternative providers: Anthropic (`ANTHROPIC_API_KEY`), OpenAI (`OPENAI_API_KEY`), OpenRouter (`OPENROUTER_API_KEY`).

Configure with `scud config --provider <provider> --model <model>`.

### Q: Where did DETAILED_WALKTHROUGH.md go?

**A:** It was merged into the more comprehensive `COMPLETE_GUIDE.md` which covers everything in one place. Old implementation details were moved to `log_docs/` for reference.

## Performance Benchmarks

Measured on MacBook Pro M1, 16GB RAM:

| Operation | Task Master | SCUD | Improvement |
|-----------|-------------|------|-------------|
| Startup (`--help`) | 2,100ms | 42ms | **50x faster** |
| List tasks | 2,200ms | 45ms | **49x faster** |
| Parse PRD (10 tasks) | 32,000ms | 8,500ms | **3.8x faster** |
| Analyze complexity | 28,000ms | 7,200ms | **3.9x faster** |
| Show task details | 2,150ms | 38ms | **57x faster** |

Token usage (MCP vs direct API):

| Operation | Task Master | SCUD | Savings |
|-----------|-------------|------|---------|
| Parse PRD | ~85,000 tokens | ~2,000 tokens | **42x fewer** |
| Analyze complexity | ~62,000 tokens | ~1,500 tokens | **41x fewer** |

## Getting Help

- **Quick Start**: See `QUICKSTART.md`
- **Complete Reference**: See `COMPLETE_GUIDE.md`
- **Command Cheat Sheet**: See `QUICK_REFERENCE.md`
- **Parallel Features**: See `PARALLEL_FEATURES.md`
- **Issues**: https://github.com/pyrex41/scud/issues

## Summary

âœ… **Same commands** - muscle memory preserved
âœ… **Same workflow** - no retraining needed
âœ… **50x faster** - dramatic performance improvement
âœ… **New features** - optional parallel development tools
âœ… **New storage format** - SCG for 75% token reduction

SCUD is a drop-in replacement that makes your existing workflow faster and more powerful.
</file>

<file path="docs/reference/QUICK_REFERENCE.md">
# SCUD Quick Reference

## 5-Phase Workflow

```
Ideation â†’ Planning â†’ Architecture â†’ Implementation â†’ Retrospective
```

| Phase | Agent | Command | Output |
|-------|-------|---------|--------|
| **Ideation** | Product Manager | `/tm-pm` | PRD document |
| **Planning** | PM + Scrum Master | `/tm-pm` â†’ `/tm-sm` | Feature files + Tasks |
| **Architecture** | Architect | `/tm-architect` | Architecture doc + Task details |
| **Implementation** | Developer | `/tm-dev` | Working code + Tests |
| **Retrospective** | Facilitator | `/tm-retrospective` | Learnings doc |

---

## Common Commands

### Setup
```bash
scud init                # Initialize SCUD
scud status              # Check current state
```

### Task Management
```bash
scud tags                # List all tags
scud use-tag <tag>       # Switch active tag
scud list                # List tasks
scud list --status done  # Filter by status
scud show <id>           # Task details
scud set-status <id> <status>  # Update status
scud next                # Find next task
scud stats               # Show statistics
scud doctor              # [EXPERIMENTAL] Diagnose stuck states
```

### [EXPERIMENTAL] Dynamic-Wave Mode
```bash
# Auto-claim next available task (sets in-progress + locks)
scud next --claim --name <agent>

# Release tasks claimed by agent
scud next --release --name <agent>

# Check workflow health
scud doctor

# Auto-fix stale locks and orphan tasks
scud doctor --fix
```

**Agent Obligation:** After `--claim`, MUST run `scud set-status <id> done` when complete!

### AI Commands (require XAI_API_KEY)
```bash
scud parse-prd <file> --tag <tag>  # Parse PRD into tasks
scud analyze-complexity             # Score all tasks
scud analyze-complexity --task <id> # Score specific task
scud expand <id>                    # Split complex task
scud expand --all                   # Split all tasks >13
scud research "<query>"             # AI research
```

Default model: `grok-code-fast-1`. Configure with `scud config`.

---

## Task Statuses

```
pending â†’ in-progress â†’ done
              â†“
      review, blocked, deferred, cancelled
```

**Valid statuses:** `pending`, `in-progress`, `done`, `review`, `blocked`, `deferred`, `cancelled`

---

## Complexity Scale (Fibonacci)

| Score | Time | Description | Action |
|-------|------|-------------|--------|
| **1** | ~30 min | Trivial (config change) | âœ… Good to go |
| **2** | 30m-1h | Simple (add validation) | âœ… Good to go |
| **3** | 1-2h | Moderate (new endpoint) | âœ… Good to go |
| **5** | 2-4h | Complex (integration) | âœ… Good to go |
| **8** | 4-8h | Very complex | âœ… Good to go |
| **13** | 1 day | Extremely complex | âš ï¸ Should split |
| **21** | 2+ days | Too large | âŒ Must split |

**Rule:** All tasks should be â‰¤8 complexity for implementation.

---

## Typical Session Flow

### Starting a New Feature

```bash
# 1. Initialize (once per project)
scud init

# 2. Create PRD
/tm-pm
# Answer questions, PRD created in docs/prd/

# 3. Create feature specs
/tm-pm  # (in planning mode)
# Creates docs/features/*.md

# 4. Parse first feature
scud parse-prd docs/features/auth.md --tag auth

# 5. Review and refine tasks
scud list
scud analyze-complexity
scud expand --all  # if needed

# 6. Architecture
/tm-architect
# Creates architecture doc, adds task details

# 7. Implementation loop
/tm-dev
# Agent uses: scud next â†’ implement â†’ scud set-status X done â†’ repeat

# 8. Retrospective
/tm-retrospective
# All tasks done, creates learnings doc
```

### Continuing Existing Feature

```bash
# Check status
scud status

# Switch tag if needed
scud use-tag auth

# Find next task
scud next

# Or continue with agent
/tm-dev
```

---

## Agent Cheat Sheet

### `/tm-pm` (Product Manager)
- **When:** Start of project, defining features
- **Creates:** PRD, feature files
- **Phase:** Ideation â†’ Planning

### `/tm-sm` (Scrum Master)
- **When:** After features are defined
- **Does:** Parse PRDs, refine tasks, break down complex tasks
- **Phase:** Planning
- **Key:** Ensures all tasks are â‰¤13 complexity

### `/tm-architect` (Architect)
- **When:** After all tasks are created and refined
- **Does:** Design solution, add technical details to ALL tasks
- **Phase:** Architecture
- **Key:** Must complete before implementation

### `/tm-dev` (Developer)
- **When:** Architecture is complete
- **Does:** Execute tasks, write tests, track progress
- **Phase:** Implementation
- **Rules:**
  - Uses `scud next` to find tasks
  - Validates dependencies
  - MUST write tests
  - MUST pass tests before marking done

### `/tm-retrospective` (Facilitator)
- **When:** All tasks are done
- **Does:** Gather metrics, reflection, capture learnings
- **Phase:** Retrospective
- **Triggers:** Requires ALL tasks = `done`

### `/status` (Status Reporter)
- **When:** Anytime
- **Does:** Show current phase, active tag, next steps
- **Phase:** Any

---

## File Structure

```
project/
â”œâ”€â”€ .scud/
â”‚   â”œâ”€â”€ tasks/
â”‚   â”‚   â””â”€â”€ tasks.scg               # All tasks in SCG format
â”‚   â””â”€â”€ config.toml                 # Active tag and settings
â”‚
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ prd/
â”‚   â”‚   â””â”€â”€ product-name-prd.md     # Product requirements
â”‚   â”œâ”€â”€ features/
â”‚   â”‚   â”œâ”€â”€ auth.md                 # Feature specifications
â”‚   â”‚   â””â”€â”€ payments.md
â”‚   â”œâ”€â”€ architecture/
â”‚   â”‚   â””â”€â”€ auth-architecture.md    # Technical designs
â”‚   â””â”€â”€ retrospectives/
â”‚       â””â”€â”€ auth-retrospective.md   # Learnings
â”‚
â””â”€â”€ .claude/commands/               # Slash commands (if using Claude Code)
    â”œâ”€â”€ tm-pm.md
    â”œâ”€â”€ tm-sm.md
    â”œâ”€â”€ tm-architect.md
    â”œâ”€â”€ tm-dev.md
    â”œâ”€â”€ tm-retrospective.md
    â””â”€â”€ status.md
```

---

## Task SCG Structure

Tasks are stored in SCG (SCUD Graph) format. See [SCG_FORMAT_SPEC.md](SCG_FORMAT_SPEC.md) for full details.

```
@nodes
1 | Create User model | P | 3 | H

@edges
2 -> 1

@details
1 | description |
  Implement User model with validation
1 | test_strategy |
  Unit tests for model validation
```

**Status codes:** P=Pending, I=InProgress, D=Done, R=Review, B=Blocked, F=Deferred, C=Cancelled, X=Expanded
**Priority codes:** H=High, M=Medium, L=Low

---

## Environment Variables

```bash
# Required for AI commands (default provider: xAI)
export XAI_API_KEY=xai-...

# Alternative providers:
# export ANTHROPIC_API_KEY=sk-ant-...
# export OPENAI_API_KEY=sk-...

# Configure provider/model with:
# scud config --provider xai --model grok-code-fast-1
```

---

## Troubleshooting Quick Fixes

| Problem | Solution |
|---------|----------|
| No tasks file | `scud init` |
| No active tag | `scud use-tag <tag>` |
| Wrong phase | Check `/status`, use correct agent |
| Dependencies not met | `scud next` to find available tasks |
| Task too complex | `scud expand <id>` or `scud expand --all` |
| No API key | `export ANTHROPIC_API_KEY=sk-ant-...` |
| Rust binary missing | `cd scud-cli && cargo build --release` |
| Stale locks | `scud doctor --fix` or `scud release <id> --force` |
| Stuck workflow | `scud doctor` to diagnose issues |
| Tasks blocked | `scud doctor` to find cancelled/missing deps |

---

## Best Practices Quick List

âœ… **Do:**
- Follow the 5 phases in order
- Keep tasks â‰¤8 complexity
- Write clear, specific PRDs
- Add technical details in architecture phase
- Write tests for every task
- Run retrospectives

âŒ **Don't:**
- Skip phases
- Create tasks >13 complexity
- Ignore dependencies
- Mark tasks done without tests
- Skip retrospectives
- Work on multiple tasks simultaneously

---

## Key Metrics

Track these for each tag:

- **Total tasks:** Number of tasks in tag
- **Total complexity:** Sum of all complexity points
- **Completion %:** (Done tasks / Total tasks) Ã— 100
- **Tasks split:** How many needed expansion
- **Duration:** Time from start to finish

View with: `scud stats`

---

## Common Workflows

### Quick Feature (Small)
```bash
scud init
/tm-pm              # Create mini-PRD
/tm-pm              # Create feature spec
scud parse-prd docs/features/feature.md --tag feature
/tm-architect       # Add details
/tm-dev             # Implement
/tm-retrospective   # Learn
```

### Full Project (Large)
```bash
scud init
/tm-pm              # Comprehensive PRD
/tm-pm              # Break into 5-7 features

# For each feature:
scud parse-prd docs/features/auth.md --tag auth
scud analyze-complexity
scud expand --all
/tm-architect
/tm-dev
/tm-retrospective

# Repeat for each feature
```

### Bug Fix Batch
```bash
# Create "bug-fixes" tag
scud parse-prd docs/features/bugs.md --tag bugs
# Or manually add tasks to tasks.scg
/tm-architect  # Add fix details
/tm-dev        # Fix bugs
```

---

## Performance Tips

### Rust CLI (Fast)
- Core commands (tags, list, show): ~5ms
- AI commands (parse-prd, expand): ~2-3s (API call)

### Speed Up AI Commands
- Use specific task IDs: `scud analyze-complexity --task 5`
- Batch operations: `scud expand --all`
- Keep prompts focused

### Optimize Workflow
- Do all complexity analysis at once
- Expand all tasks before architecture
- Use `scud next` to avoid dependency issues

---

## Integration Points

### With Claude Code
- Slash commands in `.claude/commands/`
- Agents automatically use SCUD CLI
- Seamless workflow

### With Git
- Commit after each phase
- Tag releases by feature
- Track in commit messages

### With Other Tools
- Export tasks to GitHub Issues / Jira
- Use SCUD for planning, external tools for tracking
- Keep `.scud/` in sync

---

## Resources

- **Full Guide:** `COMPLETE_GUIDE.md` - Comprehensive documentation
- **Walkthrough:** `DETAILED_WALKTHROUGH.md` - Step-by-step tutorial
- **Rust CLI:** `RUST_CLI_IMPLEMENTATION.md` - Technical details
- **README:** `README.md` - Project overview
- **Quick Start:** `QUICKSTART.md` - Get started fast

---

**Remember:** SCUD is a guide, not a prison. Follow the workflow, but adapt as needed. The goal is better software, not perfect adherence.

**Happy building! ðŸš€**
</file>

<file path="docs/reference/SCG_FORMAT_SPEC.md">
# SCG (SCUD Graph) Format Specification v1

A token-efficient, human-readable format for representing task dependency graphs. Inspired in part by Nikolai Mushegian's [JAMS spec](https://nikolai.fyi/jams/) ([GitHub](https://github.com/nmushegian/jams)).

## Overview

SCG is a text-based format that represents tasks as a Directed Acyclic Graph (DAG). It achieves ~75% token reduction compared to JSON while remaining human-editable and git-friendly.

**Storage**: `.scud/tasks/tasks.scg`

## File Structure

```
# SCUD Graph v1
# Phase: <tag>

@meta {
  name <tag>
  updated <iso8601>
}

@nodes
<id> | <title> | <status> | <complexity> | <priority>

@edges
<dependent> -> <dependency>

@parents
<parent_id>: <subtask_id>, <subtask_id>

@assignments
<id> | <assigned_to> | <locked_by> | <locked_at>

@details
<id> | description |
  <multiline content indented 2 spaces>
```

## Status Codes

| Code | Status | Description |
|------|--------|-------------|
| `P` | Pending | Not started |
| `I` | InProgress | Being worked on |
| `D` | Done | Completed |
| `R` | Review | Awaiting review |
| `B` | Blocked | Cannot proceed |
| `F` | Deferred | Postponed |
| `C` | Cancelled | Aborted |
| `X` | Expanded | Has subtasks (excluded from waves) |

## Priority Codes

| Code | Priority |
|------|----------|
| `H` | High |
| `M` | Medium |
| `L` | Low |

## Complexity

Fibonacci numbers only: `0, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89`

Tasks with complexity 13+ should typically be expanded into subtasks.

## ID Format

```
<phase>:<local_id>[.<subtask>...]
```

Examples: `auth:1`, `auth:1.1`, `api:2.3.1`

## Example

```
# SCUD Graph v1
# Phase: auth

@meta {
  name auth
  updated 2025-01-15T10:30:00Z
}

@nodes
auth:1 | Design auth system | X | 13 | H
auth:1.1 | Implement JWT tokens | D | 5 | H
auth:1.2 | Add refresh token flow | I | 3 | M
auth:2 | Rate limiting | P | 5 | M

@edges
auth:1.2 -> auth:1.1
auth:2 -> auth:1

@parents
auth:1: auth:1.1, auth:1.2

@assignments
auth:1.2 | alice | alice | 2025-01-15T09:00:00Z

@details
auth:1 | description |
  Design the authentication system architecture.
  Must support OAuth2 and API keys.
auth:1.1 | test_strategy |
  Unit tests for token generation and validation.
```

## Escaping

| Character | Escaped |
|-----------|---------|
| `\|` | `\\|` |
| `\` | `\\` |
| newline (in single-line) | `\n` |

Multiline content in `@details` uses 2-space indentation instead of escaping.

## Validation Rules

- **IDs**: Max 100 chars, alphanumeric + `-_:.`
- **Titles**: Non-empty, max 200 chars
- **Descriptions**: Max 5000 chars
- **Dependencies**: Must form a DAG (no cycles)
- **Subtasks**: Parent must have status `X`

## Multiple Phases

Phases are separated by `---`:

```
# SCUD Graph v1
# Phase: auth
...

---

# SCUD Graph v1
# Phase: api
...
```

## Graph Concepts

- **Ready**: A `Pending` task whose dependencies are all `Done` (or has no dependencies)
- **Blocked**: A task with at least one dependency not yet `Done`
- **Wave**: The current set of all ready tasksâ€”executable in parallel right now

Waves are computed dynamically based on completion state, not static DAG depth. As tasks complete, new tasks become unblocked and form the next wave. A single wave may span multiple DAG depths if branches complete at different rates.
</file>

<file path="docs/orchestrator.md">
# SCUD Orchestrator Pattern

## Overview

SCUD uses DAG-driven execution: tasks become ready when their dependencies complete. An orchestrator spawns agents for ready tasks and loops until work is done.

This guide shows how to build orchestrator patterns that spawn multiple Claude Code agents in parallel, each working on a different task.

---

## Quick Start

```bash
# 1. Install hooks first (critical!)
scud hooks install

# 2. Basic orchestrator loop
while true; do
    TASK=$(scud next --tag myproject)
    if [ -z "$TASK" ]; then
        echo "No ready tasks. Exiting."
        break
    fi

    TASK_ID=$(echo "$TASK" | grep -o "Task [0-9]*" | awk '{print $2}')
    echo "Starting task $TASK_ID"

    SCUD_TASK_ID=$TASK_ID claude "Implement task $TASK_ID" &
done

wait
echo "All tasks complete"
```

---

## How Hooks Ensure Completion

The orchestrator pattern relies on Claude Code hooks to automatically mark tasks complete:

1. **Install hooks** - Run `scud hooks install` to create `.claude/settings.local.json`
2. **Set task ID** - Pass `SCUD_TASK_ID=<id>` as environment variable to Claude session
3. **Work happens** - Agent implements the task
4. **Session ends** - Claude Code fires the Stop hook
5. **Hook marks complete** - Hook calls `scud _hook-complete` internally
6. **Task updated** - If `SCUD_TASK_ID` is set, that task is marked Done and unlocked

This prevents the ~15% of cases where agents forget to mark tasks complete.

---

## Commands for Orchestration

### Get Next Ready Task

```bash
scud next --tag myproject
```

Output (when task is ready):
```
Task 3 is ready (depends on: task-1, task-2)
Title: Implement user authentication
Dependencies: task-1 (done), task-2 (done)
Complexity: 8
```

Output (when no tasks ready):
```
No tasks ready. Check dependencies or all tasks may be complete.
```

### Monitor Active Sessions

```bash
scud whois --tag myproject
```

Output:
```
Active task assignments:

Task 3: alice (claimed 5m ago)
Task 4: bob (claimed 2m ago)
Task 5: charlie (claimed 1m ago)
```

### Check for Stale Locks

```bash
scud doctor --tag myproject
```

Output:
```
Checking tasks in tag: myproject

Issues found:
  - Task 7: Stale lock (claimed by alice 25h ago)

Run with --fix to auto-release stale locks
```

Fix stale locks:
```bash
scud doctor --tag myproject --fix
```

### View Parallel Waves

```bash
scud waves --tag myproject
```

Output:
```
Wave 1 (3 tasks):
  - Task 1: Setup database schema
  - Task 2: Create API endpoints
  - Task 3: Design UI mockups

Wave 2 (2 tasks):
  - Task 4: Implement auth middleware (depends on: task-1, task-2)
  - Task 5: Build login form (depends on: task-3)

Wave 3 (1 task):
  - Task 6: Integration tests (depends on: task-4, task-5)
```

---

## Parallel Spawning Examples

### Example 1: Simple Parallel Loop

Spawn up to 4 parallel agents:

```bash
#!/bin/bash
# parallel-simple.sh

scud hooks install

MAX_PARALLEL=4
ACTIVE=0

while true; do
    # Find ready tasks
    TASK=$(scud next --tag myproject)

    if [ -z "$TASK" ]; then
        if [ $ACTIVE -eq 0 ]; then
            echo "All tasks complete"
            break
        else
            echo "Waiting for active tasks..."
            sleep 5
            continue
        fi
    fi

    # Wait if at max parallel
    while [ $ACTIVE -ge $MAX_PARALLEL ]; do
        sleep 2
        # Count active jobs
        ACTIVE=$(jobs -r | wc -l)
    done

    # Spawn agent
    TASK_ID=$(echo "$TASK" | grep -o "Task [0-9]*" | awk '{print $2}')
    echo "Starting task $TASK_ID (active: $ACTIVE)"

    SCUD_TASK_ID=$TASK_ID claude "Implement task $TASK_ID" &
    ACTIVE=$((ACTIVE + 1))
done

wait
```

### Example 2: Claim-Based Orchestrator

Use task claiming for more control:

```bash
#!/bin/bash
# parallel-claim.sh

scud hooks install

MAX_PARALLEL=4
AGENT_NAME="orchestrator-$$"

while true; do
    # Get next ready task
    TASK=$(scud next --tag myproject)

    if [ -z "$TASK" ]; then
        # Check if any jobs still running
        if [ $(jobs -r | wc -l) -eq 0 ]; then
            echo "All tasks complete"
            break
        fi
        sleep 5
        continue
    fi

    # Wait if at capacity
    while [ $(jobs -r | wc -l) -ge $MAX_PARALLEL ]; do
        sleep 2
    done

    # Extract task ID
    TASK_ID=$(echo "$TASK" | grep -o "Task [0-9]*" | awk '{print $2}')

    # Claim task
    scud claim "$TASK_ID" --name "$AGENT_NAME-$TASK_ID" --tag myproject

    # Spawn agent with task ID
    echo "Starting task $TASK_ID"
    SCUD_TASK_ID=$TASK_ID claude "Implement task $TASK_ID" &
done

wait
echo "All spawned tasks complete"
```

### Example 3: Wave-Based Execution

Execute entire waves in parallel:

```bash
#!/bin/bash
# parallel-waves.sh

scud hooks install

TAG="myproject"

# Get wave count
WAVES=$(scud waves --tag $TAG | grep "^Wave" | wc -l)

for WAVE in $(seq 1 $WAVES); do
    echo "Starting Wave $WAVE..."

    # Get all ready tasks
    READY_TASKS=$(scud list --tag $TAG --status pending | grep "^Task" | awk '{print $2}')

    if [ -z "$READY_TASKS" ]; then
        echo "No more ready tasks"
        break
    fi

    # Spawn agents for all ready tasks
    for TASK_ID in $READY_TASKS; do
        echo "  - Starting task $TASK_ID"
        SCUD_TASK_ID=$TASK_ID claude "Implement task $TASK_ID" &
    done

    # Wait for wave to complete
    wait
    echo "Wave $WAVE complete"
done

echo "All waves complete"
```

---

## Monitoring Progress

### Real-time Statistics

```bash
watch -n 2 "scud stats --tag myproject"
```

Output:
```
Every 2.0s: scud stats --tag myproject

Tag: myproject
Total tasks: 10
  Pending: 2
  In Progress: 3
  Done: 5

Completion: 50% (5/10)
```

### Session Dashboard

```bash
watch -n 2 "scud whois --tag myproject && echo && scud stats --tag myproject"
```

---

## Best Practices

### 1. Always Install Hooks First

```bash
scud hooks install
```

Without hooks, agents may forget to mark tasks complete, breaking the DAG.

### 2. Set SCUD_TASK_ID Environment Variable

```bash
SCUD_TASK_ID=5 claude "Implement task 5"
```

This tells the hook which task to mark complete when the session ends.

### 3. Use Claiming for Team Coordination

If multiple orchestrators run simultaneously:

```bash
scud claim <task-id> --name <unique-name>
```

This prevents two agents from working on the same task.

### 4. Monitor with `whois` and `doctor`

```bash
# See active work
scud whois --tag myproject

# Find stale locks
scud doctor --tag myproject --stale-hours 2
```

### 5. Clean Up Stale Locks

If an agent crashes without releasing:

```bash
scud doctor --tag myproject --fix
```

Or manually:

```bash
scud release <task-id> --force
```

---

## Troubleshooting

### Task Not Marked Complete

**Symptom:** Agent finishes but task stays "in-progress"

**Causes:**
1. Hooks not installed (`scud hooks install`)
2. `SCUD_TASK_ID` not set when spawning agent
3. Hook file corrupted (check `.claude/settings.local.json`)

**Fix:**
```bash
# Check hook status
scud hooks status

# Reinstall if needed
scud hooks install

# Manually mark complete
scud set-status <id> done
```

### No Tasks Ready

**Symptom:** `scud next` says "No tasks ready" but tasks exist

**Causes:**
1. All ready tasks are claimed/locked
2. Dependencies not satisfied
3. All tasks complete

**Fix:**
```bash
# Check for claimed tasks
scud whois --tag myproject

# Check for stale locks
scud doctor --tag myproject

# View dependency graph
scud waves --tag myproject

# Check completion
scud stats --tag myproject
```

### Stale Locks

**Symptom:** Tasks locked by crashed agents

**Fix:**
```bash
# Find stale locks (older than 2 hours)
scud doctor --tag myproject --stale-hours 2

# Auto-fix
scud doctor --tag myproject --stale-hours 2 --fix

# Manual release
scud release <task-id> --force
```

---

## Environment Variables

| Variable | Purpose | Example |
|----------|---------|---------|
| `SCUD_TASK_ID` | Task ID for hook completion | `SCUD_TASK_ID=5 claude "work"` |
| `ANTHROPIC_API_KEY` | API key for AI commands | `export ANTHROPIC_API_KEY=sk-ant-...` |

---

## Advanced Patterns

### Conditional Spawning

Only spawn if task meets criteria:

```bash
while true; do
    TASK=$(scud next --tag myproject)
    [ -z "$TASK" ] && break

    TASK_ID=$(echo "$TASK" | grep -o "Task [0-9]*" | awk '{print $2}')
    COMPLEXITY=$(scud show $TASK_ID | grep "Complexity:" | awk '{print $2}')

    # Only spawn for low-complexity tasks
    if [ "$COMPLEXITY" -lt 10 ]; then
        SCUD_TASK_ID=$TASK_ID claude "Implement task $TASK_ID" &
    else
        echo "Skipping high-complexity task $TASK_ID"
    fi
done
wait
```

### Priority-Based Execution

Execute high-priority tasks first:

```bash
# Get all ready tasks sorted by complexity (low to high)
TASKS=$(scud list --tag myproject --status pending | sort -k3 -n)

for TASK_LINE in $TASKS; do
    TASK_ID=$(echo "$TASK_LINE" | awk '{print $2}')
    SCUD_TASK_ID=$TASK_ID claude "Implement task $TASK_ID" &
done

wait
```

### Multi-Tag Orchestrator

Work across multiple tags:

```bash
TAGS=("auth" "api" "ui")

for TAG in "${TAGS[@]}"; do
    echo "Processing tag: $TAG"

    while true; do
        TASK=$(scud next --tag $TAG)
        [ -z "$TASK" ] && break

        TASK_ID=$(echo "$TASK" | grep -o "Task [0-9]*" | awk '{print $2}')
        SCUD_TASK_ID=$TASK_ID claude "Implement $TAG task $TASK_ID" &
    done
done

wait
```

---

## Performance Tips

1. **Limit parallelism** - More than 4-5 agents can cause rate limiting
2. **Use waves** - Execute entire dependency levels at once
3. **Monitor actively** - Use `watch` with `stats` and `whois`
4. **Clean stale locks** - Run `doctor` periodically
5. **Profile complexity** - Skip or defer high-complexity tasks

---

## Reference

### Key Commands

```bash
scud hooks install          # Enable automatic completion
scud next --tag <tag>       # Find next ready task
scud claim <id> --name <n>  # Lock task
scud release <id>           # Unlock task
scud whois --tag <tag>      # Show active work
scud doctor --tag <tag>     # Check for issues
scud waves --tag <tag>      # Show parallel waves
scud stats --tag <tag>      # Show progress
```

### Hook Mechanism

1. Hooks installed via `scud hooks install`
2. Creates `.claude/settings.local.json` with Stop hook
3. Hook calls `scud _hook-complete` on every Claude session end
4. If `SCUD_TASK_ID` env var is set, that task is marked Done
5. Task lock is automatically released

---

## See Also

- [Quick Reference](reference/QUICK_REFERENCE.md) - Command cheat sheet
- [Parallel Features](features/PARALLEL_FEATURES.md) - Task locking details
- [Complete Guide](guides/COMPLETE_GUIDE.md) - Full documentation
</file>

<file path="docs/README.md">
# SCUD Documentation

This directory contains all user-facing documentation for SCUD.

## Structure

```
docs/
â”œâ”€â”€ README.md                     # This file
â”œâ”€â”€ guides/                       # Getting started and comprehensive guides
â”‚   â”œâ”€â”€ COMPLETE_GUIDE.md        # Full documentation (25,000 words)
â”‚   â””â”€â”€ MIGRATION.md             # Migration guide from BMAD-TM Lite
â”œâ”€â”€ reference/                    # Quick reference materials
â”‚   â””â”€â”€ QUICK_REFERENCE.md       # Command cheat sheet
â”œâ”€â”€ features/                     # Feature-specific documentation
â”‚   â””â”€â”€ PARALLEL_FEATURES.md     # Epic groups & task assignment
â”œâ”€â”€ prd/                          # Product Requirements Documents
â”œâ”€â”€ epics/                        # Epic descriptions
â”œâ”€â”€ architecture/                 # Technical design documents
â””â”€â”€ retrospectives/               # Project retrospectives and learnings
```

## Documentation Overview

### For Users

- **New to SCUD?** Start with [Complete Guide](guides/COMPLETE_GUIDE.md)
- **Upgrading?** See [Migration Guide](guides/MIGRATION.md)
- **Quick lookup?** Use [Quick Reference](reference/QUICK_REFERENCE.md)
- **Using parallel features?** See [Parallel Features](features/PARALLEL_FEATURES.md)

### For Development

- **PRDs**: Product requirements documents go in `prd/`
- **Epics**: Epic descriptions go in `epics/`
- **Architecture**: Technical designs go in `architecture/`
- **Retrospectives**: Post-epic learnings go in `retrospectives/`

## Additional Resources

- **Main README**: [../README.md](../README.md)
- **Development Logs**: [../log_docs/](../log_docs/)
- **Test Documentation**: [../scud-cli/TESTING.md](../scud-cli/TESTING.md)
</file>

<file path="scud-cli/.claude/settings.local.json">
{
  "permissions": {
    "allow": [
      "Bash(find:*)"
    ],
    "deny": [],
    "ask": []
  }
}
</file>

<file path="scud-cli/benches/storage_bench.rs">
use criterion::{black_box, criterion_group, criterion_main, Criterion};
use scud::models::{Phase, Task};
use scud::storage::Storage;
use std::collections::HashMap;
use tempfile::TempDir;

fn bench_load_all_vs_load_one(c: &mut Criterion) {
    let temp_dir = TempDir::new().unwrap();
    let storage = Storage::new(Some(temp_dir.path().to_path_buf()));
    storage.initialize().unwrap();

    // Create 50 phases with 100 tasks each (5000 tasks total)
    let mut tasks = HashMap::new();
    for i in 0..50 {
        let mut phase = Phase::new(format!("PHASE-{}", i));
        for j in 0..100 {
            phase.add_task(Task::new(
                format!("task-{}", j),
                format!("Task {}", j),
                "Description".to_string(),
            ));
        }
        tasks.insert(format!("PHASE-{}", i), phase);
    }
    storage.save_tasks(&tasks).unwrap();

    let mut group = c.benchmark_group("storage_operations");

    group.bench_function("load_all_phases_then_get_one", |b| {
        b.iter(|| {
            let all_tasks = storage.load_tasks().unwrap();
            black_box(all_tasks.get("PHASE-25").unwrap());
        })
    });

    group.bench_function("load_one_phase_directly", |b| {
        b.iter(|| {
            let phase = storage.load_group("PHASE-25").unwrap();
            black_box(&phase);
        })
    });

    group.finish();
}

fn bench_active_phase_cache(c: &mut Criterion) {
    let temp_dir = TempDir::new().unwrap();
    let storage = Storage::new(Some(temp_dir.path().to_path_buf()));
    storage.initialize().unwrap();

    let mut tasks = HashMap::new();
    tasks.insert("TEST-1".to_string(), Phase::new("TEST-1".to_string()));
    storage.save_tasks(&tasks).unwrap();
    storage.set_active_group("TEST-1").unwrap();

    let mut group = c.benchmark_group("active_phase_cache");

    group.bench_function("first_call_no_cache", |b| {
        b.iter(|| {
            storage.clear_cache();
            let active = storage.get_active_group().unwrap();
            black_box(active);
        })
    });

    group.bench_function("second_call_with_cache", |b| {
        // Prime the cache
        storage.get_active_group().unwrap();

        b.iter(|| {
            let active = storage.get_active_group().unwrap();
            black_box(active);
        })
    });

    group.finish();
}

criterion_group!(
    benches,
    bench_load_all_vs_load_one,
    bench_active_phase_cache
);
criterion_main!(benches);
</file>

<file path="scud-cli/bin/scud.js">
#!/usr/bin/env node

const { spawnSync } = require('child_process');
const path = require('path');
const fs = require('fs');

const platform = process.platform;
const binaryName = platform === 'win32' ? 'scud.exe' : 'scud';
const binaryPath = path.join(__dirname, binaryName);

// Check if Rust binary exists
if (!fs.existsSync(binaryPath)) {
  console.error('Error: SCUD binary not found.');
  console.error('The installation may have failed. Try reinstalling:');
  console.error('  npm install -g scud-task');
  console.error('\nOr build manually:');
  console.error('  cd ' + path.dirname(__dirname));
  console.error('  cargo build --release');
  process.exit(1);
}

// Execute the Rust binary
const args = process.argv.slice(2);
const result = spawnSync(binaryPath, args, {
  stdio: 'inherit',
  shell: false
});

process.exit(result.status || 0);
</file>

<file path="scud-cli/src/commands/ai/analyze_complexity.rs">
use anyhow::Result;
use colored::Colorize;
use futures::stream::{self, StreamExt};
use indicatif::{MultiProgress, ProgressBar, ProgressStyle};
use serde::Deserialize;
use std::path::PathBuf;
use std::sync::Arc;

use crate::llm::{LLMClient, Prompts};
use crate::storage::Storage;

#[derive(Debug, Deserialize)]
struct ComplexityAnalysis {
    complexity: u32,
    reasoning: String,
}

/// Result of analyzing a single task's complexity
struct TaskAnalysisResult {
    id: String,
    title: String,
    complexity: u32,
    #[allow(dead_code)]
    reasoning: String, // Parsed from LLM response but not stored
}

/// Number of concurrent LLM requests
const CONCURRENCY: usize = 5;

pub async fn run(
    project_root: Option<PathBuf>,
    task_id: Option<&str>,
    tag: Option<&str>,
) -> Result<()> {
    let storage = Storage::new(project_root.clone());
    let group_tag = crate::commands::helpers::resolve_group_tag(&storage, tag, true)?;

    let mut all_tasks = storage.load_tasks()?;
    let group = all_tasks
        .get_mut(&group_tag)
        .ok_or_else(|| anyhow::anyhow!("Task group '{}' not found", group_tag))?;

    // Use project_root for LLM config resolution
    let client = Arc::new(match project_root {
        Some(root) => LLMClient::new_with_project_root(root)?,
        None => LLMClient::new()?,
    });

    // Determine which tasks to analyze
    let tasks_to_analyze: Vec<(String, String, String, Option<String>)> = if let Some(id) = task_id
    {
        let task = group
            .get_task(id)
            .ok_or_else(|| anyhow::anyhow!("Task {} not found", id))?;
        vec![(
            task.id.clone(),
            task.title.clone(),
            task.description.clone(),
            task.details.clone(),
        )]
    } else {
        group
            .tasks
            .iter()
            .map(|t| {
                (
                    t.id.clone(),
                    t.title.clone(),
                    t.description.clone(),
                    t.details.clone(),
                )
            })
            .collect()
    };

    if tasks_to_analyze.is_empty() {
        println!("{}", "No tasks to analyze".yellow());
        return Ok(());
    }

    let task_count = tasks_to_analyze.len();
    println!(
        "{} {} task(s) with {} concurrent requests...",
        "Analyzing complexity for".blue(),
        task_count,
        CONCURRENCY
    );

    // Set up multi-progress display
    let multi_progress = MultiProgress::new();
    let overall_progress = multi_progress.add(ProgressBar::new(task_count as u64));
    overall_progress.set_style(
        ProgressStyle::default_bar()
            .template("{spinner:.blue} [{bar:40.cyan/blue}] {pos}/{len} tasks")
            .unwrap()
            .progress_chars("â–ˆâ–“â–‘"),
    );

    // Process tasks in parallel with bounded concurrency
    let results: Vec<Result<TaskAnalysisResult, (String, anyhow::Error)>> =
        stream::iter(tasks_to_analyze)
            .map(|(id, title, description, details)| {
                let client = Arc::clone(&client);
                let mp = multi_progress.clone();
                let overall = overall_progress.clone();

                async move {
                    let spinner = mp.add(ProgressBar::new_spinner());
                    spinner.set_style(
                        ProgressStyle::default_spinner()
                            .template("{spinner:.blue} {msg}")
                            .unwrap(),
                    );
                    spinner.set_message(format!("Task {}: {}", id, title));
                    spinner.enable_steady_tick(std::time::Duration::from_millis(100));

                    let prompt =
                        Prompts::analyze_complexity(&title, &description, details.as_deref());

                    // Retry logic
                    let mut last_error = None;
                    for attempt in 1..=3 {
                        match client.complete_json::<ComplexityAnalysis>(&prompt).await {
                            Ok(analysis) => {
                                spinner.finish_and_clear();
                                overall.inc(1);
                                return Ok(TaskAnalysisResult {
                                    id,
                                    title,
                                    complexity: analysis.complexity,
                                    reasoning: analysis.reasoning,
                                });
                            }
                            Err(e) => {
                                last_error = Some(e);
                                if attempt < 3 {
                                    spinner.set_message(format!(
                                        "Task {} (retry {}/3): {}",
                                        id,
                                        attempt + 1,
                                        title
                                    ));
                                    tokio::time::sleep(std::time::Duration::from_secs(1)).await;
                                }
                            }
                        }
                    }

                    spinner.finish_and_clear();
                    overall.inc(1);
                    Err((id, last_error.unwrap()))
                }
            })
            .buffer_unordered(CONCURRENCY)
            .collect()
            .await;

    overall_progress.finish_and_clear();

    // Process results and update tasks
    let mut success_count = 0;
    let mut error_count = 0;
    let mut high_complexity_tasks = Vec::new();

    for result in results {
        match result {
            Ok(analysis) => {
                if let Some(task) = group.get_task_mut(&analysis.id) {
                    task.complexity = analysis.complexity;
                    task.update();

                    println!(
                        "{} Task {}: {} â†’ complexity {}",
                        "âœ“".green(),
                        analysis.id.cyan(),
                        analysis.title,
                        analysis.complexity.to_string().yellow()
                    );

                    if analysis.complexity > 13 {
                        high_complexity_tasks.push(analysis.id.clone());
                    }
                    success_count += 1;
                }
            }
            Err((id, e)) => {
                println!("{} Task {} failed: {}", "âœ—".red(), id.cyan(), e);
                error_count += 1;
            }
        }
    }

    // Get stats before saving
    let stats = group.get_stats();
    let tasks_needing_expansion: Vec<_> = group
        .get_tasks_needing_expansion()
        .iter()
        .map(|t| (t.id.clone(), t.title.clone(), t.complexity))
        .collect();

    storage.save_tasks(&all_tasks)?;

    // Summary
    println!("\n{}", "âœ… Complexity analysis complete!".green().bold());
    println!();
    println!(
        "{:<25} {} ({} succeeded, {} failed)",
        "Analyzed:".yellow(),
        task_count,
        success_count.to_string().green(),
        if error_count > 0 {
            error_count.to_string().red()
        } else {
            error_count.to_string().normal()
        }
    );
    println!(
        "{:<25} {}",
        "Total complexity:".yellow(),
        stats.total_complexity
    );

    if !tasks_needing_expansion.is_empty() {
        println!();
        println!(
            "{} {} task(s) with complexity â‰¥3 need expansion:",
            "âš ".yellow(),
            tasks_needing_expansion.len()
        );
        for (id, title, complexity) in tasks_needing_expansion {
            println!("  {} {} [{}]", id.cyan(), title, complexity);
        }
        println!();
        println!("{}", "Run: scud expand --all".blue());
    }

    Ok(())
}
</file>

<file path="scud-cli/src/commands/ai/expand.rs">
use anyhow::Result;
use colored::Colorize;
use futures::stream::{self, StreamExt};
use indicatif::{MultiProgress, ProgressBar, ProgressStyle};
use serde::Deserialize;
use std::path::PathBuf;
use std::sync::Arc;

use crate::llm::{LLMClient, Prompts};
use crate::models::{Priority, Task, TaskStatus};
use crate::storage::Storage;

#[derive(Debug, Deserialize)]
struct ExpandedTask {
    title: String,
    description: String,
    #[serde(default)]
    priority: String,
    #[serde(default)]
    dependencies: Vec<String>,
}

/// Result of expanding a single task
struct TaskExpansionResult {
    parent_id: String,
    parent_priority: Priority,
    expanded_tasks: Vec<ExpandedTask>,
}

/// Number of concurrent LLM requests
const CONCURRENCY: usize = 5;

pub async fn run(
    project_root: Option<PathBuf>,
    task_id: Option<&str>,
    expand_all: bool,
    tag: Option<&str>,
) -> Result<()> {
    let storage = Storage::new(project_root.clone());
    let epic_tag = crate::commands::helpers::resolve_group_tag(&storage, tag, true)?;

    let mut all_tasks = storage.load_tasks()?;
    let epic = all_tasks
        .get_mut(&epic_tag)
        .ok_or_else(|| anyhow::anyhow!("Epic '{}' not found", epic_tag))?;

    // Use project_root for LLM client to find config.toml in correct location
    let client = Arc::new(if let Some(root) = project_root {
        LLMClient::new_with_project_root(root)?
    } else {
        LLMClient::new()?
    });

    // Determine which tasks to expand and gather their data
    let tasks_to_expand: Vec<(String, String, String, Option<String>, u32, Priority)> =
        if let Some(id) = task_id {
            let task = epic
                .get_task(id)
                .ok_or_else(|| anyhow::anyhow!("Task {} not found", id))?;
            if !task.needs_expansion() {
                let reason = if task.is_expanded() {
                    "already expanded"
                } else if task.is_subtask() {
                    "is a subtask"
                } else {
                    "complexity too low"
                };
                println!(
                    "{} Task {} doesn't need expansion ({}, complexity: {})",
                    "âŠ˜".yellow(),
                    id.cyan(),
                    reason,
                    task.complexity
                );
                return Ok(());
            }
            vec![(
                task.id.clone(),
                task.title.clone(),
                task.description.clone(),
                task.details.clone(),
                task.complexity,
                task.priority.clone(),
            )]
        } else if expand_all {
            epic.tasks
                .iter()
                .filter(|t| t.needs_expansion())
                .map(|t| {
                    (
                        t.id.clone(),
                        t.title.clone(),
                        t.description.clone(),
                        t.details.clone(),
                        t.complexity,
                        t.priority.clone(),
                    )
                })
                .collect()
        } else {
            anyhow::bail!("Specify a task ID or use --all to expand all tasks with complexity â‰¥3");
        };

    if tasks_to_expand.is_empty() {
        println!(
            "{}",
            "No tasks need expansion (all complexity <3 or already expanded)".green()
        );
        return Ok(());
    }

    let task_count = tasks_to_expand.len();
    println!(
        "{} {} task(s) with {} concurrent requests...",
        "Expanding".blue(),
        task_count,
        CONCURRENCY
    );

    // Set up multi-progress display
    let multi_progress = MultiProgress::new();
    let overall_progress = multi_progress.add(ProgressBar::new(task_count as u64));
    overall_progress.set_style(
        ProgressStyle::default_bar()
            .template("{spinner:.blue} [{bar:40.cyan/blue}] {pos}/{len} tasks")
            .unwrap()
            .progress_chars("â–ˆâ–“â–‘"),
    );

    // Process tasks in parallel with bounded concurrency
    let results: Vec<Result<TaskExpansionResult, (String, anyhow::Error)>> =
        stream::iter(tasks_to_expand)
            .map(|(id, title, description, details, complexity, priority)| {
                let client = Arc::clone(&client);
                let mp = multi_progress.clone();
                let overall = overall_progress.clone();

                async move {
                    let spinner = mp.add(ProgressBar::new_spinner());
                    spinner.set_style(
                        ProgressStyle::default_spinner()
                            .template("{spinner:.blue} {msg}")
                            .unwrap(),
                    );
                    spinner.set_message(format!("Task {}: {}", id, title));
                    spinner.enable_steady_tick(std::time::Duration::from_millis(100));

                    let recommended_subtasks =
                        Task::recommended_subtasks_for_complexity(complexity);
                    let prompt = Prompts::expand_task(
                        &title,
                        &description,
                        complexity,
                        details.as_deref(),
                        recommended_subtasks,
                    );

                    // Retry logic
                    let mut last_error = None;
                    for attempt in 1..=3 {
                        match client.complete_json::<Vec<ExpandedTask>>(&prompt).await {
                            Ok(expanded) => {
                                spinner.finish_and_clear();
                                overall.inc(1);
                                return Ok(TaskExpansionResult {
                                    parent_id: id,
                                    parent_priority: priority,
                                    expanded_tasks: expanded,
                                });
                            }
                            Err(e) => {
                                last_error = Some(e);
                                if attempt < 3 {
                                    spinner.set_message(format!(
                                        "Task {} (retry {}/3): {}",
                                        id,
                                        attempt + 1,
                                        title
                                    ));
                                    tokio::time::sleep(std::time::Duration::from_secs(1)).await;
                                }
                            }
                        }
                    }

                    spinner.finish_and_clear();
                    overall.inc(1);
                    Err((id, last_error.unwrap()))
                }
            })
            .buffer_unordered(CONCURRENCY)
            .collect()
            .await;

    overall_progress.finish_and_clear();

    // Process results and create subtasks
    let mut success_count = 0;
    let mut error_count = 0;
    let mut total_subtasks = 0;

    for result in results {
        match result {
            Ok(expansion) => {
                let parent_id = &expansion.parent_id;
                let subtask_count = expansion.expanded_tasks.len();

                println!(
                    "{} Task {} expanded into {} subtasks",
                    "âœ“".green(),
                    parent_id.cyan(),
                    subtask_count
                );

                // Create subtasks
                let mut new_subtask_ids = Vec::new();
                for (idx, expanded) in expansion.expanded_tasks.iter().enumerate() {
                    let new_id = format!("{}.{}", parent_id, idx + 1);

                    let priority = if !expanded.priority.is_empty() {
                        match expanded.priority.to_lowercase().as_str() {
                            "high" => Priority::High,
                            "low" => Priority::Low,
                            _ => Priority::Medium,
                        }
                    } else {
                        expansion.parent_priority.clone()
                    };

                    let mut new_task = Task::new(
                        new_id.clone(),
                        expanded.title.clone(),
                        expanded.description.clone(),
                    );
                    new_task.priority = priority;
                    new_task.complexity = 0;
                    new_task.parent_id = Some(parent_id.clone());

                    // Map dependency references to nested IDs
                    new_task.dependencies = expanded
                        .dependencies
                        .iter()
                        .filter_map(|dep| {
                            if let Ok(dep_idx) = dep.parse::<usize>() {
                                if dep_idx > 0 && dep_idx <= idx + 1 {
                                    Some(format!("{}.{}", parent_id, dep_idx))
                                } else {
                                    None
                                }
                            } else {
                                Some(dep.clone())
                            }
                        })
                        .collect();

                    new_subtask_ids.push(new_id.clone());
                    epic.add_task(new_task);

                    println!(
                        "  {} Created subtask {}: {}",
                        "+".green(),
                        new_id.cyan(),
                        expanded.title
                    );
                }

                // Update parent task
                if let Some(parent_task) = epic.get_task_mut(parent_id) {
                    parent_task.status = TaskStatus::Expanded;
                    parent_task.subtasks = new_subtask_ids;
                    parent_task.update();
                }

                total_subtasks += subtask_count;
                success_count += 1;
            }
            Err((id, e)) => {
                println!("{} Task {} failed: {}", "âœ—".red(), id.cyan(), e);
                error_count += 1;
            }
        }
    }

    storage.save_tasks(&all_tasks)?;

    // Check if there are multiple phases for cross-tag hint
    let has_multiple_phases = all_tasks.len() > 1;

    // Summary
    println!("\n{}", "âœ… Task expansion complete!".green().bold());
    println!();
    println!(
        "{:<25} {} ({} succeeded, {} failed)",
        "Expanded:".yellow(),
        task_count,
        success_count.to_string().green(),
        if error_count > 0 {
            error_count.to_string().red()
        } else {
            error_count.to_string().normal()
        }
    );
    println!(
        "{:<25} {}",
        "Total subtasks created:".yellow(),
        total_subtasks
    );

    // Hint about dependency analysis if we expanded tasks and have multiple phases
    if success_count > 0 && has_multiple_phases {
        println!();
        println!(
            "{} New subtasks may have cross-phase dependencies.",
            "Tip:".cyan()
        );
        println!(
            "     Run '{}' to check.",
            format!("scud reanalyze-deps --tag {}", epic_tag).green()
        );
    }

    println!();
    println!("{}", "Next steps:".blue());
    println!("  1. Review tasks: scud list");
    if success_count > 0 && has_multiple_phases {
        println!("  2. Check dependencies: scud reanalyze-deps");
        println!("  3. Start working: scud next");
    } else {
        println!("  2. Start working: scud next");
    }
    println!();

    Ok(())
}
</file>

<file path="scud-cli/src/commands/ai/mod.rs">
pub mod analyze_complexity;
pub mod expand;
pub mod parse_prd;
pub mod reanalyze_deps;
</file>

<file path="scud-cli/src/commands/ai/parse_prd.rs">
use anyhow::Result;
use colored::Colorize;
use indicatif::{ProgressBar, ProgressStyle};
use serde::Deserialize;
use std::path::{Path, PathBuf};

use crate::llm::{LLMClient, Prompts};
use crate::models::{Phase, Priority, Task};
use crate::storage::Storage;

#[derive(Debug, Deserialize)]
struct ParsedTask {
    title: String,
    description: String,
    priority: String,
    complexity: u32,
    #[serde(default)]
    dependencies: Vec<String>,
}

pub async fn run(
    project_root: Option<PathBuf>,
    file_path: &Path,
    tag: &str,
    num_tasks: u32,
) -> Result<()> {
    let storage = Storage::new(project_root.clone());

    if !storage.is_initialized() {
        anyhow::bail!("SCUD not initialized. Run: scud init");
    }

    // Read the PRD file
    println!("{} {}", "Reading PRD from:".blue(), file_path.display());
    let prd_content = storage.read_file(file_path)?;

    // Create LLM client with proper project root
    let client = match project_root {
        Some(root) => LLMClient::new_with_project_root(root)?,
        None => LLMClient::new()?,
    };

    // Show progress
    let spinner = ProgressBar::new_spinner();
    spinner.set_style(
        ProgressStyle::default_spinner()
            .template("{spinner:.blue} {msg}")
            .unwrap(),
    );
    spinner.set_message("Parsing PRD with AI...");
    spinner.enable_steady_tick(std::time::Duration::from_millis(100));

    // Call LLM to parse the PRD
    let prompt = Prompts::parse_prd(&prd_content, num_tasks);
    let parsed_tasks: Vec<ParsedTask> = client.complete_json(&prompt).await?;

    spinner.finish_with_message(format!(
        "{} Parsed {} tasks",
        "âœ“".green(),
        parsed_tasks.len()
    ));

    // Convert to our task model
    let mut group = Phase::new(tag.to_string());

    for (idx, parsed) in parsed_tasks.iter().enumerate() {
        let task_id = (idx + 1).to_string();

        let priority = match parsed.priority.to_lowercase().as_str() {
            "high" => Priority::High,
            "low" => Priority::Low,
            _ => Priority::Medium,
        };

        let mut task = Task::new(
            task_id.clone(),
            parsed.title.clone(),
            parsed.description.clone(),
        );
        task.complexity = parsed.complexity;
        task.priority = priority;
        task.dependencies = parsed.dependencies.clone();

        group.add_task(task);
    }

    // Load existing tasks (propagate errors - don't silently swallow them)
    let mut all_tasks = storage.load_tasks()?;

    // Check if other phases exist for cross-tag dependency hint
    let other_phases: Vec<_> = all_tasks.keys().filter(|k| *k != tag).cloned().collect();

    if all_tasks.contains_key(tag) {
        println!(
            "{}",
            format!("âš  Task group '{}' already exists. Overwriting...", tag).yellow()
        );
    }

    all_tasks.insert(tag.to_string(), group);
    storage.save_tasks(&all_tasks)?;

    // Set as active group
    storage.set_active_group(tag)?;

    println!(
        "\n{}",
        "âœ… PRD parsed and task group created!".green().bold()
    );
    println!();
    println!("{:<20} {}", "Tag:".yellow(), tag.cyan());
    println!("{:<20} {}", "Tasks created:".yellow(), parsed_tasks.len());

    // Hint about cross-tag dependencies if other phases exist
    if !other_phases.is_empty() {
        println!();
        println!(
            "{} Other phases detected: {}",
            "Note:".cyan(),
            other_phases.join(", ").yellow()
        );
        println!(
            "      Consider running '{}' to identify cross-phase dependencies.",
            "scud reanalyze-deps".green()
        );
    }

    println!();
    println!("{}", "Next steps:".blue());
    println!("  1. Review tasks: scud list");
    println!("  2. Expand complex tasks: scud expand --all");
    if !other_phases.is_empty() {
        println!("  3. Check cross-phase deps: scud reanalyze-deps");
        println!("  4. Start working: scud next");
    } else {
        println!("  3. Start working: scud next");
    }
    println!();

    Ok(())
}
</file>

<file path="scud-cli/src/commands/ai/reanalyze_deps.rs">
use anyhow::Result;
use colored::Colorize;
use indicatif::{ProgressBar, ProgressStyle};
use serde::Deserialize;
use std::collections::HashMap;
use std::path::PathBuf;

use crate::llm::{LLMClient, Prompts};
use crate::models::{Phase, TaskStatus};
use crate::storage::Storage;

#[derive(Debug, Deserialize)]
struct DependencySuggestion {
    task_id: String,
    add_dependencies: Vec<String>,
    remove_dependencies: Vec<String>,
    reasoning: String,
}

pub async fn run(
    project_root: Option<PathBuf>,
    tag: Option<&str>,
    all_tags: bool,
    apply: bool,
    dry_run: bool,
) -> Result<()> {
    let storage = Storage::new(project_root.clone());

    if !storage.is_initialized() {
        anyhow::bail!("SCUD not initialized. Run: scud init");
    }

    let mut all_phases = storage.load_tasks()?;

    if all_phases.is_empty() {
        println!(
            "{}",
            "No tasks found. Create tasks first with: scud parse-prd".yellow()
        );
        return Ok(());
    }

    // Determine which phases to analyze
    let phases_to_analyze: Vec<String> = match tag {
        Some(t) if !all_tags => {
            if !all_phases.contains_key(t) {
                anyhow::bail!("Tag '{}' not found", t);
            }
            vec![t.to_string()]
        }
        _ => all_phases.keys().cloned().collect(),
    };

    if phases_to_analyze.len() == 1 && all_phases.len() == 1 {
        println!(
            "{}",
            "Only one phase exists. Cross-tag dependency analysis is most useful with multiple phases.".yellow()
        );
    }

    // Build context for AI: all tasks with their current state
    let task_context = build_task_context(&all_phases);

    // Create LLM client
    let client = match project_root {
        Some(root) => LLMClient::new_with_project_root(root)?,
        None => LLMClient::new()?,
    };

    // Show progress
    let spinner = ProgressBar::new_spinner();
    spinner.set_style(
        ProgressStyle::default_spinner()
            .template("{spinner:.blue} {msg}")
            .unwrap(),
    );
    spinner.set_message(format!(
        "Analyzing dependencies across {} phase(s)...",
        phases_to_analyze.len()
    ));
    spinner.enable_steady_tick(std::time::Duration::from_millis(100));

    // Generate analysis prompt and call LLM
    let prompt = Prompts::reanalyze_dependencies(&task_context, &phases_to_analyze);
    let suggestions: Vec<DependencySuggestion> = client.complete_json(&prompt).await?;

    spinner.finish_and_clear();

    // Filter out suggestions that don't change anything
    let meaningful_suggestions: Vec<_> = suggestions
        .into_iter()
        .filter(|s| !s.add_dependencies.is_empty() || !s.remove_dependencies.is_empty())
        .collect();

    if meaningful_suggestions.is_empty() {
        println!("{} No dependency changes suggested.", "âœ“".green());
        return Ok(());
    }

    // Display suggestions
    println!(
        "\n{} {} dependency change(s) suggested:\n",
        "Found".blue(),
        meaningful_suggestions.len()
    );

    for suggestion in &meaningful_suggestions {
        println!("{} {}", "Task:".bold(), suggestion.task_id.cyan());
        if !suggestion.add_dependencies.is_empty() {
            println!(
                "  {} {}",
                "+".green(),
                suggestion
                    .add_dependencies
                    .iter()
                    .map(|s| s.green().to_string())
                    .collect::<Vec<_>>()
                    .join(", ")
            );
        }
        if !suggestion.remove_dependencies.is_empty() {
            println!(
                "  {} {}",
                "-".red(),
                suggestion
                    .remove_dependencies
                    .iter()
                    .map(|s| s.red().to_string())
                    .collect::<Vec<_>>()
                    .join(", ")
            );
        }
        println!("  {} {}", "Reason:".dimmed(), suggestion.reasoning.dimmed());
        println!();
    }

    if dry_run {
        println!("{}", "Dry run - no changes applied.".yellow());
        return Ok(());
    }

    // Apply changes
    let should_apply = apply || confirm_apply()?;

    if should_apply {
        let changes = apply_suggestions(&mut all_phases, &meaningful_suggestions)?;
        storage.save_tasks(&all_phases)?;
        println!(
            "{} {} dependencies updated across {} task(s).",
            "âœ“".green(),
            changes,
            meaningful_suggestions.len()
        );
    } else {
        println!("{}", "No changes applied.".yellow());
    }

    Ok(())
}

fn build_task_context(all_phases: &HashMap<String, Phase>) -> String {
    let mut context = String::new();

    for (tag, phase) in all_phases {
        context.push_str(&format!("\n## Phase: {}\n", tag));
        for task in &phase.tasks {
            let status_marker = match task.status {
                TaskStatus::Done => "[DONE]",
                TaskStatus::InProgress => "[IN PROGRESS]",
                TaskStatus::Pending => "[PENDING]",
                TaskStatus::Blocked => "[BLOCKED]",
                TaskStatus::Expanded => "[EXPANDED]",
                TaskStatus::Review => "[REVIEW]",
                TaskStatus::Deferred => "[DEFERRED]",
                TaskStatus::Cancelled => "[CANCELLED]",
            };

            // Build full namespaced ID for display
            let full_id = if task.id.contains(':') {
                task.id.clone()
            } else {
                format!("{}:{}", tag, task.id)
            };

            let deps_str = if task.dependencies.is_empty() {
                "none".to_string()
            } else {
                task.dependencies.join(", ")
            };

            context.push_str(&format!(
                "- {} {} - {}\n  Current deps: [{}]\n",
                full_id, status_marker, task.title, deps_str
            ));
        }
    }

    context
}

fn confirm_apply() -> Result<bool> {
    use std::io::{self, Write};

    print!("Apply these changes? [y/N] ");
    io::stdout().flush()?;

    let mut input = String::new();
    io::stdin().read_line(&mut input)?;

    Ok(input.trim().eq_ignore_ascii_case("y") || input.trim().eq_ignore_ascii_case("yes"))
}

fn apply_suggestions(
    all_phases: &mut HashMap<String, Phase>,
    suggestions: &[DependencySuggestion],
) -> Result<usize> {
    let mut changes = 0;

    for suggestion in suggestions {
        // Parse task ID to find the phase
        let (phase_tag, local_id) = parse_task_id(&suggestion.task_id);

        // Find the phase
        let phase = all_phases.get_mut(&phase_tag).ok_or_else(|| {
            anyhow::anyhow!(
                "Phase '{}' not found for task '{}'",
                phase_tag,
                suggestion.task_id
            )
        })?;

        // Find the task - try both local ID and full ID
        let task = phase
            .tasks
            .iter_mut()
            .find(|t| t.id == local_id || t.id == suggestion.task_id)
            .ok_or_else(|| {
                anyhow::anyhow!(
                    "Task '{}' not found in phase '{}'",
                    suggestion.task_id,
                    phase_tag
                )
            })?;

        // Add new dependencies
        for dep in &suggestion.add_dependencies {
            if !task.dependencies.contains(dep) {
                task.dependencies.push(dep.clone());
                changes += 1;
            }
        }

        // Remove dependencies
        for dep in &suggestion.remove_dependencies {
            if let Some(pos) = task.dependencies.iter().position(|d| d == dep) {
                task.dependencies.remove(pos);
                changes += 1;
            }
        }
    }

    Ok(changes)
}

/// Parse a task ID into (phase_tag, local_id)
/// Examples:
///   "auth:1" -> ("auth", "auth:1")
///   "1" -> (current_phase, "1")  -- but we'll return empty for tag
fn parse_task_id(task_id: &str) -> (String, String) {
    if let Some(colon_pos) = task_id.find(':') {
        let phase = task_id[..colon_pos].to_string();
        (phase, task_id.to_string())
    } else {
        // This shouldn't happen with our prompts, but handle gracefully
        (String::new(), task_id.to_string())
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use crate::models::Task;

    #[test]
    fn test_parse_task_id_with_namespace() {
        let (phase, local) = parse_task_id("auth:1");
        assert_eq!(phase, "auth");
        assert_eq!(local, "auth:1");
    }

    #[test]
    fn test_parse_task_id_without_namespace() {
        let (phase, local) = parse_task_id("1");
        assert_eq!(phase, "");
        assert_eq!(local, "1");
    }

    #[test]
    fn test_build_task_context() {
        let mut phases = HashMap::new();

        let mut auth_phase = Phase::new("auth".to_string());
        let mut task1 = Task::new(
            "auth:1".to_string(),
            "Create user model".to_string(),
            "".to_string(),
        );
        task1.status = TaskStatus::Done;
        auth_phase.add_task(task1);

        let mut api_phase = Phase::new("api".to_string());
        let mut task2 = Task::new(
            "api:1".to_string(),
            "Create endpoints".to_string(),
            "".to_string(),
        );
        task2.dependencies = vec!["auth:1".to_string()];
        api_phase.add_task(task2);

        phases.insert("auth".to_string(), auth_phase);
        phases.insert("api".to_string(), api_phase);

        let context = build_task_context(&phases);

        assert!(context.contains("Phase: auth"));
        assert!(context.contains("Phase: api"));
        assert!(context.contains("[DONE]"));
        assert!(context.contains("auth:1"));
    }
}
</file>

<file path="scud-cli/src/commands/assign.rs">
use anyhow::Result;
use colored::Colorize;
use std::path::PathBuf;

use crate::commands::helpers::resolve_group_tag;
use crate::storage::Storage;

pub fn run(
    project_root: Option<PathBuf>,
    task_id: &str,
    assignee: &str,
    tag: Option<&str>,
) -> Result<()> {
    let storage = Storage::new(project_root);
    let epic_tag = resolve_group_tag(&storage, tag, true)?;

    let mut all_tasks = storage.load_tasks()?;
    let epic = all_tasks
        .get_mut(&epic_tag)
        .ok_or_else(|| anyhow::anyhow!("Epic '{}' not found", epic_tag))?;

    let task = epic
        .get_task_mut(task_id)
        .ok_or_else(|| anyhow::anyhow!("Task {} not found in epic '{}'", task_id, epic_tag))?;

    task.assign(assignee);
    storage.save_tasks(&all_tasks)?;

    println!(
        "{} Task {} assigned to {}",
        "âœ“".green(),
        task_id.cyan(),
        assignee.green()
    );

    Ok(())
}
</file>

<file path="scud-cli/src/commands/claim.rs">
use anyhow::Result;
use colored::Colorize;
use std::path::PathBuf;

use crate::commands::helpers::resolve_group_tag;
use crate::storage::Storage;

pub fn run(
    project_root: Option<PathBuf>,
    task_id: &str,
    name: &str,
    tag: Option<&str>,
) -> Result<()> {
    let storage = Storage::new(project_root);
    let epic_tag = resolve_group_tag(&storage, tag, true)?;

    // Use atomic update_group to hold lock across read-modify-write cycle
    // This prevents race conditions where two agents claim the same task
    let mut epic = storage.load_group(&epic_tag)?;

    let task = epic
        .get_task_mut(task_id)
        .ok_or_else(|| anyhow::anyhow!("Task {} not found in epic '{}'", task_id, epic_tag))?;

    // Try to claim the task
    match task.claim(name) {
        Ok(()) => {
            // Get task title before saving (to avoid borrow checker issues)
            let task_title = task.title.clone();

            // Atomic write that holds the lock across read-modify-write
            storage.update_group(&epic_tag, &epic)?;

            println!("{}", "âœ… Task claimed successfully!".green().bold());
            println!();
            println!("{:<20} {}", "Task ID:".yellow(), task_id.cyan());
            println!("{:<20} {}", "Title:".yellow(), task_title.bold());
            println!("{:<20} {}", "Claimed by:".yellow(), name.green());
            println!("{:<20} {}", "Status:".yellow(), "locked".yellow());
            println!();
            println!("{}", "Next steps:".blue());
            println!("  1. Start working on the task");
            println!("  2. Run: scud set-status {} in-progress", task_id);
            println!("  3. When done: scud set-status {} done", task_id);
            println!("  4. Task will auto-release when marked done");
            println!();
        }
        Err(err) => {
            anyhow::bail!("Failed to claim task: {}", err);
        }
    }

    Ok(())
}
</file>

<file path="scud-cli/src/commands/commit.rs">
use anyhow::{Context, Result};
use colored::Colorize;
use std::path::PathBuf;
use std::process::Command;

use crate::storage::Storage;

pub fn run(project_root: Option<PathBuf>, message: Option<&str>, all: bool) -> Result<()> {
    let storage = Storage::new(project_root.clone());

    // Get current task ID from environment or .scud/current-task
    let task_id = get_current_task_id(&storage)?;

    // Get task details if we have a task ID
    let task_context = if let Some(ref id) = task_id {
        get_task_context(&storage, id)
    } else {
        None
    };

    // Build commit message
    let commit_message = build_commit_message(message, task_id.as_deref(), task_context.as_ref())?;

    // Show what we're about to do
    println!("{}", "SCUD Commit".cyan().bold());
    println!("{}", "-".repeat(40).dimmed());

    if let Some(ref id) = task_id {
        println!("Task: {}", id.cyan());
    }
    println!("Message: {}", commit_message.lines().next().unwrap_or(""));

    // Stage files if --all
    if all {
        println!("\n{}", "Staging all changes...".dimmed());
        let status = Command::new("git")
            .args(["add", "-A"])
            .status()
            .context("Failed to run git add")?;

        if !status.success() {
            anyhow::bail!("git add failed");
        }
    }

    // Check if there are staged changes
    let staged = Command::new("git")
        .args(["diff", "--cached", "--quiet"])
        .status()
        .context("Failed to check staged changes")?;

    if staged.success() {
        println!("\n{}", "No staged changes to commit.".yellow());
        println!(
            "Use {} to stage changes, or {} to stage all.",
            "git add <files>".cyan(),
            "scud commit --all".cyan()
        );
        return Ok(());
    }

    // Show staged files
    println!("\n{}", "Staged files:".bold());
    let staged_output = Command::new("git")
        .args(["diff", "--cached", "--name-status"])
        .output()
        .context("Failed to get staged files")?;

    for line in String::from_utf8_lossy(&staged_output.stdout).lines() {
        println!("  {}", line.dimmed());
    }

    // Create commit
    println!("\n{}", "Creating commit...".dimmed());
    let status = Command::new("git")
        .args(["commit", "-m", &commit_message])
        .status()
        .context("Failed to run git commit")?;

    if !status.success() {
        anyhow::bail!("git commit failed");
    }

    println!("\n{} Commit created successfully", "âœ“".green());

    // Show the commit
    let log = Command::new("git")
        .args(["log", "-1", "--oneline"])
        .output()
        .context("Failed to get commit info")?;

    println!("  {}", String::from_utf8_lossy(&log.stdout).trim().dimmed());

    Ok(())
}

fn get_current_task_id(storage: &Storage) -> Result<Option<String>> {
    // First check environment variable
    if let Ok(id) = std::env::var("SCUD_TASK_ID") {
        if !id.is_empty() {
            return Ok(Some(id));
        }
    }

    // Then check .scud/current-task file
    let current_task_file = storage.scud_dir().join("current-task");
    if current_task_file.exists() {
        let content = std::fs::read_to_string(&current_task_file)?;
        let id = content.trim();
        if !id.is_empty() {
            return Ok(Some(id.to_string()));
        }
    }

    Ok(None)
}

struct TaskContext {
    title: String,
    #[allow(dead_code)]
    tag: Option<String>,
}

fn get_task_context(storage: &Storage, task_id: &str) -> Option<TaskContext> {
    // Try to find the task in the active phase first
    if let Ok(Some(tag)) = storage.get_active_group() {
        if let Ok(phase) = storage.load_group(&tag) {
            if let Some(task) = phase.tasks.iter().find(|t| t.id == task_id) {
                return Some(TaskContext {
                    title: task.title.clone(),
                    tag: Some(tag),
                });
            }
        }
    }

    // Search all phases
    if let Ok(all_tasks) = storage.load_tasks() {
        for (tag, phase) in all_tasks {
            if let Some(task) = phase.tasks.iter().find(|t| t.id == task_id) {
                return Some(TaskContext {
                    title: task.title.clone(),
                    tag: Some(tag),
                });
            }
        }
    }

    None
}

fn build_commit_message(
    user_message: Option<&str>,
    task_id: Option<&str>,
    task_context: Option<&TaskContext>,
) -> Result<String> {
    let mut message = String::new();

    // Add task prefix if available
    if let Some(id) = task_id {
        message.push_str(&format!("[{}] ", id));
    }

    // Add user message or task title
    if let Some(msg) = user_message {
        message.push_str(msg);
    } else if let Some(ctx) = task_context {
        // Use task title as default message
        message.push_str(&ctx.title);
    } else {
        anyhow::bail!("No commit message provided and no task context available.\nUse: scud commit -m \"your message\"");
    }

    Ok(message)
}
</file>

<file path="scud-cli/src/commands/config.rs">
use anyhow::Result;
use colored::Colorize;
use std::fs;
use std::path::PathBuf;

use crate::config::Config;
use crate::storage::Storage;

/// SCUD agent definitions
/// Each agent has a filename, aliases for CLI, and description
/// Agents are stored in .claude/commands/scud/<filename>.md
const SCUD_AGENTS: &[(&str, &[&str], &str)] = &[
    (
        "pm",
        &["pm", "scud-pm"],
        "Product Manager - PRD creation and requirements",
    ),
    (
        "sm",
        &["sm", "scud-sm"],
        "Scrum Master - Task breakdown and planning",
    ),
    (
        "architect",
        &["architect", "scud-architect"],
        "Architect - Technical design",
    ),
    (
        "dev",
        &["dev", "scud-dev"],
        "Developer - Task implementation",
    ),
    (
        "retrospective",
        &["retrospective", "scud-retrospective"],
        "Retrospective - Post-phase analysis",
    ),
    ("status", &["status"], "Status - Workflow status reporting"),
];

/// SCUD skill definitions
/// Each skill is a directory containing SKILL.md and supporting files
/// Skills are stored in .claude/skills/<skill-name>/
const SCUD_SKILLS: &[(&str, &[&str], &str)] = &[(
    "scud-tasks",
    &["scud-tasks", "tasks"],
    "Task management - view, update, claim, and track tasks",
)];

/// SCUD task command definitions (Claude Code slash commands)
/// These wrap the CLI for common task operations
/// Commands are stored in .claude/commands/scud/
#[allow(dead_code)]
const SCUD_TASK_COMMANDS: &[&str] = &[
    "task-list",
    "task-next",
    "task-show",
    "task-status",
    "task-claim",
    "task-waves",
    "task-stats",
    "task-whois",
    "task-tags",
    "task-doctor",
];

/// OpenCode command definitions
/// These are the same commands but for OpenCode
/// Commands are stored in .opencode/command/
const OPENCODE_COMMANDS: &[&str] = &[
    "task-list",
    "task-next",
    "task-show",
    "task-status",
    "task-claim",
    "task-release",
    "task-waves",
    "task-stats",
    "task-whois",
    "task-tags",
    "task-doctor",
];

/// OpenCode hook definitions
const OPENCODE_HOOKS: &[&str] = &["session-start"];

/// OpenCode tool definitions
const OPENCODE_TOOLS: &[&str] = &["find_skills", "use_skill"];

pub fn show(project_root: Option<PathBuf>) -> Result<()> {
    let storage = Storage::new(project_root);

    if !storage.is_initialized() {
        println!("{}", "âœ— SCUD is not initialized".red());
        println!("Run: scud init");
        return Ok(());
    }

    let config = storage.load_config()?;

    println!("{}", "Current Configuration:".blue().bold());
    println!();
    println!("  {}: {}", "Provider".yellow(), config.llm.provider);
    println!("  {}: {}", "Model".yellow(), config.llm.model);
    println!("  {}: {}", "Max Tokens".yellow(), config.llm.max_tokens);
    println!();
    println!("{}", "Environment Variable:".blue().bold());
    println!("  {}: {}", "Required".yellow(), config.api_key_env_var());

    // Check if API key is set
    match std::env::var(config.api_key_env_var()) {
        Ok(key) => {
            let masked = format!(
                "{}...{}",
                &key[..10.min(key.len())],
                &key[key.len().saturating_sub(4)..]
            );
            println!(
                "  {}: {} {}",
                "Status".yellow(),
                "Set".green(),
                masked.dimmed()
            );
        }
        Err(_) => {
            println!(
                "  {}: {} (run: export {}=your-key)",
                "Status".yellow(),
                "Not Set".red(),
                config.api_key_env_var()
            );
        }
    }

    println!();
    println!("{}", "Config File:".blue().bold());
    println!("  {}", storage.config_file().display().to_string().dimmed());

    Ok(())
}

pub fn set_provider(
    project_root: Option<PathBuf>,
    provider: &str,
    model: Option<String>,
) -> Result<()> {
    let storage = Storage::new(project_root);

    if !storage.is_initialized() {
        anyhow::bail!("SCUD is not initialized. Run: scud init");
    }

    // Validate provider
    let provider = provider.to_lowercase();
    if !matches!(
        provider.as_str(),
        "xai" | "anthropic" | "openai" | "openrouter" | "claude-cli"
    ) {
        anyhow::bail!(
            "Invalid provider: {}. Valid options: xai, anthropic, openai, openrouter, claude-cli",
            provider
        );
    }

    let mut config = storage.load_config()?;
    config.llm.provider = provider.clone();

    // Set model - use provided or default for provider
    config.llm.model =
        model.unwrap_or_else(|| Config::default_model_for_provider(&provider).to_string());

    // Save config
    config.save(&storage.config_file())?;

    println!("{}", "âœ… Configuration updated!".green().bold());
    println!();
    println!("  {}: {}", "Provider".yellow(), config.llm.provider);
    println!("  {}: {}", "Model".yellow(), config.llm.model);
    println!();

    if config.requires_api_key() {
        println!("{}", "Remember to set your API key:".blue());
        println!(
            "  export {}=your-api-key",
            config.api_key_env_var().yellow()
        );
    } else {
        println!("{}", "Using Claude CLI (no API key required)".green());
        println!(
            "{}",
            "Make sure 'claude' command is available in your PATH".blue()
        );
    }

    Ok(())
}

/// Normalize agent name - accepts aliases like scud-pm, pm, architect, etc.
fn normalize_agent_name(name: &str) -> Option<&'static str> {
    let name_lower = name.to_lowercase();
    for (filename, aliases, _) in SCUD_AGENTS {
        for alias in *aliases {
            if name_lower == *alias {
                return Some(filename);
            }
        }
    }
    None
}

/// Normalize skill name - accepts aliases like scud-tasks, tasks, etc.
fn normalize_skill_name(name: &str) -> Option<&'static str> {
    let name_lower = name.to_lowercase();
    for (dirname, aliases, _) in SCUD_SKILLS {
        for alias in *aliases {
            if name_lower == *alias {
                return Some(dirname);
            }
        }
    }
    None
}

/// Get the scud commands directory path (.claude/commands/scud/)
fn get_scud_commands_dir(project_root: Option<PathBuf>) -> PathBuf {
    let base = project_root.unwrap_or_else(|| std::env::current_dir().unwrap_or_default());
    base.join(".claude").join("commands").join("scud")
}

/// Get the skills directory path (.claude/skills/)
fn get_skills_dir(project_root: Option<PathBuf>) -> PathBuf {
    let base = project_root.unwrap_or_else(|| std::env::current_dir().unwrap_or_default());
    base.join(".claude").join("skills")
}

/// Get the OpenCode command directory path (.opencode/command/)
fn get_opencode_command_dir(project_root: Option<PathBuf>) -> PathBuf {
    let base = project_root.unwrap_or_else(|| std::env::current_dir().unwrap_or_default());
    base.join(".opencode").join("command")
}

/// Get the OpenCode hook directory path (.opencode/hook/)
fn get_opencode_hook_dir(project_root: Option<PathBuf>) -> PathBuf {
    let base = project_root.unwrap_or_else(|| std::env::current_dir().unwrap_or_default());
    base.join(".opencode").join("hook")
}

/// Get the OpenCode tool directory path (.opencode/tool/)
fn get_opencode_tool_dir(project_root: Option<PathBuf>) -> PathBuf {
    let base = project_root.unwrap_or_else(|| std::env::current_dir().unwrap_or_default());
    base.join(".opencode").join("tool")
}

/// Get the OpenCode skills directory path (.opencode/skills/)
fn get_opencode_skills_dir(project_root: Option<PathBuf>) -> PathBuf {
    let base = project_root.unwrap_or_else(|| std::env::current_dir().unwrap_or_default());
    base.join(".opencode").join("skills")
}

/// Get the package root directory (contains .claude/commands/scud/ and .claude/skills/)
fn get_package_root_dir() -> Option<PathBuf> {
    // Try to find the scud-task npm package directory
    // The package is identified by having bin/install.js (the npm package's install script)
    // This handles both development (repo root) and installed (node_modules) scenarios
    let current_exe = std::env::current_exe().ok()?;

    // Search up from the executable location
    let mut search_dir = current_exe.parent()?;

    // Search up the directory tree for the npm package root
    for _ in 0..10 {
        let install_script = search_dir.join("bin").join("install.js");
        let scud_dir = search_dir.join(".claude").join("commands").join("scud");

        // Found the scud-task npm package (has bin/install.js and scud agents)
        if install_script.exists() && scud_dir.exists() && scud_dir.join("pm.md").exists() {
            return Some(search_dir.to_path_buf());
        }
        search_dir = search_dir.parent()?;
    }

    None
}

/// Get the package's agent source directory (.claude/commands/scud/)
fn get_package_agents_dir() -> Option<PathBuf> {
    get_package_root_dir().map(|root| root.join(".claude").join("commands").join("scud"))
}

/// Get the package's skills source directory (.claude/skills/)
fn get_package_skills_dir() -> Option<PathBuf> {
    get_package_root_dir().map(|root| root.join(".claude").join("skills"))
}

/// Get the package's OpenCode command source directory (.opencode/command/)
fn get_package_opencode_command_dir() -> Option<PathBuf> {
    get_package_root_dir().map(|root| root.join(".opencode").join("command"))
}

/// Get the package's OpenCode hook source directory (.opencode/hook/)
fn get_package_opencode_hook_dir() -> Option<PathBuf> {
    get_package_root_dir().map(|root| root.join(".opencode").join("hook"))
}

/// Get the package's OpenCode tool source directory (.opencode/tool/)
fn get_package_opencode_tool_dir() -> Option<PathBuf> {
    get_package_root_dir().map(|root| root.join(".opencode").join("tool"))
}

/// List installed SCUD agents
pub fn agents_list(project_root: Option<PathBuf>) -> Result<()> {
    let scud_dir = get_scud_commands_dir(project_root.clone());
    let skills_dir = get_skills_dir(project_root.clone());

    // Agents section
    println!("{}", "SCUD Workflow Agents".blue().bold());
    println!("{}", "Location: .claude/commands/scud/".dimmed());
    println!();

    let mut agents_installed = 0;
    let mut agents_not_installed = 0;

    for (filename, aliases, description) in SCUD_AGENTS {
        let agent_file = scud_dir.join(format!("{}.md", filename));
        let installed = agent_file.exists();
        let alias_str = aliases.join(", ");

        if installed {
            agents_installed += 1;
            println!(
                "  {} {} ({}) - {}",
                "âœ“".green(),
                filename.green(),
                alias_str.dimmed(),
                description
            );
        } else {
            agents_not_installed += 1;
            println!(
                "  {} {} ({}) - {}",
                "âœ—".red(),
                filename.dimmed(),
                alias_str.dimmed(),
                description
            );
        }
    }

    println!();
    println!(
        "{} installed, {} not installed",
        agents_installed.to_string().green(),
        agents_not_installed.to_string().yellow()
    );

    // Skills section
    println!();
    println!("{}", "SCUD Skills".blue().bold());
    println!("{}", "Location: .claude/skills/".dimmed());
    println!();

    let mut skills_installed = 0;
    let mut skills_not_installed = 0;

    for (dirname, aliases, description) in SCUD_SKILLS {
        let skill_dir = skills_dir.join(dirname);
        let skill_file = skill_dir.join("SKILL.md");
        let installed = skill_file.exists();
        let alias_str = aliases.join(", ");

        if installed {
            skills_installed += 1;
            println!(
                "  {} {} ({}) - {}",
                "âœ“".green(),
                dirname.green(),
                alias_str.dimmed(),
                description
            );
        } else {
            skills_not_installed += 1;
            println!(
                "  {} {} ({}) - {}",
                "âœ—".red(),
                dirname.dimmed(),
                alias_str.dimmed(),
                description
            );
        }
    }

    println!();
    println!(
        "{} installed, {} not installed",
        skills_installed.to_string().green(),
        skills_not_installed.to_string().yellow()
    );

    // OpenCode section
    println!();
    println!("{}", "OpenCode Integration".blue().bold());
    println!("{}", "Location: .opencode/".dimmed());
    println!();

    let opencode_cmd_dir = get_opencode_command_dir(project_root.clone());
    let opencode_hook_dir = get_opencode_hook_dir(project_root.clone());
    let opencode_tool_dir = get_opencode_tool_dir(project_root);

    let mut opencode_installed = 0;

    // Check commands
    for cmd in OPENCODE_COMMANDS {
        let cmd_file = opencode_cmd_dir.join(format!("{}.md", cmd));
        if cmd_file.exists() {
            opencode_installed += 1;
        }
    }

    // Check hooks
    for hook in OPENCODE_HOOKS {
        let hook_file = opencode_hook_dir.join(format!("{}.md", hook));
        if hook_file.exists() {
            opencode_installed += 1;
        }
    }

    // Check tools
    for tool in OPENCODE_TOOLS {
        let tool_file = opencode_tool_dir.join(format!("{}.json", tool));
        if tool_file.exists() {
            opencode_installed += 1;
        }
    }

    if opencode_installed > 0 {
        println!(
            "  {} {} commands, {} hooks, {} tools installed",
            "âœ“".green(),
            OPENCODE_COMMANDS
                .iter()
                .filter(|c| opencode_cmd_dir.join(format!("{}.md", c)).exists())
                .count(),
            OPENCODE_HOOKS
                .iter()
                .filter(|h| opencode_hook_dir.join(format!("{}.md", h)).exists())
                .count(),
            OPENCODE_TOOLS
                .iter()
                .filter(|t| opencode_tool_dir.join(format!("{}.json", t)).exists())
                .count(),
        );
    } else {
        println!("  {} Not installed", "âœ—".red());
    }

    println!();
    println!("{}", "Usage:".blue().bold());
    println!("  scud config agents add <name>     Add an agent or skill");
    println!("  scud config agents add --all      Add all agents, skills, and OpenCode support");
    println!("  scud config agents remove <name>  Remove an agent or skill");
    println!("  scud config agents remove --all   Remove all agents, skills, and OpenCode support");

    Ok(())
}

/// Recursively copy a directory
fn copy_dir_recursive(src: &PathBuf, dst: &PathBuf) -> Result<()> {
    fs::create_dir_all(dst)?;
    for entry in fs::read_dir(src)? {
        let entry = entry?;
        let src_path = entry.path();
        let dst_path = dst.join(entry.file_name());

        if src_path.is_dir() {
            copy_dir_recursive(&src_path, &dst_path)?;
        } else {
            fs::copy(&src_path, &dst_path)?;
        }
    }
    Ok(())
}

/// Add SCUD agent(s), skill(s), and OpenCode integration
pub fn agents_add(project_root: Option<PathBuf>, name: Option<String>, all: bool) -> Result<()> {
    if !all && name.is_none() {
        anyhow::bail!("Please specify an agent/skill name or use --all to add all");
    }

    let package_agents_dir = get_package_agents_dir().ok_or_else(|| {
        anyhow::anyhow!(
            "Could not find SCUD package agent files. Make sure scud-task is installed."
        )
    })?;

    let package_skills_dir = get_package_skills_dir();
    let package_opencode_cmd_dir = get_package_opencode_command_dir();
    let package_opencode_hook_dir = get_package_opencode_hook_dir();
    let package_opencode_tool_dir = get_package_opencode_tool_dir();

    let scud_dir = get_scud_commands_dir(project_root.clone());
    let skills_dir = get_skills_dir(project_root.clone());
    let opencode_cmd_dir = get_opencode_command_dir(project_root.clone());
    let opencode_hook_dir = get_opencode_hook_dir(project_root.clone());
    let opencode_tool_dir = get_opencode_tool_dir(project_root.clone());
    let opencode_skills_dir = get_opencode_skills_dir(project_root);

    // Ensure directories exist
    fs::create_dir_all(&scud_dir)?;
    fs::create_dir_all(&skills_dir)?;

    let mut agents_added = 0;
    let mut agents_already_exist = 0;
    let mut skills_added = 0;
    let mut skills_already_exist = 0;
    let mut opencode_added = 0;
    let mut opencode_already_exist = 0;

    // Determine what to add
    let (agents_to_add, skills_to_add): (Vec<&str>, Vec<&str>) = if all {
        (
            SCUD_AGENTS
                .iter()
                .map(|(filename, _, _)| *filename)
                .collect(),
            SCUD_SKILLS.iter().map(|(dirname, _, _)| *dirname).collect(),
        )
    } else {
        let name_ref = name.as_ref().unwrap();
        // Try agent first, then skill
        if let Some(agent) = normalize_agent_name(name_ref) {
            (vec![agent], vec![])
        } else if let Some(skill) = normalize_skill_name(name_ref) {
            (vec![], vec![skill])
        } else {
            anyhow::bail!(
                "Unknown agent/skill: '{}'. Valid agents: pm, sm, architect, dev, retrospective, status. Valid skills: scud-tasks",
                name_ref
            );
        }
    };

    // Add agents
    if !agents_to_add.is_empty() {
        println!("{}", "Agents:".blue().bold());
        for agent_name in &agents_to_add {
            let source = package_agents_dir.join(format!("{}.md", agent_name));
            let dest = scud_dir.join(format!("{}.md", agent_name));

            if dest.exists() {
                agents_already_exist += 1;
                println!("  {} {} (already installed)", "Â·".yellow(), agent_name);
                continue;
            }

            if !source.exists() {
                println!("  {} {} (source not found)", "âœ—".red(), agent_name);
                continue;
            }

            fs::copy(&source, &dest)?;
            agents_added += 1;
            println!("  {} {}", "âœ“".green(), agent_name.green());
        }
    }

    // Add skills
    if !skills_to_add.is_empty() {
        if let Some(ref pkg_skills) = package_skills_dir {
            println!("{}", "Skills:".blue().bold());
            for skill_name in &skills_to_add {
                let source = pkg_skills.join(skill_name);
                let dest = skills_dir.join(skill_name);

                if dest.join("SKILL.md").exists() {
                    skills_already_exist += 1;
                    println!("  {} {} (already installed)", "Â·".yellow(), skill_name);
                    continue;
                }

                if !source.exists() || !source.join("SKILL.md").exists() {
                    println!("  {} {} (source not found)", "âœ—".red(), skill_name);
                    continue;
                }

                copy_dir_recursive(&source, &dest)?;
                skills_added += 1;
                println!("  {} {}", "âœ“".green(), skill_name.green());

                // Also copy skill to OpenCode skills directory
                let opencode_dest = opencode_skills_dir.join(skill_name);
                if !opencode_dest.join("SKILL.md").exists() {
                    fs::create_dir_all(&opencode_skills_dir)?;
                    copy_dir_recursive(&source, &opencode_dest)?;
                }
            }
        } else if !skills_to_add.is_empty() {
            println!(
                "{}",
                "Skills directory not found in package".yellow().dimmed()
            );
        }
    }

    // Add OpenCode integration (only when --all)
    if all {
        println!("{}", "OpenCode:".blue().bold());

        // Ensure OpenCode directories exist
        fs::create_dir_all(&opencode_cmd_dir)?;
        fs::create_dir_all(&opencode_hook_dir)?;
        fs::create_dir_all(&opencode_tool_dir)?;

        // Add commands
        if let Some(ref pkg_cmd_dir) = package_opencode_cmd_dir {
            for cmd in OPENCODE_COMMANDS {
                let source = pkg_cmd_dir.join(format!("{}.md", cmd));
                let dest = opencode_cmd_dir.join(format!("{}.md", cmd));

                if dest.exists() {
                    opencode_already_exist += 1;
                    continue;
                }

                if source.exists() {
                    fs::copy(&source, &dest)?;
                    opencode_added += 1;
                }
            }
        }

        // Add hooks
        if let Some(ref pkg_hook_dir) = package_opencode_hook_dir {
            for hook in OPENCODE_HOOKS {
                let source = pkg_hook_dir.join(format!("{}.md", hook));
                let dest = opencode_hook_dir.join(format!("{}.md", hook));

                if dest.exists() {
                    opencode_already_exist += 1;
                    continue;
                }

                if source.exists() {
                    fs::copy(&source, &dest)?;
                    opencode_added += 1;
                }
            }
        }

        // Add tools
        if let Some(ref pkg_tool_dir) = package_opencode_tool_dir {
            for tool in OPENCODE_TOOLS {
                let source = pkg_tool_dir.join(format!("{}.json", tool));
                let dest = opencode_tool_dir.join(format!("{}.json", tool));

                if dest.exists() {
                    opencode_already_exist += 1;
                    continue;
                }

                if source.exists() {
                    fs::copy(&source, &dest)?;
                    opencode_added += 1;
                }
            }
        }

        if opencode_added > 0 {
            println!("  {} {} files installed", "âœ“".green(), opencode_added);
        }
        if opencode_already_exist > 0 {
            println!(
                "  {} {} files already installed",
                "Â·".yellow(),
                opencode_already_exist
            );
        }
    }

    println!();
    let total_added = agents_added + skills_added + opencode_added;
    let total_existing = agents_already_exist + skills_already_exist + opencode_already_exist;

    if total_added > 0 {
        println!(
            "{}",
            format!("âœ… Added {} item(s)", total_added).green().bold()
        );
    }
    if total_existing > 0 {
        println!(
            "{}",
            format!("{} item(s) already installed", total_existing).yellow()
        );
    }

    Ok(())
}

/// Recursively remove a directory
fn remove_dir_recursive(path: &PathBuf) -> Result<()> {
    if path.exists() {
        fs::remove_dir_all(path)?;
    }
    Ok(())
}

/// Remove SCUD agent(s), skill(s), and OpenCode integration
pub fn agents_remove(project_root: Option<PathBuf>, name: Option<String>, all: bool) -> Result<()> {
    if !all && name.is_none() {
        anyhow::bail!("Please specify an agent/skill name or use --all to remove all");
    }

    let scud_dir = get_scud_commands_dir(project_root.clone());
    let skills_dir = get_skills_dir(project_root.clone());
    let opencode_cmd_dir = get_opencode_command_dir(project_root.clone());
    let opencode_hook_dir = get_opencode_hook_dir(project_root.clone());
    let opencode_tool_dir = get_opencode_tool_dir(project_root.clone());
    let opencode_skills_dir = get_opencode_skills_dir(project_root);

    let mut agents_removed = 0;
    let mut agents_not_found = 0;
    let mut skills_removed = 0;
    let mut skills_not_found = 0;
    let mut opencode_removed = 0;

    // Determine what to remove
    let (agents_to_remove, skills_to_remove): (Vec<&str>, Vec<&str>) = if all {
        (
            SCUD_AGENTS
                .iter()
                .map(|(filename, _, _)| *filename)
                .collect(),
            SCUD_SKILLS.iter().map(|(dirname, _, _)| *dirname).collect(),
        )
    } else {
        let name_ref = name.as_ref().unwrap();
        // Try agent first, then skill
        if let Some(agent) = normalize_agent_name(name_ref) {
            (vec![agent], vec![])
        } else if let Some(skill) = normalize_skill_name(name_ref) {
            (vec![], vec![skill])
        } else {
            anyhow::bail!(
                "Unknown agent/skill: '{}'. Valid agents: pm, sm, architect, dev, retrospective, status. Valid skills: scud-tasks",
                name_ref
            );
        }
    };

    // Remove agents
    if !agents_to_remove.is_empty() {
        println!("{}", "Agents:".blue().bold());
        for agent_name in &agents_to_remove {
            let agent_file = scud_dir.join(format!("{}.md", agent_name));

            if !agent_file.exists() {
                agents_not_found += 1;
                println!("  {} {} (not installed)", "Â·".yellow(), agent_name);
                continue;
            }

            fs::remove_file(&agent_file)?;
            agents_removed += 1;
            println!("  {} {}", "âœ“".green(), agent_name);
        }
    }

    // Remove skills
    if !skills_to_remove.is_empty() {
        println!("{}", "Skills:".blue().bold());
        for skill_name in &skills_to_remove {
            let skill_dir = skills_dir.join(skill_name);

            if !skill_dir.exists() {
                skills_not_found += 1;
                println!("  {} {} (not installed)", "Â·".yellow(), skill_name);
                continue;
            }

            remove_dir_recursive(&skill_dir)?;
            skills_removed += 1;
            println!("  {} {}", "âœ“".green(), skill_name);

            // Also remove from OpenCode skills directory
            let opencode_skill = opencode_skills_dir.join(skill_name);
            if opencode_skill.exists() {
                remove_dir_recursive(&opencode_skill)?;
            }
        }
    }

    // Remove OpenCode integration (only when --all)
    if all {
        println!("{}", "OpenCode:".blue().bold());

        // Remove commands
        for cmd in OPENCODE_COMMANDS {
            let cmd_file = opencode_cmd_dir.join(format!("{}.md", cmd));
            if cmd_file.exists() {
                fs::remove_file(&cmd_file)?;
                opencode_removed += 1;
            }
        }

        // Remove hooks
        for hook in OPENCODE_HOOKS {
            let hook_file = opencode_hook_dir.join(format!("{}.md", hook));
            if hook_file.exists() {
                fs::remove_file(&hook_file)?;
                opencode_removed += 1;
            }
        }

        // Remove tools
        for tool in OPENCODE_TOOLS {
            let tool_file = opencode_tool_dir.join(format!("{}.json", tool));
            if tool_file.exists() {
                fs::remove_file(&tool_file)?;
                opencode_removed += 1;
            }
        }

        if opencode_removed > 0 {
            println!("  {} {} files removed", "âœ“".green(), opencode_removed);
        } else {
            println!("  {} Not installed", "Â·".yellow());
        }
    }

    println!();
    let total_removed = agents_removed + skills_removed + opencode_removed;
    let total_not_found = agents_not_found + skills_not_found;

    if total_removed > 0 {
        println!(
            "{}",
            format!("âœ… Removed {} item(s)", total_removed)
                .green()
                .bold()
        );
    }
    if total_not_found > 0 {
        println!(
            "{}",
            format!("{} item(s) were not installed", total_not_found).yellow()
        );
    }

    Ok(())
}
</file>

<file path="scud-cli/src/commands/convert.rs">
//! Convert between task storage formats

use anyhow::{Context, Result};
use colored::Colorize;
use std::collections::HashMap;
use std::fs;
use std::path::PathBuf;

use crate::formats::{parse_scg, serialize_scg, Format};
use crate::models::Phase;
use crate::storage::Storage;

pub fn run(
    project_root: Option<PathBuf>,
    from_format: &str,
    to_format: &str,
    backup: bool,
) -> Result<()> {
    let from = Format::from_extension(from_format)
        .ok_or_else(|| anyhow::anyhow!("Unknown format: {}", from_format))?;
    let to = Format::from_extension(to_format)
        .ok_or_else(|| anyhow::anyhow!("Unknown format: {}", to_format))?;

    if from == to {
        println!("{}", "Source and target formats are the same".yellow());
        return Ok(());
    }

    // SCG â†’ JSON conversion is blocked because the CLI only reads tasks.scg
    // Converting would break all storage operations
    if from == Format::Scg && to == Format::Json {
        anyhow::bail!(
            "SCG to JSON conversion is not supported.\n\
             The SCUD CLI requires tasks.scg for storage.\n\
             Use 'scud show' or 'scud list' to view tasks."
        );
    }

    let storage = Storage::new(project_root);
    let taskmaster_dir = storage.scud_dir();
    let tasks_dir = taskmaster_dir.join("tasks");

    // Determine source file
    let source_file = tasks_dir.join(format!("tasks.{}", from.extension()));
    let target_file = tasks_dir.join(format!("tasks.{}", to.extension()));

    if !source_file.exists() {
        anyhow::bail!(
            "Source file not found: {}\nExpected format: {}",
            source_file.display(),
            from_format
        );
    }

    println!(
        "{} {} -> {}",
        "Converting".blue(),
        source_file.display(),
        target_file.display()
    );

    // Read source
    let content = fs::read_to_string(&source_file)
        .with_context(|| format!("Failed to read {}", source_file.display()))?;

    // Parse based on source format
    let phases: HashMap<String, Phase> = match from {
        Format::Json => serde_json::from_str(&content).with_context(|| "Failed to parse JSON")?,
        Format::Scg => {
            // Parse multi-phase SCG
            parse_multi_phase_scg(&content)?
        }
    };

    println!("  {} phase(s) found", phases.len());
    for (tag, phase) in &phases {
        println!("    {} {} tasks", tag.cyan(), phase.tasks.len());
    }

    // Serialize to target format
    let output = match to {
        Format::Json => {
            serde_json::to_string_pretty(&phases).with_context(|| "Failed to serialize to JSON")?
        }
        Format::Scg => {
            let mut out = String::new();
            let mut sorted_tags: Vec<_> = phases.keys().collect();
            sorted_tags.sort();

            for (i, tag) in sorted_tags.iter().enumerate() {
                if i > 0 {
                    out.push_str("\n---\n\n");
                }
                let phase = phases.get(*tag).unwrap();
                out.push_str(&serialize_scg(phase));
            }
            out
        }
    };

    // Backup if requested
    if backup && source_file.exists() {
        let backup_file = tasks_dir.join(format!("tasks.{}.backup", from.extension()));
        fs::copy(&source_file, &backup_file)
            .with_context(|| format!("Failed to create backup at {}", backup_file.display()))?;
        println!(
            "  {} Backup created: {}",
            "âœ“".green(),
            backup_file.display()
        );
    }

    // Write target
    fs::write(&target_file, &output)
        .with_context(|| format!("Failed to write {}", target_file.display()))?;

    // Remove source if different file
    if source_file != target_file {
        fs::remove_file(&source_file)
            .with_context(|| format!("Failed to remove old file {}", source_file.display()))?;
    }

    println!();
    println!("{}", "Conversion complete!".green().bold());
    println!();
    println!("{}", "Verify with:".blue());
    println!("  scud list");
    println!("  scud stats");

    Ok(())
}

fn parse_multi_phase_scg(content: &str) -> Result<HashMap<String, Phase>> {
    let mut phases = HashMap::new();

    // Empty content returns empty map
    if content.trim().is_empty() {
        return Ok(phases);
    }

    // Split by phase separator (---)
    let sections: Vec<&str> = content.split("\n---\n").collect();

    for section in sections {
        let section = section.trim();
        if section.is_empty() {
            continue;
        }

        let phase = parse_scg(section).with_context(|| "Failed to parse SCG section")?;

        phases.insert(phase.name.clone(), phase);
    }

    Ok(phases)
}
</file>

<file path="scud-cli/src/commands/doctor.rs">
use anyhow::Result;
use colored::Colorize;
use std::collections::HashSet;
use std::path::PathBuf;

use crate::models::task::TaskStatus;
use crate::storage::Storage;

/// Diagnostic issue severity
#[derive(Debug, Clone, PartialEq, Eq)]
pub enum Severity {
    Warning,
    Error,
    Critical,
}

impl Severity {
    pub fn as_str(&self) -> &'static str {
        match self {
            Severity::Warning => "WARNING",
            Severity::Error => "ERROR",
            Severity::Critical => "CRITICAL",
        }
    }
}

/// A diagnostic issue found by the doctor command
#[derive(Debug, Clone)]
pub struct DiagnosticIssue {
    pub severity: Severity,
    pub epic_tag: String,
    pub task_id: Option<String>,
    pub message: String,
    pub suggestion: String,
}

/// Results from running diagnostics
#[derive(Debug, Default)]
pub struct DiagnosticResults {
    pub issues: Vec<DiagnosticIssue>,
    pub stale_locks: Vec<(String, String, String, f64)>, // (epic, task_id, locked_by, hours)
    pub blocked_by_cancelled: Vec<(String, String, String)>, // (epic, task_id, blocked_dep)
    pub blocked_by_missing: Vec<(String, String, String)>, // (epic, task_id, missing_dep)
    pub orphan_in_progress: Vec<(String, String)>, // (epic, task_id) - in-progress >24h without lock
    pub missing_active_epic: bool,
    pub corrupt_files: Vec<String>,
}

impl DiagnosticResults {
    pub fn has_issues(&self) -> bool {
        !self.issues.is_empty()
            || !self.stale_locks.is_empty()
            || !self.blocked_by_cancelled.is_empty()
            || !self.blocked_by_missing.is_empty()
            || !self.orphan_in_progress.is_empty()
            || self.missing_active_epic
            || !self.corrupt_files.is_empty()
    }

    pub fn critical_count(&self) -> usize {
        self.issues
            .iter()
            .filter(|i| i.severity == Severity::Critical)
            .count()
            + self.corrupt_files.len()
    }

    pub fn error_count(&self) -> usize {
        self.issues
            .iter()
            .filter(|i| i.severity == Severity::Error)
            .count()
            + self.blocked_by_cancelled.len()
            + self.blocked_by_missing.len()
    }

    pub fn warning_count(&self) -> usize {
        self.issues
            .iter()
            .filter(|i| i.severity == Severity::Warning)
            .count()
            + self.stale_locks.len()
            + self.orphan_in_progress.len()
            + if self.missing_active_epic { 1 } else { 0 }
    }
}

pub fn run(
    project_root: Option<PathBuf>,
    tag: Option<&str>,
    stale_hours: f64,
    fix: bool,
) -> Result<()> {
    println!(
        "{}",
        "[EXPERIMENTAL] SCUD Doctor - Workflow Diagnostics"
            .blue()
            .bold()
    );
    println!("{}", "=".repeat(60).blue());
    println!();

    let storage = Storage::new(project_root);

    // Check if storage files exist and are readable
    let tasks_result = storage.load_tasks();

    let mut results = DiagnosticResults::default();

    // Check for corrupt/missing files
    if let Err(ref e) = tasks_result {
        results.corrupt_files.push(format!("tasks file: {}", e));
    }

    // Check active epic
    match storage.get_active_group() {
        Ok(Some(_)) => {}
        Ok(None) => {
            results.missing_active_epic = true;
        }
        Err(_) => {
            results.missing_active_epic = true;
        }
    }

    // If we couldn't load tasks, show what we found and exit
    if !results.corrupt_files.is_empty() {
        print_results(&results, fix);
        return Ok(());
    }

    let mut all_tasks = tasks_result?;

    // Filter to specific tag if provided
    let epic_tags: Vec<String> = if let Some(t) = tag {
        if all_tasks.contains_key(t) {
            vec![t.to_string()]
        } else {
            anyhow::bail!("Phase '{}' not found", t);
        }
    } else {
        all_tasks.keys().cloned().collect()
    };

    // Run diagnostics on each epic
    for epic_tag in &epic_tags {
        let epic = match all_tasks.get(epic_tag) {
            Some(e) => e,
            None => continue,
        };

        // Build set of all task IDs for dependency checking
        let all_task_ids: HashSet<_> = epic.tasks.iter().map(|t| t.id.clone()).collect();

        for task in &epic.tasks {
            // Check for stale locks
            if task.is_stale_lock(stale_hours) {
                if let (Some(locked_by), Some(hours)) = (&task.locked_by, task.lock_age_hours()) {
                    results.stale_locks.push((
                        epic_tag.clone(),
                        task.id.clone(),
                        locked_by.clone(),
                        hours,
                    ));
                }
            }

            // Check for orphan in-progress tasks (in-progress but not locked, >24h old)
            if task.status == TaskStatus::InProgress && !task.is_locked() {
                if let Some(ref updated_at) = task.updated_at {
                    if let Ok(dt) = chrono::DateTime::parse_from_rfc3339(updated_at) {
                        let hours =
                            (chrono::Utc::now().signed_duration_since(dt)).num_hours() as f64;
                        if hours > stale_hours {
                            results
                                .orphan_in_progress
                                .push((epic_tag.clone(), task.id.clone()));
                        }
                    }
                }
            }

            // Check for dependencies on cancelled/blocked tasks
            if task.status == TaskStatus::Pending {
                for dep_id in &task.dependencies {
                    // Check if dependency exists
                    if !all_task_ids.contains(dep_id) {
                        results.blocked_by_missing.push((
                            epic_tag.clone(),
                            task.id.clone(),
                            dep_id.clone(),
                        ));
                        continue;
                    }

                    // Check if dependency is cancelled or blocked
                    if let Some(dep_task) = epic.get_task(dep_id) {
                        match dep_task.status {
                            TaskStatus::Cancelled => {
                                results.blocked_by_cancelled.push((
                                    epic_tag.clone(),
                                    task.id.clone(),
                                    dep_id.clone(),
                                ));
                            }
                            TaskStatus::Blocked => {
                                results.issues.push(DiagnosticIssue {
                                    severity: Severity::Warning,
                                    epic_tag: epic_tag.clone(),
                                    task_id: Some(task.id.clone()),
                                    message: format!(
                                        "Task {} depends on blocked task {}",
                                        task.id, dep_id
                                    ),
                                    suggestion: format!(
                                        "Resolve blocker for {} or remove dependency",
                                        dep_id
                                    ),
                                });
                            }
                            TaskStatus::Deferred => {
                                results.issues.push(DiagnosticIssue {
                                    severity: Severity::Warning,
                                    epic_tag: epic_tag.clone(),
                                    task_id: Some(task.id.clone()),
                                    message: format!(
                                        "Task {} depends on deferred task {}",
                                        task.id, dep_id
                                    ),
                                    suggestion: format!("Un-defer {} or update dependency", dep_id),
                                });
                            }
                            _ => {}
                        }
                    }
                }
            }
        }
    }

    // Apply fixes if requested
    if fix && results.has_issues() {
        println!("{}", "Attempting auto-fixes...".yellow());
        println!();

        let mut fixed_count = 0;

        // Fix stale locks
        for (epic_tag, task_id, locked_by, hours) in &results.stale_locks {
            if let Some(epic) = all_tasks.get_mut(epic_tag) {
                if let Some(task) = epic.get_task_mut(task_id) {
                    task.release();
                    println!(
                        "{} Released stale lock: {} (was locked by {} for {:.1}h)",
                        "âœ“".green(),
                        task_id.cyan(),
                        locked_by,
                        hours
                    );
                    fixed_count += 1;
                }
            }
        }

        // Fix orphan in-progress tasks (reset to pending)
        for (epic_tag, task_id) in &results.orphan_in_progress {
            if let Some(epic) = all_tasks.get_mut(epic_tag) {
                if let Some(task) = epic.get_task_mut(task_id) {
                    task.set_status(TaskStatus::Pending);
                    println!(
                        "{} Reset orphan task to pending: {}",
                        "âœ“".green(),
                        task_id.cyan()
                    );
                    fixed_count += 1;
                }
            }
        }

        if fixed_count > 0 {
            storage.save_tasks(&all_tasks)?;
            println!();
            println!("{} {} issue(s) fixed", "âœ“".green(), fixed_count);
        } else {
            println!(
                "{}",
                "No auto-fixable issues found. Manual intervention required.".yellow()
            );
        }
        println!();
    }

    print_results(&results, fix);

    Ok(())
}

fn print_results(results: &DiagnosticResults, fix_attempted: bool) {
    if !results.has_issues() {
        println!(
            "{}",
            "âœ“ No issues found! Workflow is healthy.".green().bold()
        );
        return;
    }

    // Print critical issues (corrupt files)
    if !results.corrupt_files.is_empty() {
        println!("{}", "CRITICAL: File Issues".red().bold());
        println!("{}", "-".repeat(40).red());
        for file_issue in &results.corrupt_files {
            println!("  {} {}", "âœ—".red(), file_issue);
        }
        println!();
        print_recovery_instructions();
        return;
    }

    // Print stale locks
    if !results.stale_locks.is_empty() {
        println!("{}", "Stale Locks (>threshold hours)".yellow().bold());
        println!("{}", "-".repeat(40).yellow());
        for (epic, task_id, locked_by, hours) in &results.stale_locks {
            println!(
                "  {} {} in {} locked by {} ({:.1}h)",
                "âš ".yellow(),
                task_id.cyan(),
                epic.dimmed(),
                locked_by.green(),
                hours
            );
            if !fix_attempted {
                println!(
                    "    {}",
                    format!("â†’ scud release {} --force -e {}", task_id, epic).dimmed()
                );
            }
        }
        println!();
    }

    // Print blocked by cancelled
    if !results.blocked_by_cancelled.is_empty() {
        println!("{}", "Tasks Blocked by Cancelled Dependencies".red().bold());
        println!("{}", "-".repeat(40).red());
        for (epic, task_id, dep_id) in &results.blocked_by_cancelled {
            println!(
                "  {} {} depends on cancelled task {}",
                "âœ—".red(),
                task_id.cyan(),
                dep_id.yellow()
            );
            println!(
                "    {}",
                format!(
                    "â†’ Remove dependency or un-cancel {} (in epic {})",
                    dep_id, epic
                )
                .dimmed()
            );
        }
        println!();
    }

    // Print blocked by missing
    if !results.blocked_by_missing.is_empty() {
        println!("{}", "Tasks with Missing Dependencies".red().bold());
        println!("{}", "-".repeat(40).red());
        for (epic, task_id, dep_id) in &results.blocked_by_missing {
            println!(
                "  {} {} depends on non-existent task {}",
                "âœ—".red(),
                task_id.cyan(),
                dep_id.yellow()
            );
            println!(
                "    {}",
                format!("â†’ Remove dependency from {} (in epic {})", task_id, epic).dimmed()
            );
        }
        println!();
    }

    // Print orphan in-progress
    if !results.orphan_in_progress.is_empty() {
        println!(
            "{}",
            "Orphan In-Progress Tasks (no lock, stale)".yellow().bold()
        );
        println!("{}", "-".repeat(40).yellow());
        for (epic, task_id) in &results.orphan_in_progress {
            println!(
                "  {} {} in {} - in-progress but not locked",
                "âš ".yellow(),
                task_id.cyan(),
                epic.dimmed()
            );
            if !fix_attempted {
                println!(
                    "    {}",
                    format!(
                        "â†’ scud set-status {} pending -t {}  # or done if complete",
                        task_id, epic
                    )
                    .dimmed()
                );
            }
        }
        println!();
    }

    // Print missing active epic
    if results.missing_active_epic {
        println!("{}", "No Active Phase Set".yellow().bold());
        println!("{}", "-".repeat(40).yellow());
        println!("  {} No active epic/tag is set", "âš ".yellow());
        println!(
            "    {}",
            "â†’ scud tags <epic-name>  # to set active epic".dimmed()
        );
        println!();
    }

    // Print other issues
    for issue in &results.issues {
        let (icon, color_fn): (&str, fn(&str) -> colored::ColoredString) = match issue.severity {
            Severity::Critical => ("âœ—", |s: &str| s.red()),
            Severity::Error => ("âœ—", |s: &str| s.red()),
            Severity::Warning => ("âš ", |s: &str| s.yellow()),
        };

        println!(
            "  {} [{}] {}",
            color_fn(icon),
            issue.severity.as_str(),
            issue.message
        );
        if let Some(ref task_id) = issue.task_id {
            println!(
                "    Task: {} in {}",
                task_id.cyan(),
                issue.epic_tag.dimmed()
            );
        }
        println!("    {}", format!("â†’ {}", issue.suggestion).dimmed());
    }

    // Summary
    println!();
    println!("{}", "Summary".blue().bold());
    println!("{}", "-".repeat(40).blue());
    println!(
        "  Critical: {}  Errors: {}  Warnings: {}",
        results.critical_count().to_string().red(),
        results.error_count().to_string().yellow(),
        results.warning_count().to_string().blue()
    );

    if !fix_attempted && (!results.stale_locks.is_empty() || !results.orphan_in_progress.is_empty())
    {
        println!();
        println!("{}", "To auto-fix recoverable issues, run:".blue());
        println!("  scud doctor --fix");
    }
}

fn print_recovery_instructions() {
    println!();
    println!("{}", "=".repeat(60).red());
    println!("{}", "RECOVERY INSTRUCTIONS".red().bold());
    println!("{}", "=".repeat(60).red());
    println!();
    println!("The task storage appears corrupted or missing. To recover:");
    println!();
    println!("1. Check if .scud/ directory exists:");
    println!("   {}", "ls -la .scud/".cyan());
    println!();
    println!("2. If missing, initialize SCUD:");
    println!("   {}", "scud init".cyan());
    println!();
    println!("3. If corrupted, check for backups:");
    println!("   {}", "ls -la .scud/tasks/*.bak".cyan());
    println!();
    println!("4. If no backups, you may need to recreate tasks:");
    println!(
        "   {}",
        "scud parse-prd <prd-file> --tag <epic-name>".cyan()
    );
    println!();
    println!("5. For manual recovery, task files are located at:");
    println!("   {}", ".scud/tasks/tasks.scg (or tasks.json)".dimmed());
    println!("   {}", ".scud/active-tag".dimmed());
    println!();
    println!(
        "{}",
        "If issues persist, consider consulting a high-context agent".yellow()
    );
    println!(
        "{}",
        "with full codebase access to inspect and repair the files.".yellow()
    );
}

#[cfg(test)]
mod tests {
    use super::*;
    use crate::models::phase::Phase;
    use crate::models::task::Task;

    #[test]
    fn test_diagnostic_results_has_issues() {
        let empty = DiagnosticResults::default();
        assert!(!empty.has_issues());

        let mut with_stale = DiagnosticResults::default();
        with_stale.stale_locks.push((
            "epic".to_string(),
            "task".to_string(),
            "user".to_string(),
            48.0,
        ));
        assert!(with_stale.has_issues());
    }

    #[test]
    fn test_diagnostic_results_counts() {
        let mut results = DiagnosticResults::default();

        // Add stale locks (warnings)
        results.stale_locks.push((
            "epic".to_string(),
            "task1".to_string(),
            "user".to_string(),
            48.0,
        ));
        results.stale_locks.push((
            "epic".to_string(),
            "task2".to_string(),
            "user".to_string(),
            36.0,
        ));

        // Add blocked by cancelled (errors)
        results.blocked_by_cancelled.push((
            "epic".to_string(),
            "task3".to_string(),
            "dep1".to_string(),
        ));

        // Add corrupt files (critical)
        results
            .corrupt_files
            .push("tasks.json: parse error".to_string());

        assert_eq!(results.warning_count(), 2);
        assert_eq!(results.error_count(), 1);
        assert_eq!(results.critical_count(), 1);
    }

    #[test]
    fn test_severity_as_str() {
        assert_eq!(Severity::Warning.as_str(), "WARNING");
        assert_eq!(Severity::Error.as_str(), "ERROR");
        assert_eq!(Severity::Critical.as_str(), "CRITICAL");
    }

    fn create_test_phase_with_issues() -> Phase {
        let mut phase = Phase::new("test-phase".to_string());

        // Task 1: Done
        let mut task1 = Task::new("1".to_string(), "Task 1".to_string(), "Desc".to_string());
        task1.set_status(TaskStatus::Done);
        phase.add_task(task1);

        // Task 2: Cancelled (will block task 3)
        let mut task2 = Task::new("2".to_string(), "Task 2".to_string(), "Desc".to_string());
        task2.set_status(TaskStatus::Cancelled);
        phase.add_task(task2);

        // Task 3: Pending, depends on cancelled task 2
        let mut task3 = Task::new("3".to_string(), "Task 3".to_string(), "Desc".to_string());
        task3.dependencies = vec!["2".to_string()];
        phase.add_task(task3);

        // Task 4: Pending, depends on non-existent task
        let mut task4 = Task::new("4".to_string(), "Task 4".to_string(), "Desc".to_string());
        task4.dependencies = vec!["nonexistent".to_string()];
        phase.add_task(task4);

        phase
    }

    #[test]
    fn test_detect_cancelled_dependency() {
        let phase = create_test_phase_with_issues();

        let task3 = phase.get_task("3").unwrap();
        let mut found_cancelled_dep = false;

        for dep_id in &task3.dependencies {
            if let Some(dep_task) = phase.get_task(dep_id) {
                if dep_task.status == TaskStatus::Cancelled {
                    found_cancelled_dep = true;
                }
            }
        }

        assert!(found_cancelled_dep);
    }

    #[test]
    fn test_detect_missing_dependency() {
        let phase = create_test_phase_with_issues();
        let all_task_ids: std::collections::HashSet<_> =
            phase.tasks.iter().map(|t| t.id.clone()).collect();
        // Use all_task_ids to check for missing dependencies
        let _task_count = all_task_ids.len();

        let task4 = phase.get_task("4").unwrap();
        let mut found_missing_dep = false;

        for dep_id in &task4.dependencies {
            if !all_task_ids.contains(dep_id) {
                found_missing_dep = true;
            }
        }

        assert!(found_missing_dep);
    }

    #[test]
    fn test_stale_lock_detection() {
        let mut task = Task::new("1".to_string(), "Test".to_string(), "Desc".to_string());
        task.claim("alice").unwrap();

        // Not stale immediately
        assert!(!task.is_stale_lock(24.0));

        // Simulate old lock
        let two_days_ago = chrono::Utc::now() - chrono::Duration::hours(48);
        task.locked_at = Some(two_days_ago.to_rfc3339());

        assert!(task.is_stale_lock(24.0));
        assert!(!task.is_stale_lock(72.0));
    }
}
</file>

<file path="scud-cli/src/commands/helpers.rs">
use anyhow::Result;
use colored::Colorize;
use dialoguer::Select;
use std::collections::HashMap;

use crate::models::phase::Phase;
use crate::models::task::Task;
use crate::storage::Storage;

/// Flatten all tasks from all phases into a single Vec for cross-tag dependency checking
pub fn flatten_all_tasks(all_phases: &HashMap<String, Phase>) -> Vec<&Task> {
    all_phases
        .values()
        .flat_map(|phase| phase.tasks.iter())
        .collect()
}

/// Check if we're running in an interactive terminal
pub fn is_interactive() -> bool {
    atty::is(atty::Stream::Stdin) && atty::is(atty::Stream::Stdout)
}

/// Resolve task group tag with fallback to active group and interactive selection
///
/// Priority:
/// 1. Explicit --tag argument
/// 2. Active group (from workflow-state.json)
/// 3. Interactive selection (if TTY available)
/// 4. Error with helpful message
pub fn resolve_group_tag(
    storage: &Storage,
    explicit_tag: Option<&str>,
    allow_interactive: bool,
) -> Result<String> {
    // Priority 1: Explicit --tag argument
    if let Some(tag) = explicit_tag {
        let tasks = storage.load_tasks()?;
        if !tasks.contains_key(tag) {
            anyhow::bail!("Task group '{}' not found. Run: scud tags", tag);
        }
        return Ok(tag.to_string());
    }

    // Priority 2: Active group
    if let Some(active) = storage.get_active_group()? {
        return Ok(active);
    }

    // Priority 3: Interactive selection
    if allow_interactive && is_interactive() {
        let tasks = storage.load_tasks()?;
        if tasks.is_empty() {
            anyhow::bail!(
                "No task groups found. Create one with: scud parse-prd <file> --tag <tag>"
            );
        }

        let mut tags: Vec<&String> = tasks.keys().collect();
        tags.sort();

        // Show selection prompt
        println!("{}", "No active task group set.".yellow());
        let selection = Select::new()
            .with_prompt("Select a task group")
            .items(&tags)
            .default(0)
            .interact()?;

        let selected = tags[selection].clone();

        // Set as active for next time
        storage.set_active_group(&selected)?;
        println!("{} {}", "Active group set to:".green(), selected.green());

        return Ok(selected);
    }

    // Priority 4: Error
    anyhow::bail!("No active task group. Use --tag <tag> or run: scud tags <tag>")
}
</file>

<file path="scud-cli/src/commands/hook_complete.rs">
use anyhow::Result;
use std::env;
use std::fs;
use std::path::PathBuf;

use crate::models::task::TaskStatus;
use crate::storage::Storage;

/// Called by Claude Code Stop hook to enforce task completion.
/// This command is internal and hidden from help.
pub fn run(project_root: Option<PathBuf>) -> Result<()> {
    // Try to get task ID from environment or file
    let task_id = get_current_task_id(project_root.as_ref());

    match task_id {
        Some(id) => complete_task(project_root, &id)?,
        None => {
            // No task context - this is fine, just exit silently
            // (The hook fires on every session end, not just task sessions)
        }
    }

    Ok(())
}

fn complete_task(project_root: Option<PathBuf>, task_id: &str) -> Result<()> {
    let storage = Storage::new(project_root);

    // Get active tag
    let tag = match storage.get_active_group()? {
        Some(t) => t,
        None => {
            eprintln!("Hook: No active tag, skipping completion");
            return Ok(());
        }
    };

    // Load and complete the task
    let mut phase = storage.load_group(&tag)?;

    if let Some(task) = phase.get_task_mut(task_id) {
        if task.status != TaskStatus::Done {
            task.status = TaskStatus::Done;
            task.release(); // Clear any lock
            task.update();
            eprintln!("âœ“ Hook: Task {} marked complete", task_id);
        }
    } else {
        eprintln!("Hook: Task {} not found in tag {}", task_id, tag);
    }

    // Save the updated phase
    storage.update_group(&tag, &phase)?;

    // Clean up the current-task file
    let current_task_file = get_current_task_file(storage.scud_dir());
    let _ = fs::remove_file(current_task_file);

    Ok(())
}

fn get_current_task_file(scud_dir: PathBuf) -> PathBuf {
    scud_dir.join("current-task")
}

fn get_current_task_id(project_root: Option<&PathBuf>) -> Option<String> {
    // Check environment variable first (set by orchestrator)
    if let Ok(id) = env::var("SCUD_TASK_ID") {
        if !id.is_empty() {
            return Some(id);
        }
    }

    // Check .scud/current-task file (set by claim or session start)
    let storage = Storage::new(project_root.cloned());
    let current_task_file = get_current_task_file(storage.scud_dir());

    if let Ok(id) = fs::read_to_string(current_task_file) {
        let id = id.trim().to_string();
        if !id.is_empty() {
            return Some(id);
        }
    }

    None
}
</file>

<file path="scud-cli/src/commands/hooks.rs">
use anyhow::Result;
use serde_json::{json, Value};
use std::fs;
use std::path::PathBuf;

pub fn run(project_root: Option<PathBuf>, action: &str) -> Result<()> {
    match action {
        "install" => install_hooks(project_root)?,
        "uninstall" => uninstall_hooks(project_root)?,
        "status" => show_status(project_root)?,
        _ => {
            println!("Usage: scud hooks <install|uninstall|status>");
            println!();
            println!("Commands:");
            println!("  install    Install Claude Code hooks for automatic task completion");
            println!("  uninstall  Remove Claude Code hooks");
            println!("  status     Show current hook installation status");
        }
    }
    Ok(())
}

fn get_settings_path(project_root: Option<PathBuf>) -> PathBuf {
    let root = project_root.unwrap_or_else(|| std::env::current_dir().unwrap());
    root.join(".claude/settings.local.json")
}

fn install_hooks(project_root: Option<PathBuf>) -> Result<()> {
    let settings_path = get_settings_path(project_root);

    // Ensure .claude directory exists
    if let Some(parent) = settings_path.parent() {
        fs::create_dir_all(parent)?;
    }

    // Load existing settings or create new
    let mut settings: Value = if settings_path.exists() {
        let content = fs::read_to_string(&settings_path)?;
        serde_json::from_str(&content).unwrap_or(json!({}))
    } else {
        json!({})
    };

    // Add hooks configuration
    let hooks = json!({
        "Stop": [{
            "matcher": "",
            "hooks": [{
                "type": "command",
                "command": "scud _hook-complete"
            }]
        }]
    });

    settings["hooks"] = hooks;

    // Write back
    let content = serde_json::to_string_pretty(&settings)?;
    fs::write(&settings_path, content)?;

    println!("âœ“ Claude Code hooks installed");
    println!();
    println!("Active hooks:");
    println!("  â€¢ Stop â†’ scud _hook-complete (enforces task completion)");
    println!();
    println!("Hooks are stored in: .claude/settings.local.json");
    Ok(())
}

fn uninstall_hooks(project_root: Option<PathBuf>) -> Result<()> {
    let settings_path = get_settings_path(project_root);

    if !settings_path.exists() {
        println!("No hooks installed (settings file not found)");
        return Ok(());
    }

    let content = fs::read_to_string(&settings_path)?;
    let mut settings: Value = serde_json::from_str(&content)?;

    if let Some(obj) = settings.as_object_mut() {
        obj.remove("hooks");
    }

    let content = serde_json::to_string_pretty(&settings)?;
    fs::write(&settings_path, content)?;

    println!("âœ“ Claude Code hooks uninstalled");
    Ok(())
}

fn show_status(project_root: Option<PathBuf>) -> Result<()> {
    let settings_path = get_settings_path(project_root);

    if !settings_path.exists() {
        println!("Hooks: NOT INSTALLED");
        println!();
        println!("Run: scud hooks install");
        return Ok(());
    }

    let content = fs::read_to_string(&settings_path)?;
    let settings: Value = serde_json::from_str(&content)?;

    if settings.get("hooks").is_some() {
        println!("Hooks: INSTALLED");
        println!();
        println!("Active hooks:");
        println!("  â€¢ Stop â†’ scud _hook-complete");
        println!();
        println!("Settings file: .claude/settings.local.json");
    } else {
        println!("Hooks: NOT INSTALLED");
        println!();
        println!("Run: scud hooks install");
    }

    Ok(())
}
</file>

<file path="scud-cli/src/commands/init.rs">
use anyhow::Result;
use colored::Colorize;
use dialoguer::{Input, Select};
use std::path::PathBuf;

use crate::commands::helpers::is_interactive;
use crate::config::{Config, LLMConfig};
use crate::storage::Storage;

pub fn run(project_root: Option<PathBuf>, provider_arg: Option<String>) -> Result<()> {
    let storage = Storage::new(project_root);

    if storage.is_initialized() {
        println!("{}", "âœ“ SCUD is already initialized".green());
        return Ok(());
    }

    println!("{}", "Initializing SCUD...".blue());
    println!();

    let (provider, model) = if let Some(provider_name) = provider_arg {
        // Non-interactive mode with command-line argument
        let provider = provider_name.to_lowercase();
        if !matches!(
            provider.as_str(),
            "xai" | "anthropic" | "openai" | "openrouter"
        ) {
            anyhow::bail!(
                "Invalid provider: {}. Valid options: xai, anthropic, openai, openrouter",
                provider
            );
        }
        let model = Config::default_model_for_provider(&provider).to_string();
        (provider, model)
    } else if is_interactive() {
        // Interactive mode - prompt for LLM provider
        let providers = vec![
            "xAI (Grok)",
            "Anthropic (Claude)",
            "OpenAI (GPT)",
            "OpenRouter",
        ];
        let provider_selection = Select::new()
            .with_prompt("Select your LLM provider")
            .items(&providers)
            .default(0)
            .interact()?;

        let provider = match provider_selection {
            0 => "xai",
            1 => "anthropic",
            2 => "openai",
            3 => "openrouter",
            _ => "anthropic",
        };

        // Build model options: suggested models + "Custom" option
        let suggested = Config::suggested_models_for_provider(provider);
        let mut model_options: Vec<String> = suggested.iter().map(|s| s.to_string()).collect();
        model_options.push("Custom (enter model name)".to_string());

        let model_selection = Select::new()
            .with_prompt("Select model (or choose Custom to enter any model)")
            .items(&model_options)
            .default(0)
            .interact()?;

        let model = if model_selection == model_options.len() - 1 {
            // User selected "Custom"
            Input::<String>::new()
                .with_prompt("Enter model name")
                .interact_text()?
        } else {
            suggested[model_selection].to_string()
        };

        (provider.to_string(), model)
    } else {
        // Non-interactive without provider arg: use default (anthropic)
        let provider = "anthropic";
        let model = Config::default_model_for_provider(provider);
        (provider.to_string(), model.to_string())
    };

    let config = Config {
        llm: LLMConfig {
            provider,
            model,
            max_tokens: 4096,
        },
    };

    storage.initialize_with_config(&config)?;

    println!("\n{}", "âœ… SCUD initialized successfully!".green().bold());
    println!("\n{}", "Configuration:".blue());
    println!("  Provider: {}", config.llm.provider.yellow());
    println!("  Model: {}", config.llm.model.yellow());
    println!("\n{}", "Environment variable required:".blue());
    println!(
        "  export {}=your-api-key",
        config.api_key_env_var().yellow()
    );
    println!("\n{}", "Next steps:".blue());
    println!("  1. Set your API key environment variable");
    println!("  2. Run: scud tags");
    println!("  3. Create or import tasks, then use: /scud:task-next\n");

    Ok(())
}
</file>

<file path="scud-cli/src/commands/list.rs">
use anyhow::Result;
use std::path::PathBuf;

use crate::commands::helpers::resolve_group_tag;
use crate::formats::serialize_scg;
use crate::models::{Phase, TaskStatus};
use crate::storage::Storage;

pub fn run(
    project_root: Option<PathBuf>,
    status_filter: Option<&str>,
    tag: Option<&str>,
    json_output: bool,
) -> Result<()> {
    let storage = Storage::new(project_root);

    // Resolve phase tag (explicit --tag, active phase, or interactive selection)
    let phase_tag = resolve_group_tag(&storage, tag, true)?;
    let tasks = storage.load_tasks()?;
    let phase = tasks
        .get(&phase_tag)
        .ok_or_else(|| anyhow::anyhow!("Phase '{}' not found", phase_tag))?;

    // Parse filter status once
    let filter_status = status_filter
        .map(|s| {
            TaskStatus::from_str(s).ok_or_else(|| {
                anyhow::anyhow!("Invalid status: {}. Valid: {:?}", s, TaskStatus::all())
            })
        })
        .transpose()?;

    // Create filtered phase for output
    let filtered_phase = if filter_status.is_some() {
        let filtered_tasks: Vec<_> = phase
            .tasks
            .iter()
            .filter(|t| {
                filter_status
                    .as_ref()
                    .map(|fs| t.status == *fs)
                    .unwrap_or(true)
            })
            .cloned()
            .collect();

        let mut filtered = Phase::new(phase.name.clone());
        filtered.tasks = filtered_tasks;
        filtered
    } else {
        phase.clone()
    };

    if filtered_phase.tasks.is_empty() {
        if json_output {
            println!("[]");
        } else {
            // Output empty SCG
            println!("# SCUD Graph v1");
            println!("# Phase: {}", phase_tag);
            println!();
            println!("@nodes");
            println!("# id | title | status | complexity | priority");
            println!("# (no tasks)");
        }
        return Ok(());
    }

    if json_output {
        // JSON output
        let json = serde_json::to_string_pretty(&filtered_phase.tasks)?;
        println!("{}", json);
    } else {
        // SCG output (default)
        let scg = serialize_scg(&filtered_phase);
        print!("{}", scg);
    }

    Ok(())
}
</file>

<file path="scud-cli/src/commands/mermaid.rs">
use anyhow::Result;
use std::path::PathBuf;

use crate::models::task::TaskStatus;
use crate::storage::Storage;

/// Generate a Mermaid diagram of the task graph
pub fn run(project_root: Option<PathBuf>, tag: Option<&str>, all_tags: bool) -> Result<()> {
    let storage = Storage::new(project_root);
    let all_tasks = storage.load_tasks()?;

    // Determine which phase(s) to include
    let phase_tags: Vec<String> = if all_tags {
        all_tasks.keys().cloned().collect()
    } else if let Some(t) = tag {
        if !all_tasks.contains_key(t) {
            anyhow::bail!("Phase '{}' not found. Run: scud tags", t);
        }
        vec![t.to_string()]
    } else {
        // Use active phase
        let active = storage.get_active_group()?;
        match active {
            Some(t) => vec![t],
            None => anyhow::bail!("No active task group. Use --tag <phase-tag> or run: scud tags"),
        }
    };

    // Start Mermaid graph
    println!("```mermaid");
    println!("flowchart TD");

    // Add subgraph for each tag
    for tag in &phase_tags {
        if let Some(phase) = all_tasks.get(tag) {
            if phase_tags.len() > 1 {
                println!("    subgraph {}[\"Phase: {}\"]", sanitize_id(tag), tag);
            }

            // Add task nodes
            for task in &phase.tasks {
                let node_id = sanitize_id(&task.id);
                let label = escape_label(&task.title);
                let style_class = status_to_class(&task.status);

                // Format: id["title"] or id(["title"]) for different shapes
                let shape = match task.status {
                    TaskStatus::Expanded => format!("{}[[\"{}\"]]", node_id, label),
                    TaskStatus::Done => format!("{}([\"{}\"])", node_id, label),
                    TaskStatus::Blocked => format!("{}{{\"{}\"}}", node_id, label),
                    _ => format!("{}[\"{}\"]", node_id, label),
                };

                println!("    {}", shape);

                // Add class for styling
                if !style_class.is_empty() {
                    println!("    class {} {}", node_id, style_class);
                }
            }

            // Add dependency edges
            for task in &phase.tasks {
                let task_node = sanitize_id(&task.id);
                for dep in &task.dependencies {
                    let dep_node = sanitize_id(dep);
                    // dep --> task (dependency flows to dependent)
                    println!("    {} --> {}", dep_node, task_node);
                }

                // Add parent-child relationships (subtasks)
                if let Some(ref parent_id) = task.parent_id {
                    let parent_node = sanitize_id(parent_id);
                    // Dotted line for parent-child
                    println!("    {} -.-> {}", parent_node, task_node);
                }
            }

            if phase_tags.len() > 1 {
                println!("    end");
            }
        }
    }

    // Add style definitions
    println!();
    println!("    %% Status styles");
    println!("    classDef pending fill:#f9f9f9,stroke:#999,color:#333");
    println!("    classDef inprogress fill:#e3f2fd,stroke:#1976d2,color:#1976d2,stroke-width:2px");
    println!("    classDef done fill:#e8f5e9,stroke:#4caf50,color:#2e7d32");
    println!("    classDef blocked fill:#ffebee,stroke:#f44336,color:#c62828");
    println!("    classDef review fill:#fff3e0,stroke:#ff9800,color:#e65100");
    println!("    classDef expanded fill:#f3e5f5,stroke:#9c27b0,color:#6a1b9a");
    println!("    classDef deferred fill:#eceff1,stroke:#607d8b,color:#455a64");
    println!(
        "    classDef cancelled fill:#fafafa,stroke:#bdbdbd,color:#9e9e9e,stroke-dasharray: 5 5"
    );

    println!("```");

    Ok(())
}

/// Sanitize task ID for use as Mermaid node ID
fn sanitize_id(id: &str) -> String {
    // Replace characters that are problematic in Mermaid IDs
    id.replace([':', '.', '-', ' '], "_")
}

/// Escape label text for Mermaid
fn escape_label(text: &str) -> String {
    text.replace('"', "'").replace('\n', " ")
}

/// Map task status to Mermaid class name
fn status_to_class(status: &TaskStatus) -> &'static str {
    match status {
        TaskStatus::Pending => "pending",
        TaskStatus::InProgress => "inprogress",
        TaskStatus::Done => "done",
        TaskStatus::Blocked => "blocked",
        TaskStatus::Review => "review",
        TaskStatus::Expanded => "expanded",
        TaskStatus::Deferred => "deferred",
        TaskStatus::Cancelled => "cancelled",
    }
}
</file>

<file path="scud-cli/src/commands/migrate.rs">
use anyhow::Result;
use colored::Colorize;
use std::collections::HashMap;
use std::path::PathBuf;

use crate::models::task::{Task, TaskStatus};
use crate::storage::Storage;

/// Migrate task IDs to namespaced format and fix legacy patterns
pub fn run(project_root: Option<PathBuf>, dry_run: bool) -> Result<()> {
    let storage = Storage::new(project_root);

    // Check if tasks file exists
    let tasks_file = storage.tasks_file();
    if !tasks_file.exists() {
        println!("{}", "No tasks file found. Nothing to migrate.".yellow());
        return Ok(());
    }

    let mut all_tasks = storage.load_tasks()?;
    let mut changes: Vec<String> = Vec::new();
    let mut parent_fixes = 0;
    let mut subtask_links = 0;

    for (epic_tag, epic) in all_tasks.iter_mut() {
        let mut id_map: HashMap<String, String> = HashMap::new();

        // Phase 1: Collect ID mappings for tasks that need namespacing
        for task in &epic.tasks {
            if !task.id.contains(':') {
                let new_id = Task::make_id(epic_tag, &task.id);
                id_map.insert(task.id.clone(), new_id.clone());
                changes.push(format!("{}: {} -> {}", epic_tag, task.id, new_id));
            }
        }

        // Phase 2: Update IDs and references
        for task in &mut epic.tasks {
            // Update task ID if it's not namespaced
            if let Some(new_id) = id_map.get(&task.id) {
                task.id = new_id.clone();
            }

            // Update dependencies to use namespaced IDs
            task.dependencies = task
                .dependencies
                .iter()
                .map(|dep| {
                    id_map.get(dep).cloned().unwrap_or_else(|| {
                        if dep.contains(':') {
                            dep.clone()
                        } else {
                            Task::make_id(epic_tag, dep)
                        }
                    })
                })
                .collect();

            // Update parent_id if present
            if let Some(ref parent) = task.parent_id {
                task.parent_id = Some(
                    id_map
                        .get(parent)
                        .cloned()
                        .unwrap_or_else(|| Task::make_id(epic_tag, parent)),
                );
            }

            // Update subtask references
            task.subtasks = task
                .subtasks
                .iter()
                .map(|sub| {
                    id_map
                        .get(sub)
                        .cloned()
                        .unwrap_or_else(|| Task::make_id(epic_tag, sub))
                })
                .collect();

            // Fix [PARENT] prefix -> Expanded status
            if task.title.starts_with("[PARENT]") {
                task.title = task.title.trim_start_matches("[PARENT]").trim().to_string();
                task.status = TaskStatus::Expanded;
                parent_fixes += 1;
            }
        }

        // Phase 3: Infer parent-child relationships from ID patterns (e.g., 10.1 is subtask of 10)
        // First pass: collect the relationships
        let task_ids: Vec<String> = epic.tasks.iter().map(|t| t.id.clone()).collect();
        let mut parent_child_links: Vec<(String, String)> = Vec::new(); // (child_id, parent_id)

        for task in &epic.tasks {
            // If this task looks like a subtask (contains dot in local_id) and has no parent_id
            let local_id = task.local_id().to_string();
            if local_id.contains('.') && task.parent_id.is_none() {
                // Extract parent local ID (e.g., "10.1" -> "10")
                if let Some(parent_local) = local_id.rsplit_once('.').map(|(p, _)| p.to_string()) {
                    let parent_id = Task::make_id(epic_tag, &parent_local);
                    if task_ids.contains(&parent_id) {
                        parent_child_links.push((task.id.clone(), parent_id));
                    }
                }
            }
        }

        // Second pass: apply the relationships
        for (child_id, parent_id) in parent_child_links {
            // Set parent_id on child
            if let Some(child) = epic.tasks.iter_mut().find(|t| t.id == child_id) {
                child.parent_id = Some(parent_id.clone());
                subtask_links += 1;
            }
            // Add child to parent's subtasks
            if let Some(parent) = epic.tasks.iter_mut().find(|t| t.id == parent_id) {
                if !parent.subtasks.contains(&child_id) {
                    parent.subtasks.push(child_id);
                }
            }
        }
    }

    // Phase 4: Ensure parents with subtasks have Expanded status
    for (_, epic) in all_tasks.iter_mut() {
        let subtask_ids: Vec<String> = epic
            .tasks
            .iter()
            .filter(|t| t.parent_id.is_some())
            .filter_map(|t| t.parent_id.clone())
            .collect();

        for task in &mut epic.tasks {
            if subtask_ids.contains(&task.id)
                && task.status != TaskStatus::Expanded
                && (task.status == TaskStatus::Pending || task.status == TaskStatus::InProgress)
            {
                task.status = TaskStatus::Expanded;
                parent_fixes += 1;
            }
        }
    }

    if dry_run {
        println!("{}", "Dry run - no changes made".yellow());
        println!();

        if changes.is_empty() && parent_fixes == 0 && subtask_links == 0 {
            println!("{}", "No migrations needed. Data is up to date!".green());
            return Ok(());
        }

        if !changes.is_empty() {
            println!("{}", "ID changes:".blue().bold());
            for change in &changes {
                println!("  {}", change);
            }
            println!();
        }

        println!("{}", "Summary:".blue().bold());
        println!("  {} ID namespacing changes", changes.len());
        println!("  {} [PARENT] prefix fixes", parent_fixes);
        println!("  {} subtask relationships inferred", subtask_links);
    } else {
        if changes.is_empty() && parent_fixes == 0 && subtask_links == 0 {
            println!("{}", "No migrations needed. Data is up to date!".green());
            return Ok(());
        }

        storage.save_tasks(&all_tasks)?;

        println!("{}", "Migration complete!".green().bold());
        println!();
        println!("  {} task IDs namespaced", changes.len());
        println!(
            "  {} [PARENT] prefixes converted to Expanded status",
            parent_fixes
        );
        println!("  {} subtask relationships established", subtask_links);
        println!();
        println!(
            "{}",
            "Tip: Run 'scud list' to verify the migration.".dimmed()
        );
    }

    Ok(())
}
</file>

<file path="scud-cli/src/commands/mod.rs">
pub mod ai;
pub mod commit;
pub mod config;
pub mod convert;
pub mod doctor;
pub mod helpers;
pub mod init;
pub mod list;
pub mod mermaid;
pub mod migrate;
pub mod next;
pub mod next_batch;
pub mod sessions;
pub mod set_status;
pub mod show;
pub mod stats;
pub mod tags;
pub mod warmup;
pub mod waves;

// Task assignment commands
pub mod assign;
pub mod claim;
pub mod release;
pub mod whois;

// Claude Code hooks
pub mod hook_complete;
pub mod hooks;
</file>

<file path="scud-cli/src/commands/next_batch.rs">
use anyhow::Result;
use std::path::PathBuf;

use crate::commands::helpers::{flatten_all_tasks, resolve_group_tag};
use crate::models::task::TaskStatus;
use crate::storage::Storage;

pub fn run(project_root: Option<PathBuf>, tag: Option<&str>, limit: usize) -> Result<()> {
    let storage = Storage::new(project_root);
    let phase_tag = resolve_group_tag(&storage, tag, true)?;

    // Load all phases for cross-tag dependency checking
    let all_phases = storage.load_tasks()?;
    let all_tasks_flat = flatten_all_tasks(&all_phases);

    let phase = all_phases
        .get(&phase_tag)
        .ok_or_else(|| anyhow::anyhow!("Phase '{}' not found", phase_tag))?;

    // Filter tasks with cross-tag aware dependency checking
    let ready_tasks: Vec<_> = phase
        .tasks
        .iter()
        .filter(|t| t.status == TaskStatus::Pending)
        .filter(|t| t.has_dependencies_met_refs(&all_tasks_flat))
        .filter(|t| !t.is_locked())
        .take(limit)
        .collect();

    let output = serde_json::json!({
        "tag": phase_tag,
        "count": ready_tasks.len(),
        "tasks": ready_tasks.iter().map(|t| {
            serde_json::json!({
                "id": t.id,
                "title": t.title,
                "complexity": t.complexity,
                "priority": format!("{:?}", t.priority)
            })
        }).collect::<Vec<_>>()
    });

    println!("{}", serde_json::to_string_pretty(&output)?);
    Ok(())
}
</file>

<file path="scud-cli/src/commands/next.rs">
use anyhow::Result;
use colored::Colorize;
use std::path::PathBuf;

use crate::commands::helpers::{flatten_all_tasks, resolve_group_tag};
use crate::models::task::{Task, TaskStatus};
use crate::storage::Storage;

/// Result of finding the next task
pub enum NextTaskResult<'a> {
    /// Found a task with dependencies met
    Available(&'a crate::models::task::Task),
    /// No pending tasks at all
    NoPendingTasks,
    /// Pending tasks exist but blocked by dependencies
    BlockedByDependencies,
    /// All pending tasks are locked by others
    AllLocked,
}

/// Find the next available task, considering locks
/// all_tasks should contain tasks from all phases for cross-tag dependency resolution
pub fn find_next_available<'a>(
    phase: &'a crate::models::phase::Phase,
    all_tasks: &[&Task],
    exclude_locked: bool,
) -> NextTaskResult<'a> {
    let pending_tasks: Vec<_> = phase
        .tasks
        .iter()
        .filter(|t| t.status == TaskStatus::Pending)
        .collect();

    if pending_tasks.is_empty() {
        return NextTaskResult::NoPendingTasks;
    }

    // Find tasks with dependencies met (checking across all phases)
    let deps_met: Vec<_> = pending_tasks
        .iter()
        .filter(|t| t.has_dependencies_met_refs(all_tasks))
        .collect();

    if deps_met.is_empty() {
        return NextTaskResult::BlockedByDependencies;
    }

    // Filter out locked tasks if requested
    if exclude_locked {
        let unlocked: Vec<_> = deps_met.iter().filter(|t| !t.is_locked()).collect();
        if unlocked.is_empty() {
            return NextTaskResult::AllLocked;
        }
        return NextTaskResult::Available(unlocked[0]);
    }

    NextTaskResult::Available(deps_met[0])
}

pub fn run(
    project_root: Option<PathBuf>,
    tag: Option<&str>,
    claim: bool,
    name: Option<&str>,
    release: bool,
    spawn: bool,
) -> Result<()> {
    let storage = Storage::new(project_root);
    let phase_tag = resolve_group_tag(&storage, tag, true)?;

    // Handle --release mode
    if release {
        let agent_name =
            name.ok_or_else(|| anyhow::anyhow!("--name is required with --release"))?;
        return handle_release(&storage, &phase_tag, agent_name);
    }

    // Handle --claim mode (experimental dynamic-wave)
    if claim {
        let agent_name = name.ok_or_else(|| anyhow::anyhow!("--name is required with --claim"))?;
        return handle_claim(&storage, &phase_tag, agent_name);
    }

    // Standard next task behavior (read-only)
    let tasks = storage.load_tasks()?;
    let all_tasks_flat = flatten_all_tasks(&tasks);
    let phase = tasks
        .get(&phase_tag)
        .ok_or_else(|| anyhow::anyhow!("Phase '{}' not found", phase_tag))?;

    // Handle --spawn mode (machine-readable JSON output)
    if spawn {
        match find_next_available(phase, &all_tasks_flat, true) {
            NextTaskResult::Available(task) => {
                let output = serde_json::json!({
                    "task_id": task.id,
                    "title": task.title,
                    "tag": phase_tag,
                    "complexity": task.complexity,
                });
                println!("{}", serde_json::to_string(&output)?);
            }
            _ => {
                println!("null");
            }
        }
        return Ok(());
    }

    match find_next_available(phase, &all_tasks_flat, false) {
        NextTaskResult::Available(task) => {
            print_task_details(task);
            print_standard_instructions(&task.id);
        }
        NextTaskResult::NoPendingTasks => {
            println!("{}", "All tasks completed or in progress!".green().bold());
            println!("Run: scud list --status in-progress");
        }
        NextTaskResult::BlockedByDependencies => {
            println!(
                "{}",
                "No available tasks - all pending tasks blocked by dependencies".yellow()
            );
            println!("Run: scud list --status pending");
            println!("Run: scud doctor  # to diagnose stuck states");
        }
        NextTaskResult::AllLocked => {
            println!("{}", "All available tasks are currently locked".yellow());
            println!("Run: scud whois  # to see who's working on what");
        }
    }

    Ok(())
}

fn handle_claim(storage: &Storage, phase_tag: &str, agent_name: &str) -> Result<()> {
    println!(
        "{}",
        "[EXPERIMENTAL] Dynamic-wave mode: claiming next task"
            .yellow()
            .bold()
    );
    println!();

    // Load all phases for cross-tag dependency checking
    let all_phases = storage.load_tasks()?;
    let all_tasks_flat = flatten_all_tasks(&all_phases);

    // Use atomic update_group to hold lock across read-modify-write cycle
    // This prevents race conditions when multiple agents claim simultaneously
    let mut phase = storage.load_group(phase_tag)?;

    // Find next available task (exclude locked ones)
    let task_id = {
        let pending_tasks: Vec<_> = phase
            .tasks
            .iter()
            .filter(|t| t.status == TaskStatus::Pending)
            .collect();

        if pending_tasks.is_empty() {
            println!("{}", "No pending tasks available".yellow());
            println!();
            println!("{}", "All tasks may be:".blue());
            println!("  - Already done");
            println!("  - In progress by others");
            println!("  - Blocked by dependencies");
            println!();
            println!("Run: scud list  # to see all tasks");
            println!("Run: scud stats  # to see completion status");
            return Ok(());
        }

        // Find first task with dependencies met that isn't locked (cross-tag aware)
        let available: Vec<_> = pending_tasks
            .iter()
            .filter(|t| t.has_dependencies_met_refs(&all_tasks_flat) && !t.is_locked())
            .collect();

        if available.is_empty() {
            // Check if blocked by deps or by locks
            let deps_met: Vec<_> = pending_tasks
                .iter()
                .filter(|t| t.has_dependencies_met_refs(&all_tasks_flat))
                .collect();

            if deps_met.is_empty() {
                println!(
                    "{}",
                    "No tasks available - all pending tasks blocked by dependencies"
                        .yellow()
                        .bold()
                );
                println!();
                println!("{}", "Possible causes:".blue());
                println!("  - Dependencies not marked as done");
                println!("  - Circular dependency issues");
                println!("  - Dependencies on cancelled/blocked tasks");
                println!();
                println!("Run: scud doctor  # to diagnose stuck states");
            } else {
                println!(
                    "{}",
                    "No tasks available - all eligible tasks are locked by other agents"
                        .yellow()
                        .bold()
                );
                println!();
                println!("{}", "Currently locked tasks:".blue());
                for task in deps_met {
                    if let Some(ref locked_by) = task.locked_by {
                        println!(
                            "  {} - {} (locked by {})",
                            task.id.cyan(),
                            task.title,
                            locked_by.green()
                        );
                    }
                }
                println!();
                println!("Run: scud whois  # to see all assignments");
                println!("Run: scud doctor  # to check for stale locks");
            }
            return Ok(());
        }

        available[0].id.clone()
    };

    // Claim the task
    let task = phase
        .get_task_mut(&task_id)
        .ok_or_else(|| anyhow::anyhow!("Task {} not found", task_id))?;

    task.claim(agent_name).map_err(|e| anyhow::anyhow!(e))?;
    task.set_status(TaskStatus::InProgress);

    // Get task details before saving
    let task_title = task.title.clone();
    let task_description = task.description.clone();
    let task_complexity = task.complexity;
    let task_details = task.details.clone();
    let task_test_strategy = task.test_strategy.clone();

    // Use atomic update_group which holds lock across read-modify-write
    storage.update_group(phase_tag, &phase)?;

    // Print claimed task details
    println!("{}", "Task claimed successfully!".green().bold());
    println!();
    println!("{:<20} {}", "ID:".yellow(), task_id.cyan());
    println!("{:<20} {}", "Title:".yellow(), task_title.bold());
    println!("{:<20} {}", "Complexity:".yellow(), task_complexity);
    println!("{:<20} {}", "Claimed by:".yellow(), agent_name.green());
    println!("{:<20} {}", "Status:".yellow(), "in-progress".cyan());
    println!();
    println!("{}", "Description:".yellow());
    println!("{}", task_description);

    if let Some(details) = &task_details {
        println!();
        println!("{}", "Technical Details:".yellow());
        println!("{}", details);
    }

    if let Some(test_strategy) = &task_test_strategy {
        println!();
        println!("{}", "Test Strategy:".yellow());
        println!("{}", test_strategy);
    }

    // Critical: Status discipline messaging
    println!();
    println!("{}", "=".repeat(60).yellow());
    println!("{}", "IMPORTANT: Status Update Required".red().bold());
    println!("{}", "=".repeat(60).yellow());
    println!();
    println!(
        "{}",
        "When you complete this task, you MUST run:".yellow().bold()
    );
    println!();
    println!(
        "    {}",
        format!("scud set-status {} done", task_id).cyan().bold()
    );
    println!();
    println!(
        "{}",
        "This ensures the workflow stays healthy and other agents".dimmed()
    );
    println!("{}", "can claim dependent tasks.".dimmed());
    println!();

    Ok(())
}

fn handle_release(storage: &Storage, phase_tag: &str, agent_name: &str) -> Result<()> {
    println!(
        "{}",
        "[EXPERIMENTAL] Releasing tasks for agent".yellow().bold()
    );
    println!();

    // Use atomic update_group to hold lock across read-modify-write cycle
    let mut phase = storage.load_group(phase_tag)?;

    // Find tasks locked by this agent
    let mut released_count = 0;
    for task in &mut phase.tasks {
        if task.is_locked_by(agent_name) {
            let task_id = task.id.clone();
            let task_title = task.title.clone();
            // Clear both lock and assignment for clean release
            task.release();
            task.assigned_to = None;
            // Reset status back to pending if it was in-progress
            if task.status == TaskStatus::InProgress {
                task.set_status(TaskStatus::Pending);
            }
            println!(
                "{} Released: {} - {}",
                "âœ“".green(),
                task_id.cyan(),
                task_title
            );
            released_count += 1;
        }
    }

    if released_count == 0 {
        println!(
            "{}",
            format!("No tasks found locked by '{}'", agent_name).yellow()
        );
        return Ok(());
    }

    // Use atomic update_group which holds lock across read-modify-write
    storage.update_group(phase_tag, &phase)?;

    println!();
    println!("{} {} task(s) released", "âœ“".green(), released_count);

    Ok(())
}

fn print_task_details(task: &crate::models::task::Task) {
    println!("{}", "Next Available Task:".green().bold());
    println!();
    println!("{:<20} {}", "ID:".yellow(), task.id.cyan());
    println!("{:<20} {}", "Title:".yellow(), task.title.bold());
    println!("{:<20} {}", "Complexity:".yellow(), task.complexity);
    println!("{:<20} {:?}", "Priority:".yellow(), task.priority);

    if let Some(ref assigned) = task.assigned_to {
        println!("{:<20} {}", "Assigned to:".yellow(), assigned.green());
    }

    if task.is_locked() {
        if let Some(ref locked_by) = task.locked_by {
            println!(
                "{:<20} {} (by {})",
                "Status:".yellow(),
                "LOCKED".red(),
                locked_by
            );
        }
    }

    println!();
    println!("{}", "Description:".yellow());
    println!("{}", task.description);

    if let Some(details) = &task.details {
        println!();
        println!("{}", "Technical Details:".yellow());
        println!("{}", details);
    }

    if let Some(test_strategy) = &task.test_strategy {
        println!();
        println!("{}", "Test Strategy:".yellow());
        println!("{}", test_strategy);
    }
}

fn print_standard_instructions(task_id: &str) {
    println!();
    println!("{}", "To start this task:".blue());
    println!("  scud set-status {} in-progress", task_id);
    println!();
    println!(
        "{}",
        "Or use experimental dynamic-wave mode:".blue().dimmed()
    );
    println!(
        "  scud next --claim --name <your-name>  {}",
        "# auto-claims next task".dimmed()
    );
}

#[cfg(test)]
mod tests {
    use super::*;
    use crate::models::phase::Phase;
    use crate::models::task::{Task, TaskStatus};

    fn create_test_phase() -> Phase {
        let mut phase = Phase::new("test-phase".to_string());

        let mut task1 = Task::new("1".to_string(), "Task 1".to_string(), "Desc 1".to_string());
        task1.set_status(TaskStatus::Done);

        let mut task2 = Task::new("2".to_string(), "Task 2".to_string(), "Desc 2".to_string());
        task2.dependencies = vec!["1".to_string()];
        // task2 is pending with deps met

        let mut task3 = Task::new("3".to_string(), "Task 3".to_string(), "Desc 3".to_string());
        task3.dependencies = vec!["2".to_string()];
        // task3 is pending with deps NOT met

        phase.add_task(task1);
        phase.add_task(task2);
        phase.add_task(task3);

        phase
    }

    /// Helper to get task refs from phase for testing
    fn get_task_refs(phase: &Phase) -> Vec<&Task> {
        phase.tasks.iter().collect()
    }

    #[test]
    fn test_find_next_available_basic() {
        let phase = create_test_phase();
        let all_tasks = get_task_refs(&phase);

        match find_next_available(&phase, &all_tasks, false) {
            NextTaskResult::Available(task) => {
                assert_eq!(task.id, "2");
            }
            _ => panic!("Expected Available result"),
        }
    }

    #[test]
    fn test_find_next_available_exclude_locked() {
        let mut phase = create_test_phase();

        // Lock task 2
        phase.get_task_mut("2").unwrap().claim("alice").unwrap();

        let all_tasks = get_task_refs(&phase);

        // Without exclude_locked, should still find task 2
        match find_next_available(&phase, &all_tasks, false) {
            NextTaskResult::Available(task) => {
                assert_eq!(task.id, "2");
            }
            _ => panic!("Expected Available result"),
        }

        // With exclude_locked, should return AllLocked
        match find_next_available(&phase, &all_tasks, true) {
            NextTaskResult::AllLocked => {}
            _ => panic!("Expected AllLocked result"),
        }
    }

    #[test]
    fn test_find_next_no_pending() {
        let mut phase = Phase::new("test".to_string());
        let mut task = Task::new("1".to_string(), "Done".to_string(), "Desc".to_string());
        task.set_status(TaskStatus::Done);
        phase.add_task(task);

        let all_tasks = get_task_refs(&phase);

        match find_next_available(&phase, &all_tasks, false) {
            NextTaskResult::NoPendingTasks => {}
            _ => panic!("Expected NoPendingTasks result"),
        }
    }

    #[test]
    fn test_find_next_blocked_by_deps() {
        let mut phase = Phase::new("test".to_string());

        let task1 = Task::new("1".to_string(), "Task 1".to_string(), "Desc".to_string());
        // task1 is pending

        let mut task2 = Task::new("2".to_string(), "Task 2".to_string(), "Desc".to_string());
        task2.dependencies = vec!["1".to_string()];
        // task2 depends on pending task1

        // Add task2 first, task1 second (so task2 is checked first)
        phase.add_task(task2);
        phase.add_task(task1);

        let all_tasks = get_task_refs(&phase);

        // task1 should be found since it has no deps
        match find_next_available(&phase, &all_tasks, false) {
            NextTaskResult::Available(task) => {
                assert_eq!(task.id, "1");
            }
            _ => panic!("Expected task 1 to be available"),
        }
    }

    #[test]
    fn test_find_next_all_blocked() {
        let mut phase = Phase::new("test".to_string());

        let mut task1 = Task::new("1".to_string(), "Task 1".to_string(), "Desc".to_string());
        task1.dependencies = vec!["nonexistent".to_string()];
        // task1 depends on non-existent task

        phase.add_task(task1);

        let all_tasks = get_task_refs(&phase);

        match find_next_available(&phase, &all_tasks, false) {
            NextTaskResult::BlockedByDependencies => {}
            _ => panic!("Expected BlockedByDependencies result"),
        }
    }

    #[test]
    fn test_find_next_cross_tag_dependency() {
        // Create a phase with a task that depends on a task from another "phase"
        let mut phase = Phase::new("api".to_string());
        let mut api_task = Task::new(
            "api:1".to_string(),
            "API Task".to_string(),
            "Desc".to_string(),
        );
        api_task.dependencies = vec!["auth:1".to_string()]; // Depends on auth phase
        phase.add_task(api_task);

        // Create "auth" task (simulating another phase)
        let mut auth_task = Task::new(
            "auth:1".to_string(),
            "Auth Task".to_string(),
            "Desc".to_string(),
        );
        auth_task.set_status(TaskStatus::Done);

        // Combine all tasks (simulating flattened all_phases)
        let all_tasks: Vec<&Task> = vec![&phase.tasks[0], &auth_task];

        // With cross-tag tasks included, dependency should be met
        match find_next_available(&phase, &all_tasks, false) {
            NextTaskResult::Available(task) => {
                assert_eq!(task.id, "api:1");
            }
            _ => panic!("Expected Available result with cross-tag dependency met"),
        }
    }

    #[test]
    fn test_find_next_cross_tag_dependency_not_met() {
        // Create a phase with a task that depends on a task from another "phase"
        let mut phase = Phase::new("api".to_string());
        let mut api_task = Task::new(
            "api:1".to_string(),
            "API Task".to_string(),
            "Desc".to_string(),
        );
        api_task.dependencies = vec!["auth:1".to_string()]; // Depends on auth phase
        phase.add_task(api_task);

        // Create "auth" task (NOT done)
        let auth_task = Task::new(
            "auth:1".to_string(),
            "Auth Task".to_string(),
            "Desc".to_string(),
        );

        // Combine all tasks (simulating flattened all_phases)
        let all_tasks: Vec<&Task> = vec![&phase.tasks[0], &auth_task];

        // With cross-tag dep NOT met, should be blocked
        match find_next_available(&phase, &all_tasks, false) {
            NextTaskResult::BlockedByDependencies => {}
            _ => panic!("Expected BlockedByDependencies with cross-tag dep not met"),
        }
    }
}
</file>

<file path="scud-cli/src/commands/release.rs">
use anyhow::Result;
use colored::Colorize;
use std::path::PathBuf;

use crate::commands::helpers::resolve_group_tag;
use crate::storage::Storage;

pub fn run(
    project_root: Option<PathBuf>,
    task_id: &str,
    force: bool,
    tag: Option<&str>,
) -> Result<()> {
    let storage = Storage::new(project_root);
    let epic_tag = resolve_group_tag(&storage, tag, true)?;

    // Use atomic update_group to hold lock across read-modify-write cycle
    // This prevents race conditions during release
    let mut epic = storage.load_group(&epic_tag)?;

    let task = epic
        .get_task_mut(task_id)
        .ok_or_else(|| anyhow::anyhow!("Task {} not found in epic '{}'", task_id, epic_tag))?;

    if !task.is_locked() {
        println!("{}", "âŠ˜ Task is not locked".yellow());
        return Ok(());
    }

    if !force {
        if let Some(ref locked_by) = task.locked_by {
            println!("{}", "âš  Task is locked".yellow());
            println!("{:<20} {}", "Locked by:".yellow(), locked_by.green());
            if let Some(age) = task.lock_age_hours() {
                println!("{:<20} {:.1}h ago", "Locked:".yellow(), age);
            }
            println!();
            println!("To force release: scud release {} --force", task_id);
            return Ok(());
        }
    }

    let was_locked_by = task.locked_by.clone();
    task.release();
    // Also clear assignment to avoid stale whois entries
    task.assigned_to = None;

    // Atomic write that holds the lock across read-modify-write
    storage.update_group(&epic_tag, &epic)?;

    println!("{} Task {} released", "âœ“".green(), task_id.cyan());
    if let Some(locked_by) = was_locked_by {
        println!("  Previously locked by: {}", locked_by);
    }

    Ok(())
}
</file>

<file path="scud-cli/src/commands/sessions.rs">
use anyhow::Result;
use colored::Colorize;
use std::path::PathBuf;

use crate::storage::Storage;

pub fn run(project_root: Option<PathBuf>, tag: Option<&str>) -> Result<()> {
    let storage = Storage::new(project_root);

    let tags: Vec<String> = if let Some(t) = tag {
        vec![t.to_string()]
    } else {
        // Get all tags by listing all groups from storage
        storage.load_tasks()?.keys().cloned().collect()
    };

    println!("{}", "Active Sessions".bold());
    println!();

    let mut found_any = false;

    for tag in tags {
        if let Ok(phase) = storage.load_group(&tag) {
            let locked_tasks: Vec<_> = phase.tasks.iter().filter(|t| t.is_locked()).collect();

            if !locked_tasks.is_empty() {
                found_any = true;
                println!("  {} {}", "Tag:".dimmed(), tag.cyan());

                for task in locked_tasks {
                    let age = task.lock_age_hours().unwrap_or(0.0);
                    let stale_marker = if age > 1.0 {
                        " (STALE)".red().to_string()
                    } else {
                        String::new()
                    };
                    let locked_by = task.locked_by.as_deref().unwrap_or("unknown");

                    println!(
                        "    {} | {} | {} | {:.1}h{}",
                        task.id.yellow(),
                        truncate(&task.title, 30),
                        locked_by.green(),
                        age,
                        stale_marker
                    );
                }
                println!();
            }
        }
    }

    if !found_any {
        println!("  {}", "No active sessions".dimmed());
    }

    Ok(())
}

fn truncate(s: &str, max_len: usize) -> String {
    if s.len() <= max_len {
        s.to_string()
    } else {
        format!("{}...", &s[..max_len - 3])
    }
}
</file>

<file path="scud-cli/src/commands/set_status.rs">
use anyhow::Result;
use colored::Colorize;
use std::path::PathBuf;

use crate::commands::helpers::resolve_group_tag;
use crate::models::TaskStatus;
use crate::storage::Storage;

pub fn run(
    project_root: Option<PathBuf>,
    task_id: &str,
    status_str: &str,
    tag: Option<&str>,
) -> Result<()> {
    let new_status = TaskStatus::from_str(status_str).ok_or_else(|| {
        anyhow::anyhow!(
            "Invalid status: {}. Valid: {:?}",
            status_str,
            TaskStatus::all()
        )
    })?;

    let storage = Storage::new(project_root);

    let epic_tag = resolve_group_tag(&storage, tag, true)?;
    let mut epic = storage.load_group(&epic_tag)?;

    let task = epic
        .get_task_mut(task_id)
        .ok_or_else(|| anyhow::anyhow!("Task {} not found in epic '{}'", task_id, epic_tag))?;

    // Auto-release lock when marking task as done (fulfills the promise in claim messaging)
    let was_locked = task.is_locked();
    let is_done = new_status == TaskStatus::Done;
    if is_done && was_locked {
        task.release();
        task.assigned_to = None;
    }

    task.set_status(new_status);

    storage.update_group(&epic_tag, &epic)?;

    // Show lock release message if applicable
    if is_done && was_locked {
        println!(
            "{} Task {} â†’ {} (lock released)",
            "âœ“".green(),
            task_id.cyan(),
            status_str.green()
        );
        return Ok(());
    }

    println!(
        "{} Task {} â†’ {}",
        "âœ“".green(),
        task_id.cyan(),
        status_str.green()
    );

    Ok(())
}
</file>

<file path="scud-cli/src/commands/show.rs">
use anyhow::Result;
use colored::Colorize;
use std::path::PathBuf;

use crate::commands::helpers::resolve_group_tag;
use crate::storage::Storage;

pub fn run(project_root: Option<PathBuf>, task_id: &str, tag: Option<&str>) -> Result<()> {
    let storage = Storage::new(project_root);

    let phase_tag = resolve_group_tag(&storage, tag, true)?;
    let tasks = storage.load_tasks()?;
    let phase = tasks
        .get(&phase_tag)
        .ok_or_else(|| anyhow::anyhow!("Phase '{}' not found", phase_tag))?;

    let task = phase
        .get_task(task_id)
        .ok_or_else(|| anyhow::anyhow!("Task {} not found in phase '{}'", task_id, phase_tag))?;

    println!("\n{}", "Task Details".blue().bold());
    println!("{}", "=============".blue());
    println!("{:<20} {}", "ID:".yellow(), task.id.cyan());
    println!("{:<20} {}", "Title:".yellow(), task.title.bold());
    println!("{:<20} {}", "Status:".yellow(), task.status.as_str());
    println!("{:<20} {}", "Complexity:".yellow(), task.complexity);
    println!("{:<20} {:?}", "Priority:".yellow(), task.priority);

    if !task.dependencies.is_empty() {
        println!("{:<20} {:?}", "Dependencies:".yellow(), task.dependencies);
    }

    println!("\n{}", "Description:".yellow());
    println!("{}", task.description);

    if let Some(details) = &task.details {
        println!("\n{}", "Technical Details:".yellow());
        println!("{}", details);
    }

    if let Some(test_strategy) = &task.test_strategy {
        println!("\n{}", "Test Strategy:".yellow());
        println!("{}", test_strategy);
    }

    if let Some(created) = &task.created_at {
        println!("\n{:<20} {}", "Created:".yellow(), created);
    }

    if let Some(updated) = &task.updated_at {
        println!("{:<20} {}", "Updated:".yellow(), updated);
    }

    println!();
    Ok(())
}
</file>

<file path="scud-cli/src/commands/stats.rs">
use anyhow::Result;
use colored::Colorize;
use std::path::PathBuf;

use crate::commands::helpers::resolve_group_tag;
use crate::storage::Storage;

pub fn run(project_root: Option<PathBuf>, tag: Option<&str>) -> Result<()> {
    let storage = Storage::new(project_root);

    let phase_tag = resolve_group_tag(&storage, tag, true)?;
    let tasks = storage.load_tasks()?;
    let phase = tasks
        .get(&phase_tag)
        .ok_or_else(|| anyhow::anyhow!("Phase '{}' not found", phase_tag))?;
    let active_phase = &phase.name;

    let stats = phase.get_stats();

    let completion_pct = if stats.total > 0 {
        (stats.done as f32 / stats.total as f32 * 100.0) as u32
    } else {
        0
    };

    println!(
        "\n{} {}",
        "Phase Statistics:".blue().bold(),
        active_phase.green()
    );
    println!("{}", "=================".blue());
    println!();
    println!("{:<20} {}", "Total Tasks:".yellow(), stats.total);
    println!("{:<20} {}", "Pending:".yellow(), stats.pending);
    println!("{:<20} {}", "In Progress:".yellow(), stats.in_progress);
    println!(
        "{:<20} {}",
        "Done:".yellow(),
        stats.done.to_string().green()
    );
    println!(
        "{:<20} {}",
        "Blocked:".yellow(),
        stats.blocked.to_string().red()
    );
    println!();
    println!(
        "{:<20} {}",
        "Total Complexity:".yellow(),
        stats.total_complexity
    );
    println!(
        "{:<20} {}%",
        "Completion:".yellow(),
        completion_pct.to_string().green()
    );

    // Show progress bar
    let bar_length = 50;
    let filled = (completion_pct as f32 / 100.0 * bar_length as f32) as usize;
    let empty = bar_length - filled;
    let bar = format!("[{}{}]", "=".repeat(filled).green(), " ".repeat(empty));
    println!("\n{}", bar);
    println!();

    Ok(())
}
</file>

<file path="scud-cli/src/commands/tags.rs">
use anyhow::Result;
use colored::Colorize;
use dialoguer::Select;
use std::path::PathBuf;

use crate::commands::helpers::is_interactive;
use crate::storage::Storage;

/// List phase tags or set active tag
///
/// Usage:
///   scud tags         - List all tags, prompt to select if interactive
///   scud tags <tag>   - Set active tag
pub fn run(project_root: Option<PathBuf>, set_tag: Option<&str>) -> Result<()> {
    let storage = Storage::new(project_root);
    let tasks = storage.load_tasks()?;

    if tasks.is_empty() {
        println!("{}", "No phases found.".yellow());
        println!("Create one with: scud parse-prd <file> --tag <tag>");
        return Ok(());
    }

    // If tag provided, set it as active (absorbs use-tag functionality)
    if let Some(tag) = set_tag {
        if !tasks.contains_key(tag) {
            anyhow::bail!("Phase '{}' not found", tag);
        }
        storage.set_active_group(tag)?;
        println!("{} {}", "Active phase:".green(), tag.green().bold());

        if let Some(phase) = tasks.get(tag) {
            let stats = phase.get_stats();
            println!(
                "  {} tasks ({} pending, {} in-progress, {} done)",
                stats.total, stats.pending, stats.in_progress, stats.done
            );
        }
        return Ok(());
    }

    // Display all tags
    let active_phase = storage.get_active_group()?;
    println!("{}", "Phase Tags:".blue().bold());
    println!();

    let mut tag_list: Vec<&String> = tasks.keys().collect();
    tag_list.sort();

    for (idx, tag) in tag_list.iter().enumerate() {
        let phase = tasks.get(*tag).unwrap();
        let stats = phase.get_stats();
        let is_active = active_phase.as_ref() == Some(*tag);

        let indicator = if is_active {
            "â—".green()
        } else {
            "â—‹".white()
        };
        let tag_display = if is_active {
            tag.green().bold()
        } else {
            tag.normal()
        };

        println!(
            "  {} [{}] {} ({} tasks, {} pending, {} done)",
            indicator,
            idx + 1,
            tag_display,
            stats.total,
            stats.pending,
            stats.done
        );
    }

    println!();

    // Interactive selection if no active phase or user is in interactive mode
    if is_interactive() {
        let default_idx = active_phase
            .as_ref()
            .and_then(|a| tag_list.iter().position(|t| *t == a))
            .unwrap_or(0);

        let selection = Select::new()
            .with_prompt("Select phase to activate (Ctrl+C to cancel)")
            .items(&tag_list)
            .default(default_idx)
            .interact_opt()?;

        if let Some(idx) = selection {
            let selected = tag_list[idx];
            storage.set_active_group(selected)?;
            println!("\n{} {}", "Active phase:".green(), selected.green().bold());
        }
    } else if active_phase.is_none() {
        println!("{}", "Set active phase: scud tags <tag>".yellow());
    }

    Ok(())
}
</file>

<file path="scud-cli/src/commands/warmup.rs">
use anyhow::Result;
use colored::Colorize;
use std::path::PathBuf;
use std::process::Command;

use crate::commands::helpers::flatten_all_tasks;
use crate::storage::Storage;

pub fn run(project_root: Option<PathBuf>) -> Result<()> {
    let storage = Storage::new(project_root);

    if !storage.is_initialized() {
        println!("{}", "SCUD not initialized. Run: scud init".yellow());
        return Ok(());
    }

    println!("{}", "SCUD Session Warmup".cyan().bold());
    println!("{}", "=".repeat(50).dimmed());

    // 1. Show working directory
    let cwd = std::env::current_dir()?;
    println!("\n{} {}", "Working directory:".bold(), cwd.display());

    // 2. Show recent git commits
    println!("\n{}", "Recent commits:".bold());
    match Command::new("git")
        .args(["log", "--oneline", "-5", "--no-decorate"])
        .output()
    {
        Ok(output) if output.status.success() => {
            let commits = String::from_utf8_lossy(&output.stdout);
            if commits.trim().is_empty() {
                println!("  {}", "(no commits yet)".dimmed());
            } else {
                for line in commits.lines() {
                    println!("  {}", line.dimmed());
                }
            }
        }
        _ => println!("  {}", "(not a git repository)".dimmed()),
    }

    // 3. Show active tag and stats
    println!("\n{}", "Task status:".bold());
    match storage.get_active_group()? {
        Some(tag) => {
            println!("  Active tag: {}", tag.green());

            // Load and show stats
            if let Ok(phase) = storage.load_group(&tag) {
                let stats = phase.get_stats();
                println!(
                    "  Progress: {}/{} tasks done ({}%)",
                    stats.done.to_string().green(),
                    stats.total,
                    if stats.total > 0 {
                        (stats.done * 100 / stats.total).to_string()
                    } else {
                        "0".to_string()
                    }
                );
                println!(
                    "  Status: {} pending, {} in-progress, {} blocked",
                    stats.pending.to_string().yellow(),
                    stats.in_progress.to_string().cyan(),
                    stats.blocked.to_string().red()
                );
            }
        }
        None => {
            println!("  {}", "No active tag set".yellow());
            println!("  Run: scud tags <tag-name>");
        }
    }

    // 4. Show active sessions (who's working on what)
    println!("\n{}", "Active sessions:".bold());
    let tasks = storage.load_tasks()?;
    let mut found_sessions = false;

    for (tag, phase) in &tasks {
        for task in &phase.tasks {
            if task.is_locked() {
                found_sessions = true;
                let age = task
                    .lock_age_hours()
                    .map(|h| format!("{:.1}h", h))
                    .unwrap_or_else(|| "?".to_string());
                let stale = task.is_stale_lock(1.0);
                let stale_marker = if stale {
                    " (STALE)".red().to_string()
                } else {
                    "".to_string()
                };
                println!(
                    "  {} | {} | {} | {}{}",
                    tag.dimmed(),
                    task.id.cyan(),
                    task.locked_by.as_deref().unwrap_or("?").yellow(),
                    age,
                    stale_marker
                );
            }
        }
    }
    if !found_sessions {
        println!("  {}", "(no active sessions)".dimmed());
    }

    // 5. Show next available task (with cross-tag dependency checking)
    println!("\n{}", "Next available task:".bold());
    let all_tasks_flat = flatten_all_tasks(&tasks);
    if let Some(tag) = storage.get_active_group()? {
        if let Some(phase) = tasks.get(&tag) {
            let available: Vec<_> = phase
                .tasks
                .iter()
                .filter(|t| {
                    t.status == crate::models::TaskStatus::Pending
                        && t.has_dependencies_met_refs(&all_tasks_flat)
                        && !t.is_locked()
                })
                .collect();

            if let Some(task) = available.first() {
                println!(
                    "  {} {} (complexity: {})",
                    task.id.cyan(),
                    task.title,
                    task.complexity
                );
                println!("  Run: {}", "scud next --claim --name <your-name>".green());
            } else if phase
                .tasks
                .iter()
                .all(|t| t.status == crate::models::TaskStatus::Done)
            {
                println!("  {}", "All tasks complete!".green());
            } else {
                println!(
                    "  {}",
                    "(no tasks available - check dependencies or locks)".yellow()
                );
            }
        }
    } else {
        println!("  {}", "(set active tag first)".dimmed());
    }

    // 6. Check for stale locks
    let stale_count: usize = tasks
        .values()
        .flat_map(|p| p.tasks.iter())
        .filter(|t| t.is_stale_lock(1.0))
        .count();

    if stale_count > 0 {
        println!(
            "\n{} {} stale lock(s) detected. Run: {}",
            "Warning:".yellow().bold(),
            stale_count,
            "scud doctor --fix".cyan()
        );
    }

    println!("\n{}", "=".repeat(50).dimmed());
    println!(
        "Ready to work. Use {} to find your next task.",
        "scud next".cyan()
    );

    Ok(())
}
</file>

<file path="scud-cli/src/commands/waves.rs">
use anyhow::Result;
use colored::Colorize;
use std::collections::{HashMap, HashSet};
use std::path::PathBuf;

use crate::models::task::{Task, TaskStatus};
use crate::storage::Storage;

#[derive(Debug)]
pub struct Wave {
    pub number: usize,
    pub tasks: Vec<String>,
}

pub fn run(
    project_root: Option<PathBuf>,
    tag: Option<&str>,
    max_parallel: usize,
    all_tags: bool,
) -> Result<()> {
    // Validate max_parallel to prevent divide-by-zero panics
    if max_parallel == 0 {
        anyhow::bail!("--max-parallel must be at least 1");
    }

    let storage = Storage::new(project_root);
    let all_tasks = storage.load_tasks()?;

    // Determine which phase(s) to plan
    let phase_tags: Vec<String> = if all_tags {
        all_tasks.keys().cloned().collect()
    } else if let Some(t) = tag {
        if !all_tasks.contains_key(t) {
            anyhow::bail!("Phase '{}' not found. Run: scud tags", t);
        }
        vec![t.to_string()]
    } else {
        // Use active phase
        let active = storage.get_active_group()?;
        match active {
            Some(t) => vec![t],
            None => anyhow::bail!("No active task group. Use --tag <phase-tag> or run: scud tags"),
        }
    };

    // Collect actionable tasks from specified phase(s)
    let mut actionable: Vec<&Task> = Vec::new();
    for tag in &phase_tags {
        if let Some(phase) = all_tasks.get(tag) {
            for task in &phase.tasks {
                // Only include actionable tasks (not done, not expanded parents, not cancelled)
                if task.status != TaskStatus::Done
                    && task.status != TaskStatus::Expanded
                    && task.status != TaskStatus::Cancelled
                {
                    // If it's a subtask, only include if parent is expanded
                    if let Some(ref parent_id) = task.parent_id {
                        let parent_expanded = phase
                            .get_task(parent_id)
                            .map(|p| p.is_expanded())
                            .unwrap_or(false);
                        if parent_expanded {
                            actionable.push(task);
                        }
                    } else {
                        // Top-level task that's not expanded
                        actionable.push(task);
                    }
                }
            }
        }
    }

    if actionable.is_empty() {
        println!("{}", "No actionable tasks found.".yellow());
        println!("All tasks may be completed, expanded, or cancelled.");
        return Ok(());
    }

    // Build dependency graph and compute waves
    let waves = compute_waves(&actionable, max_parallel);

    // Display waves
    println!(
        "\n{} {}",
        "Execution Waves".blue().bold(),
        format!("(max {} parallel)", max_parallel).dimmed()
    );
    println!("{}", "=".repeat(50).blue());
    println!();

    let mut total_tasks = 0;
    for wave in &waves {
        total_tasks += wave.tasks.len();

        let batch_info = if wave.tasks.len() > max_parallel {
            format!(
                " (batched into {} rounds)",
                wave.tasks.len().div_ceil(max_parallel)
            )
        } else {
            String::new()
        };

        println!(
            "{} {} task{}{}",
            format!("Wave {}:", wave.number).yellow().bold(),
            wave.tasks.len(),
            if wave.tasks.len() == 1 { "" } else { "s" },
            batch_info.dimmed()
        );

        for (round_idx, chunk) in wave.tasks.chunks(max_parallel).enumerate() {
            if wave.tasks.len() > max_parallel {
                println!("  {} {}", "Round".dimmed(), round_idx + 1);
            }

            for task_id in chunk {
                // Find task details
                if let Some(task) = actionable.iter().find(|t| &t.id == task_id) {
                    let status_indicator = match task.status {
                        TaskStatus::Pending => "â—‹".white(),
                        TaskStatus::InProgress => "â—".cyan(),
                        TaskStatus::Blocked => "âœ—".red(),
                        _ => "?".dimmed(),
                    };

                    let deps = if task.dependencies.is_empty() {
                        String::new()
                    } else {
                        format!(" <- {}", task.dependencies.join(", "))
                            .dimmed()
                            .to_string()
                    };

                    let complexity = if task.complexity > 0 {
                        format!(" [{}]", task.complexity).dimmed().to_string()
                    } else {
                        String::new()
                    };

                    println!(
                        "    {} {} {}{}{}",
                        status_indicator,
                        task_id.cyan(),
                        task.title,
                        complexity,
                        deps
                    );
                }
            }
        }
        println!();
    }

    // Summary
    println!("{}", "Summary".blue().bold());
    println!("{}", "-".repeat(30).blue());

    let total_waves = waves.len();
    let total_rounds: usize = waves
        .iter()
        .map(|w| w.tasks.len().div_ceil(max_parallel))
        .sum();

    println!("  Total tasks:   {}", total_tasks);
    println!("  Total waves:   {}", total_waves);
    println!("  Total rounds:  {}", total_rounds);

    if total_tasks > 0 && total_rounds > 0 {
        let speedup = total_tasks as f64 / total_rounds as f64;
        println!("  Speedup:       {}", format!("{:.1}x", speedup).green());
        println!(
            "  {}",
            format!(
                "(from {} sequential to {} parallel rounds)",
                total_tasks, total_rounds
            )
            .dimmed()
        );
    }

    // Show blocked tasks if any
    let blocked: Vec<_> = actionable
        .iter()
        .filter(|t| t.status == TaskStatus::Blocked)
        .collect();
    if !blocked.is_empty() {
        println!();
        println!("{}", "Blocked Tasks:".red().bold());
        for task in blocked {
            println!("  {} {}", task.id.red(), task.title);
        }
    }

    println!();

    Ok(())
}

/// Compute execution waves using Kahn's algorithm (topological sort with level assignment)
/// When processing tasks from multiple phases, we namespace task IDs to avoid collisions
fn compute_waves(tasks: &[&Task], _max_parallel: usize) -> Vec<Wave> {
    // Build a map from task pointer to its namespaced ID
    // This handles the case where multiple phases have tasks with the same local ID
    let task_ids: HashSet<String> = tasks.iter().map(|t| t.id.clone()).collect();

    // Build in-degree map (how many dependencies does each task have within our set?)
    let mut in_degree: HashMap<String, usize> = HashMap::new();
    let mut dependents: HashMap<String, Vec<String>> = HashMap::new();

    for task in tasks {
        in_degree.entry(task.id.clone()).or_insert(0);

        for dep in &task.dependencies {
            // Only count dependencies that are in our actionable task set
            if task_ids.contains(dep) {
                *in_degree.entry(task.id.clone()).or_insert(0) += 1;
                dependents
                    .entry(dep.clone())
                    .or_default()
                    .push(task.id.clone());
            }
        }
    }

    // Kahn's algorithm with wave tracking
    let mut waves: Vec<Wave> = Vec::new();
    let mut remaining = in_degree.clone();
    let mut wave_number = 1;

    while !remaining.is_empty() {
        // Find all tasks with no remaining dependencies (in-degree = 0)
        let ready: Vec<String> = remaining
            .iter()
            .filter(|(_, &deg)| deg == 0)
            .map(|(id, _)| id.clone())
            .collect();

        if ready.is_empty() {
            // Circular dependency detected
            println!("{}", "Warning: Circular dependency detected!".red().bold());
            println!("The following tasks have unresolved dependencies:");
            for id in remaining.keys() {
                if let Some(task) = tasks.iter().find(|t| &t.id == id) {
                    let unmet_deps: Vec<_> = task
                        .dependencies
                        .iter()
                        .filter(|d| remaining.contains_key(*d))
                        .collect();
                    println!("  {} depends on {:?}", id, unmet_deps);
                }
            }
            break;
        }

        // Remove ready tasks from remaining and update dependents
        for task_id in &ready {
            remaining.remove(task_id);

            if let Some(deps) = dependents.get(task_id) {
                for dep_id in deps {
                    if let Some(deg) = remaining.get_mut(dep_id) {
                        *deg = deg.saturating_sub(1);
                    }
                }
            }
        }

        waves.push(Wave {
            number: wave_number,
            tasks: ready,
        });
        wave_number += 1;
    }

    waves
}
</file>

<file path="scud-cli/src/commands/whois.rs">
use anyhow::Result;
use colored::Colorize;
use std::collections::HashMap;
use std::path::PathBuf;

use crate::commands::helpers::resolve_group_tag;
use crate::storage::Storage;

pub fn run(project_root: Option<PathBuf>, tag: Option<&str>) -> Result<()> {
    let storage = Storage::new(project_root);

    // If tag provided, only show that epic. Otherwise show all epics.
    let (tasks, scope_label) = if let Some(t) = tag {
        let epic_tag = resolve_group_tag(&storage, Some(t), false)?;
        let all_tasks = storage.load_tasks()?;
        let epic = all_tasks
            .get(&epic_tag)
            .ok_or_else(|| anyhow::anyhow!("Epic '{}' not found", epic_tag))?;
        let mut filtered = HashMap::new();
        filtered.insert(epic_tag.clone(), epic.clone());
        (filtered, format!("(epic: {})", epic_tag))
    } else {
        (storage.load_tasks()?, "(all epics)".to_string())
    };

    let mut assignments: HashMap<String, Vec<(String, String, String)>> = HashMap::new();
    let mut stale_locks: Vec<(String, String, String, f64)> = Vec::new();

    // Collect all assignments across all epics
    for (epic_tag, epic) in tasks.iter() {
        for task in &epic.tasks {
            if let Some(ref assigned) = task.assigned_to {
                assignments.entry(assigned.clone()).or_default().push((
                    epic_tag.clone(),
                    task.id.clone(),
                    task.title.clone(),
                ));
            }

            // Check for stale locks
            if task.is_stale_lock(24.0) {
                if let (Some(locked_by), Some(age)) = (&task.locked_by, task.lock_age_hours()) {
                    stale_locks.push((epic_tag.clone(), task.id.clone(), locked_by.clone(), age));
                }
            }
        }
    }

    if assignments.is_empty() {
        println!("{}", "No tasks are currently assigned".yellow());
        println!();
        println!("{}", "Assign tasks with:".blue());
        println!("  scud assign <task-id> <assignee>");
        println!("  scud claim <task-id> --name <your-name>");
        return Ok(());
    }

    println!(
        "\n{} {}",
        "Task Assignments".blue().bold(),
        scope_label.dimmed()
    );
    println!("{}", "=".repeat(60).blue());
    println!();

    for (assignee, tasks_list) in assignments.iter() {
        println!("{} {}", "â—".green(), assignee.green().bold());
        for (epic, task_id, title) in tasks_list {
            println!("  {} {} - {}", epic.cyan(), task_id.yellow(), title);
        }
        println!();
    }

    // Show stale locks warning
    if !stale_locks.is_empty() {
        println!("{}", "âš  Stale Locks (>24h)".yellow().bold());
        println!("{}", "=".repeat(60).yellow());
        println!();

        for (epic, task_id, locked_by, age) in stale_locks {
            println!(
                "  {} {} locked by {} ({:.1}h ago)",
                epic.cyan(),
                task_id.yellow(),
                locked_by.red(),
                age
            );
        }
        println!();
        println!("{}", "Consider releasing stale locks:".blue());
        println!("  scud release <task-id> --force");
        println!();
    }

    Ok(())
}
</file>

<file path="scud-cli/src/formats/mod.rs">
//! Task graph serialization formats
//!
//! This module provides parsers and serializers for different
//! task storage formats.

mod scg;

pub use scg::{parse_scg, serialize_scg};

/// Supported file formats
#[derive(Debug, Clone, Copy, PartialEq)]
pub enum Format {
    /// Legacy JSON format
    Json,
    /// SCUD Graph format (.scg)
    Scg,
}

impl Format {
    pub fn from_extension(ext: &str) -> Option<Self> {
        match ext.to_lowercase().as_str() {
            "json" => Some(Format::Json),
            "scg" => Some(Format::Scg),
            _ => None,
        }
    }

    pub fn extension(&self) -> &'static str {
        match self {
            Format::Json => "json",
            Format::Scg => "scg",
        }
    }
}
</file>

<file path="scud-cli/src/formats/scg.rs">
//! SCUD Graph (.scg) format parser and serializer
//!
//! A token-efficient, graph-native format for task storage.

use anyhow::{Context, Result};
use std::collections::HashMap;

use crate::models::{Phase, Priority, Task, TaskStatus};

const FORMAT_VERSION: &str = "v1";
const HEADER_PREFIX: &str = "# SCUD Graph";

/// Status code mapping
fn status_to_code(status: &TaskStatus) -> char {
    match status {
        TaskStatus::Pending => 'P',
        TaskStatus::InProgress => 'I',
        TaskStatus::Done => 'D',
        TaskStatus::Review => 'R',
        TaskStatus::Blocked => 'B',
        TaskStatus::Deferred => 'F',
        TaskStatus::Cancelled => 'C',
        TaskStatus::Expanded => 'X',
    }
}

fn code_to_status(code: char) -> Option<TaskStatus> {
    match code {
        'P' => Some(TaskStatus::Pending),
        'I' => Some(TaskStatus::InProgress),
        'D' => Some(TaskStatus::Done),
        'R' => Some(TaskStatus::Review),
        'B' => Some(TaskStatus::Blocked),
        'F' => Some(TaskStatus::Deferred),
        'C' => Some(TaskStatus::Cancelled),
        'X' => Some(TaskStatus::Expanded),
        _ => None,
    }
}

fn priority_to_code(priority: &Priority) -> char {
    match priority {
        Priority::Critical => 'C',
        Priority::High => 'H',
        Priority::Medium => 'M',
        Priority::Low => 'L',
    }
}

fn code_to_priority(code: char) -> Option<Priority> {
    match code {
        'C' => Some(Priority::Critical),
        'H' => Some(Priority::High),
        'M' => Some(Priority::Medium),
        'L' => Some(Priority::Low),
        _ => None,
    }
}

/// Escape special characters in text fields
fn escape_text(text: &str) -> String {
    text.replace('\\', "\\\\")
        .replace('|', "\\|")
        .replace('\n', "\\n")
}

/// Unescape special characters
fn unescape_text(text: &str) -> String {
    let mut result = String::with_capacity(text.len());
    let mut chars = text.chars().peekable();

    while let Some(c) = chars.next() {
        if c == '\\' {
            match chars.next() {
                Some('\\') => result.push('\\'),
                Some('|') => result.push('|'),
                Some('n') => result.push('\n'),
                Some(other) => {
                    result.push('\\');
                    result.push(other);
                }
                None => result.push('\\'),
            }
        } else {
            result.push(c);
        }
    }
    result
}

/// Split a line by pipe character, respecting escaped pipes
fn split_by_pipe(line: &str) -> Vec<String> {
    let mut parts = Vec::new();
    let mut current = String::new();
    let mut chars = line.chars().peekable();

    while let Some(c) = chars.next() {
        if c == '\\' {
            // Check for escaped pipe or backslash
            if let Some(&next) = chars.peek() {
                if next == '|' || next == '\\' {
                    current.push(c);
                    current.push(chars.next().unwrap());
                    continue;
                }
            }
            current.push(c);
        } else if c == '|' {
            parts.push(current.trim().to_string());
            current = String::new();
        } else {
            current.push(c);
        }
    }
    parts.push(current.trim().to_string());
    parts
}

/// Parse SCG format into Phase
pub fn parse_scg(content: &str) -> Result<Phase> {
    let mut lines = content.lines().peekable();

    // Parse header
    let first_line = lines.next().context("Empty file")?;
    if !first_line.starts_with(HEADER_PREFIX) {
        anyhow::bail!(
            "Invalid SCG header: expected '{}', got '{}'",
            HEADER_PREFIX,
            first_line
        );
    }

    let phase_line = lines.next().context("Missing phase tag line")?;
    let phase_tag = phase_line
        .strip_prefix("# Phase:")
        .or_else(|| phase_line.strip_prefix("# Epic:")) // backwards compatibility
        .map(|s| s.trim())
        .context("Invalid phase line format")?;

    let mut phase = Phase::new(phase_tag.to_string());
    let mut tasks: HashMap<String, Task> = HashMap::new();
    let mut edges: Vec<(String, String)> = Vec::new();
    let mut parents: HashMap<String, Vec<String>> = HashMap::new();
    let mut details: HashMap<String, HashMap<String, String>> = HashMap::new();
    // Type: (assigned_to, locked_by, locked_at)
    type AssignmentInfo = (Option<String>, Option<String>, Option<String>);
    let mut assignments: HashMap<String, AssignmentInfo> = HashMap::new();

    // Track current section
    let mut current_section: Option<&str> = None;
    let mut current_detail_id: Option<String> = None;
    let mut current_detail_field: Option<String> = None;
    let mut current_detail_content: Vec<String> = Vec::new();

    for line in lines {
        let trimmed = line.trim();

        // Skip empty lines
        if trimmed.is_empty() {
            continue;
        }

        // Check for section headers
        if trimmed.starts_with('@') {
            // Flush any pending detail
            flush_detail(
                &current_detail_id,
                &current_detail_field,
                &mut current_detail_content,
                &mut details,
            );
            current_detail_id = None;
            current_detail_field = None;

            current_section = Some(match trimmed {
                "@meta {" | "@meta" => "meta",
                "@nodes" => "nodes",
                "@edges" => "edges",
                "@parents" => "parents",
                "@assignments" => "assignments",
                "@details" => "details",
                _ => continue,
            });
            continue;
        }

        // Handle continuation lines in details
        if current_section == Some("details")
            && line.starts_with("  ")
            && current_detail_id.is_some()
        {
            current_detail_content.push(line[2..].to_string());
            continue;
        }

        // Skip meta closing brace and comment lines
        if trimmed == "}" || trimmed.starts_with('#') {
            continue;
        }

        match current_section {
            Some("meta") => {
                // Parse "key value" pairs
                if let Some((key, value)) = trimmed.split_once(char::is_whitespace) {
                    let value = value.trim();
                    // Meta fields are informational, phase name is already set
                    if key == "name" && phase.name != value {
                        phase = Phase::new(value.to_string());
                    }
                }
            }
            Some("nodes") => {
                // Parse "id | title | status | complexity | priority"
                let parts = split_by_pipe(trimmed);
                if parts.len() >= 5 {
                    let id = parts[0].clone();
                    let title = unescape_text(&parts[1]);
                    let status =
                        code_to_status(parts[2].chars().next().unwrap_or('P')).unwrap_or_default();
                    let complexity: u32 = parts[3].parse().unwrap_or(0);
                    let priority = code_to_priority(parts[4].chars().next().unwrap_or('M'))
                        .unwrap_or_default();

                    let mut task = Task::new(id.clone(), title, String::new());
                    task.status = status;
                    task.complexity = complexity;
                    task.priority = priority;
                    tasks.insert(id, task);
                }
            }
            Some("edges") => {
                // Parse "dependent -> dependency"
                if let Some((dependent, dependency)) = trimmed.split_once("->") {
                    edges.push((dependent.trim().to_string(), dependency.trim().to_string()));
                }
            }
            Some("parents") => {
                // Parse "parent: child1, child2, ..."
                if let Some((parent, children)) = trimmed.split_once(':') {
                    let child_ids: Vec<String> = children
                        .split(',')
                        .map(|s| s.trim().to_string())
                        .filter(|s| !s.is_empty())
                        .collect();
                    parents.insert(parent.trim().to_string(), child_ids);
                }
            }
            Some("assignments") => {
                // Parse "id | assigned_to | locked_by | locked_at"
                let parts = split_by_pipe(trimmed);
                if parts.len() >= 4 {
                    let id = parts[0].clone();
                    let assigned = if parts[1].is_empty() {
                        None
                    } else {
                        Some(parts[1].clone())
                    };
                    let locked_by = if parts[2].is_empty() {
                        None
                    } else {
                        Some(parts[2].clone())
                    };
                    let locked_at = if parts[3].is_empty() {
                        None
                    } else {
                        Some(parts[3].clone())
                    };
                    assignments.insert(id, (assigned, locked_by, locked_at));
                }
            }
            Some("details") => {
                // Flush previous detail if starting new one
                flush_detail(
                    &current_detail_id,
                    &current_detail_field,
                    &mut current_detail_content,
                    &mut details,
                );

                // Parse "id | field |"
                let parts = split_by_pipe(trimmed);
                if parts.len() >= 2 {
                    current_detail_id = Some(parts[0].clone());
                    current_detail_field = Some(parts[1].clone());
                    current_detail_content.clear();
                }
            }
            _ => {}
        }
    }

    // Flush any remaining detail
    flush_detail(
        &current_detail_id,
        &current_detail_field,
        &mut current_detail_content,
        &mut details,
    );

    // Apply edges (dependencies)
    for (dependent, dependency) in edges {
        if let Some(task) = tasks.get_mut(&dependent) {
            task.dependencies.push(dependency);
        }
    }

    // Apply parent-child relationships
    for (parent_id, child_ids) in parents {
        if let Some(parent) = tasks.get_mut(&parent_id) {
            parent.subtasks = child_ids.clone();
        }
        for child_id in child_ids {
            if let Some(child) = tasks.get_mut(&child_id) {
                child.parent_id = Some(parent_id.clone());
            }
        }
    }

    // Apply details
    for (id, fields) in details {
        if let Some(task) = tasks.get_mut(&id) {
            if let Some(desc) = fields.get("description") {
                task.description = desc.clone();
            }
            if let Some(det) = fields.get("details") {
                task.details = Some(det.clone());
            }
            if let Some(ts) = fields.get("test_strategy") {
                task.test_strategy = Some(ts.clone());
            }
        }
    }

    // Apply assignments
    for (id, (assigned, locked_by, locked_at)) in assignments {
        if let Some(task) = tasks.get_mut(&id) {
            task.assigned_to = assigned;
            task.locked_by = locked_by;
            task.locked_at = locked_at;
        }
    }

    // Add all tasks to phase
    phase.tasks = tasks.into_values().collect();

    // Sort tasks by ID for consistent ordering
    phase.tasks.sort_by(|a, b| natural_sort_ids(&a.id, &b.id));

    Ok(phase)
}

/// Natural sort for task IDs: "1" < "2" < "10", "1.1" < "1.2" < "1.10"
fn natural_sort_ids(a: &str, b: &str) -> std::cmp::Ordering {
    let a_parts: Vec<&str> = a.split('.').collect();
    let b_parts: Vec<&str> = b.split('.').collect();

    for (ap, bp) in a_parts.iter().zip(b_parts.iter()) {
        match (ap.parse::<u32>(), bp.parse::<u32>()) {
            (Ok(an), Ok(bn)) => {
                if an != bn {
                    return an.cmp(&bn);
                }
            }
            _ => {
                if ap != bp {
                    return ap.cmp(bp);
                }
            }
        }
    }
    a_parts.len().cmp(&b_parts.len())
}

/// Helper to flush current detail
fn flush_detail(
    id: &Option<String>,
    field: &Option<String>,
    content: &mut Vec<String>,
    details: &mut HashMap<String, HashMap<String, String>>,
) {
    if let (Some(id), Some(field)) = (id, field) {
        let text = content.join("\n");
        details
            .entry(id.clone())
            .or_default()
            .insert(field.clone(), text);
        content.clear();
    }
}

/// Serialize Phase to SCG format
pub fn serialize_scg(phase: &Phase) -> String {
    let mut output = String::new();

    // Header
    output.push_str(&format!("{} {}\n", HEADER_PREFIX, FORMAT_VERSION));
    output.push_str(&format!("# Phase: {}\n\n", phase.name));

    // Meta section
    let now = chrono::Utc::now().to_rfc3339();
    output.push_str("@meta {\n");
    output.push_str(&format!("  name {}\n", phase.name));
    output.push_str(&format!("  updated {}\n", now));
    output.push_str("}\n\n");

    // Sort tasks for consistent output
    let mut sorted_tasks = phase.tasks.clone();
    sorted_tasks.sort_by(|a, b| natural_sort_ids(&a.id, &b.id));

    // Nodes section
    output.push_str("@nodes\n");
    output.push_str("# id | title | status | complexity | priority\n");
    for task in &sorted_tasks {
        output.push_str(&format!(
            "{} | {} | {} | {} | {}\n",
            task.id,
            escape_text(&task.title),
            status_to_code(&task.status),
            task.complexity,
            priority_to_code(&task.priority)
        ));
    }
    output.push('\n');

    // Edges section (dependencies)
    let edges: Vec<_> = sorted_tasks
        .iter()
        .flat_map(|t| t.dependencies.iter().map(move |dep| (&t.id, dep)))
        .collect();

    if !edges.is_empty() {
        output.push_str("@edges\n");
        output.push_str("# dependent -> dependency\n");
        for (dependent, dependency) in edges {
            output.push_str(&format!("{} -> {}\n", dependent, dependency));
        }
        output.push('\n');
    }

    // Parents section
    let parents: Vec<_> = sorted_tasks
        .iter()
        .filter(|t| !t.subtasks.is_empty())
        .collect();

    if !parents.is_empty() {
        output.push_str("@parents\n");
        output.push_str("# parent: subtasks...\n");
        for task in parents {
            output.push_str(&format!("{}: {}\n", task.id, task.subtasks.join(", ")));
        }
        output.push('\n');
    }

    // Assignments section
    let assignments: Vec<_> = sorted_tasks
        .iter()
        .filter(|t| t.assigned_to.is_some() || t.locked_by.is_some())
        .collect();

    if !assignments.is_empty() {
        output.push_str("@assignments\n");
        output.push_str("# id | assigned_to | locked_by | locked_at\n");
        for task in assignments {
            output.push_str(&format!(
                "{} | {} | {} | {}\n",
                task.id,
                task.assigned_to.as_deref().unwrap_or(""),
                task.locked_by.as_deref().unwrap_or(""),
                task.locked_at.as_deref().unwrap_or("")
            ));
        }
        output.push('\n');
    }

    // Details section
    let tasks_with_details: Vec<_> = sorted_tasks
        .iter()
        .filter(|t| !t.description.is_empty() || t.details.is_some() || t.test_strategy.is_some())
        .collect();

    if !tasks_with_details.is_empty() {
        output.push_str("@details\n");
        for task in tasks_with_details {
            if !task.description.is_empty() {
                output.push_str(&format!("{} | description |\n", task.id));
                for line in task.description.lines() {
                    output.push_str(&format!("  {}\n", line));
                }
            }
            if let Some(ref details) = task.details {
                output.push_str(&format!("{} | details |\n", task.id));
                for line in details.lines() {
                    output.push_str(&format!("  {}\n", line));
                }
            }
            if let Some(ref test_strategy) = task.test_strategy {
                output.push_str(&format!("{} | test_strategy |\n", task.id));
                for line in test_strategy.lines() {
                    output.push_str(&format!("  {}\n", line));
                }
            }
        }
    }

    output
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_status_codes() {
        assert_eq!(status_to_code(&TaskStatus::Pending), 'P');
        assert_eq!(status_to_code(&TaskStatus::InProgress), 'I');
        assert_eq!(status_to_code(&TaskStatus::Done), 'D');
        assert_eq!(status_to_code(&TaskStatus::Expanded), 'X');

        assert_eq!(code_to_status('P'), Some(TaskStatus::Pending));
        assert_eq!(code_to_status('X'), Some(TaskStatus::Expanded));
        assert_eq!(code_to_status('Z'), None);
    }

    #[test]
    fn test_priority_codes() {
        assert_eq!(priority_to_code(&Priority::Critical), 'C');
        assert_eq!(priority_to_code(&Priority::High), 'H');
        assert_eq!(priority_to_code(&Priority::Medium), 'M');
        assert_eq!(priority_to_code(&Priority::Low), 'L');

        assert_eq!(code_to_priority('C'), Some(Priority::Critical));
        assert_eq!(code_to_priority('H'), Some(Priority::High));
        assert_eq!(code_to_priority('M'), Some(Priority::Medium));
        assert_eq!(code_to_priority('L'), Some(Priority::Low));
        assert_eq!(code_to_priority('Z'), None);
    }

    #[test]
    fn test_escape_unescape() {
        assert_eq!(escape_text("hello|world"), "hello\\|world");
        assert_eq!(escape_text("line1\nline2"), "line1\\nline2");
        assert_eq!(unescape_text("hello\\|world"), "hello|world");
        assert_eq!(unescape_text("line1\\nline2"), "line1\nline2");
    }

    #[test]
    fn test_round_trip() {
        let mut epic = Phase::new("test-epic".to_string());

        let mut task1 = Task::new(
            "1".to_string(),
            "First task".to_string(),
            "Description".to_string(),
        );
        task1.complexity = 5;
        task1.priority = Priority::High;
        task1.status = TaskStatus::Done;

        let mut task2 = Task::new(
            "2".to_string(),
            "Second task".to_string(),
            "Another desc".to_string(),
        );
        task2.dependencies = vec!["1".to_string()];
        task2.complexity = 3;

        epic.add_task(task1);
        epic.add_task(task2);

        let scg = serialize_scg(&epic);
        let parsed = parse_scg(&scg).unwrap();

        assert_eq!(parsed.name, "test-epic");
        assert_eq!(parsed.tasks.len(), 2);

        let t1 = parsed.get_task("1").unwrap();
        assert_eq!(t1.title, "First task");
        assert_eq!(t1.complexity, 5);
        assert_eq!(t1.status, TaskStatus::Done);

        let t2 = parsed.get_task("2").unwrap();
        assert_eq!(t2.dependencies, vec!["1".to_string()]);
    }

    #[test]
    fn test_parent_child() {
        let mut epic = Phase::new("parent-test".to_string());

        let mut parent = Task::new(
            "1".to_string(),
            "Parent".to_string(),
            "Parent task".to_string(),
        );
        parent.status = TaskStatus::Expanded;
        parent.subtasks = vec!["1.1".to_string(), "1.2".to_string()];

        let mut child1 = Task::new(
            "1.1".to_string(),
            "Child 1".to_string(),
            "First child".to_string(),
        );
        child1.parent_id = Some("1".to_string());

        let mut child2 = Task::new(
            "1.2".to_string(),
            "Child 2".to_string(),
            "Second child".to_string(),
        );
        child2.parent_id = Some("1".to_string());
        child2.dependencies = vec!["1.1".to_string()];

        epic.add_task(parent);
        epic.add_task(child1);
        epic.add_task(child2);

        let scg = serialize_scg(&epic);
        let parsed = parse_scg(&scg).unwrap();

        let p = parsed.get_task("1").unwrap();
        assert_eq!(p.subtasks, vec!["1.1", "1.2"]);

        let c1 = parsed.get_task("1.1").unwrap();
        assert_eq!(c1.parent_id, Some("1".to_string()));

        let c2 = parsed.get_task("1.2").unwrap();
        assert_eq!(c2.parent_id, Some("1".to_string()));
        assert_eq!(c2.dependencies, vec!["1.1".to_string()]);
    }

    #[test]
    fn test_malformed_header() {
        let result = parse_scg("not a valid scg file");
        assert!(result.is_err());
    }

    #[test]
    fn test_empty_phase() {
        let content = "# SCUD Graph v1\n# Phase: empty\n\n@nodes\n# id | title | status | complexity | priority\n";
        let phase = parse_scg(content).unwrap();
        assert_eq!(phase.name, "empty");
        assert!(phase.tasks.is_empty());
    }

    #[test]
    fn test_special_characters_in_title() {
        let mut epic = Phase::new("test".to_string());
        let task = Task::new(
            "1".to_string(),
            "Task with | pipe".to_string(),
            "Desc".to_string(),
        );
        epic.add_task(task);

        let scg = serialize_scg(&epic);
        let parsed = parse_scg(&scg).unwrap();

        assert_eq!(parsed.get_task("1").unwrap().title, "Task with | pipe");
    }

    #[test]
    fn test_multiline_description() {
        let mut epic = Phase::new("test".to_string());
        let task = Task::new(
            "1".to_string(),
            "Task".to_string(),
            "Line 1\nLine 2\nLine 3".to_string(),
        );
        epic.add_task(task);

        let scg = serialize_scg(&epic);
        let parsed = parse_scg(&scg).unwrap();

        let t = parsed.get_task("1").unwrap();
        assert_eq!(t.description, "Line 1\nLine 2\nLine 3");
    }

    #[test]
    fn test_assignments() {
        let mut epic = Phase::new("test".to_string());
        let mut task = Task::new("1".to_string(), "Task".to_string(), "Desc".to_string());
        task.assigned_to = Some("alice".to_string());
        task.locked_by = Some("alice".to_string());
        task.locked_at = Some("2025-01-01T00:00:00Z".to_string());
        epic.add_task(task);

        let scg = serialize_scg(&epic);
        let parsed = parse_scg(&scg).unwrap();

        let t = parsed.get_task("1").unwrap();
        assert_eq!(t.assigned_to, Some("alice".to_string()));
        assert_eq!(t.locked_by, Some("alice".to_string()));
        assert_eq!(t.locked_at, Some("2025-01-01T00:00:00Z".to_string()));
    }

    #[test]
    fn test_natural_sort_order() {
        let mut epic = Phase::new("test".to_string());

        // Add tasks in random order
        for id in ["10", "2", "1", "1.10", "1.2", "1.1"] {
            let task = Task::new(id.to_string(), format!("Task {}", id), String::new());
            epic.add_task(task);
        }

        let scg = serialize_scg(&epic);
        let parsed = parse_scg(&scg).unwrap();

        let ids: Vec<&str> = parsed.tasks.iter().map(|t| t.id.as_str()).collect();
        assert_eq!(ids, vec!["1", "1.1", "1.2", "1.10", "2", "10"]);
    }

    #[test]
    fn test_all_statuses() {
        let mut epic = Phase::new("test".to_string());

        let statuses = [
            ("1", TaskStatus::Pending),
            ("2", TaskStatus::InProgress),
            ("3", TaskStatus::Done),
            ("4", TaskStatus::Review),
            ("5", TaskStatus::Blocked),
            ("6", TaskStatus::Deferred),
            ("7", TaskStatus::Cancelled),
            ("8", TaskStatus::Expanded),
        ];

        for (id, status) in &statuses {
            let mut task = Task::new(id.to_string(), format!("Task {}", id), String::new());
            task.status = status.clone();
            epic.add_task(task);
        }

        let scg = serialize_scg(&epic);
        let parsed = parse_scg(&scg).unwrap();

        for (id, expected_status) in statuses {
            let task = parsed.get_task(id).unwrap();
            assert_eq!(
                task.status, expected_status,
                "Status mismatch for task {}",
                id
            );
        }
    }

    #[test]
    fn test_all_priorities() {
        let mut epic = Phase::new("test".to_string());

        let priorities = [
            ("1", Priority::Critical),
            ("2", Priority::High),
            ("3", Priority::Medium),
            ("4", Priority::Low),
        ];

        for (id, priority) in &priorities {
            let mut task = Task::new(id.to_string(), format!("Task {}", id), String::new());
            task.priority = priority.clone();
            epic.add_task(task);
        }

        let scg = serialize_scg(&epic);
        let parsed = parse_scg(&scg).unwrap();

        for (id, expected_priority) in priorities {
            let task = parsed.get_task(id).unwrap();
            assert_eq!(
                task.priority, expected_priority,
                "Priority mismatch for task {}",
                id
            );
        }
    }

    #[test]
    fn test_details_and_test_strategy() {
        let mut epic = Phase::new("test".to_string());
        let mut task = Task::new(
            "1".to_string(),
            "Task".to_string(),
            "Description".to_string(),
        );
        task.details = Some("Detailed implementation notes".to_string());
        task.test_strategy = Some("Unit tests and integration tests".to_string());
        epic.add_task(task);

        let scg = serialize_scg(&epic);
        let parsed = parse_scg(&scg).unwrap();

        let t = parsed.get_task("1").unwrap();
        assert_eq!(t.description, "Description");
        assert_eq!(t.details, Some("Detailed implementation notes".to_string()));
        assert_eq!(
            t.test_strategy,
            Some("Unit tests and integration tests".to_string())
        );
    }

    // Additional edge case tests

    #[test]
    fn test_backslash_escape() {
        let mut epic = Phase::new("test".to_string());
        let task = Task::new(
            "1".to_string(),
            "Task with \\ backslash".to_string(),
            "Desc".to_string(),
        );
        epic.add_task(task);

        let scg = serialize_scg(&epic);
        let parsed = parse_scg(&scg).unwrap();

        assert_eq!(
            parsed.get_task("1").unwrap().title,
            "Task with \\ backslash"
        );
    }

    #[test]
    fn test_multiple_special_chars() {
        let mut epic = Phase::new("test".to_string());
        let task = Task::new(
            "1".to_string(),
            "Task with | pipe and \\ backslash".to_string(),
            "Line 1\nLine 2 with | and \\".to_string(),
        );
        epic.add_task(task);

        let scg = serialize_scg(&epic);
        let parsed = parse_scg(&scg).unwrap();

        let t = parsed.get_task("1").unwrap();
        assert_eq!(t.title, "Task with | pipe and \\ backslash");
        assert_eq!(t.description, "Line 1\nLine 2 with | and \\");
    }

    #[test]
    fn test_unicode_content() {
        let mut epic = Phase::new("unicode-test".to_string());
        let task = Task::new(
            "1".to_string(),
            "æ—¥æœ¬èªžã‚¿ã‚¤ãƒˆãƒ« ðŸš€ Ã‰mojis".to_string(),
            "æè¿° with Ã©mojis ðŸ˜€".to_string(),
        );
        epic.add_task(task);

        let scg = serialize_scg(&epic);
        let parsed = parse_scg(&scg).unwrap();

        let t = parsed.get_task("1").unwrap();
        assert_eq!(t.title, "æ—¥æœ¬èªžã‚¿ã‚¤ãƒˆãƒ« ðŸš€ Ã‰mojis");
        assert_eq!(t.description, "æè¿° with Ã©mojis ðŸ˜€");
    }

    #[test]
    fn test_empty_dependencies() {
        let mut epic = Phase::new("test".to_string());
        let task = Task::new("1".to_string(), "Task".to_string(), "Desc".to_string());
        epic.add_task(task);

        let scg = serialize_scg(&epic);
        let parsed = parse_scg(&scg).unwrap();

        assert!(parsed.get_task("1").unwrap().dependencies.is_empty());
    }

    #[test]
    fn test_multiple_dependencies() {
        let mut epic = Phase::new("test".to_string());

        let task1 = Task::new("1".to_string(), "Task 1".to_string(), String::new());
        let task2 = Task::new("2".to_string(), "Task 2".to_string(), String::new());
        let mut task3 = Task::new("3".to_string(), "Task 3".to_string(), String::new());
        task3.dependencies = vec!["1".to_string(), "2".to_string()];

        epic.add_task(task1);
        epic.add_task(task2);
        epic.add_task(task3);

        let scg = serialize_scg(&epic);
        let parsed = parse_scg(&scg).unwrap();

        let t3 = parsed.get_task("3").unwrap();
        assert_eq!(t3.dependencies.len(), 2);
        assert!(t3.dependencies.contains(&"1".to_string()));
        assert!(t3.dependencies.contains(&"2".to_string()));
    }

    #[test]
    fn test_complexity_boundary_values() {
        let mut epic = Phase::new("test".to_string());

        // Test all Fibonacci complexity values
        let complexities: Vec<u32> = vec![0, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89];
        for (i, c) in complexities.iter().enumerate() {
            let mut task = Task::new(
                format!("{}", i + 1),
                format!("Task {}", i + 1),
                String::new(),
            );
            task.complexity = *c;
            epic.add_task(task);
        }

        let scg = serialize_scg(&epic);
        let parsed = parse_scg(&scg).unwrap();

        for (i, expected) in complexities.iter().enumerate() {
            let task = parsed.get_task(&format!("{}", i + 1)).unwrap();
            assert_eq!(
                task.complexity,
                *expected,
                "Complexity mismatch for task {}",
                i + 1
            );
        }
    }

    #[test]
    fn test_long_description() {
        let mut epic = Phase::new("test".to_string());
        let long_desc = "A".repeat(5000); // Max description length
        let task = Task::new("1".to_string(), "Task".to_string(), long_desc.clone());
        epic.add_task(task);

        let scg = serialize_scg(&epic);
        let parsed = parse_scg(&scg).unwrap();

        assert_eq!(parsed.get_task("1").unwrap().description, long_desc);
    }

    #[test]
    fn test_empty_description() {
        let mut epic = Phase::new("test".to_string());
        let task = Task::new("1".to_string(), "Task".to_string(), String::new());
        epic.add_task(task);

        let scg = serialize_scg(&epic);
        let parsed = parse_scg(&scg).unwrap();

        assert_eq!(parsed.get_task("1").unwrap().description, "");
    }

    #[test]
    fn test_whitespace_handling() {
        // Ensure whitespace in titles is preserved
        let mut epic = Phase::new("test".to_string());
        let task = Task::new(
            "1".to_string(),
            "  Task with   spaces  ".to_string(),
            "Desc".to_string(),
        );
        epic.add_task(task);

        let scg = serialize_scg(&epic);
        let parsed = parse_scg(&scg).unwrap();

        // After round-trip, leading/trailing spaces in title should be trimmed
        // (this is expected behavior from split/trim)
        let t = parsed.get_task("1").unwrap();
        assert_eq!(t.title, "Task with   spaces");
    }

    #[test]
    fn test_nested_subtasks() {
        let mut epic = Phase::new("test".to_string());

        // Create hierarchy: 1 -> 1.1, 1.2 -> 1.2.1
        let mut parent = Task::new("1".to_string(), "Parent".to_string(), String::new());
        parent.status = TaskStatus::Expanded;
        parent.subtasks = vec!["1.1".to_string(), "1.2".to_string()];

        let mut child1 = Task::new("1.1".to_string(), "Child 1".to_string(), String::new());
        child1.parent_id = Some("1".to_string());

        let mut child2 = Task::new("1.2".to_string(), "Child 2".to_string(), String::new());
        child2.parent_id = Some("1".to_string());
        child2.status = TaskStatus::Expanded;
        child2.subtasks = vec!["1.2.1".to_string()];

        let mut grandchild =
            Task::new("1.2.1".to_string(), "Grandchild".to_string(), String::new());
        grandchild.parent_id = Some("1.2".to_string());

        epic.add_task(parent);
        epic.add_task(child1);
        epic.add_task(child2);
        epic.add_task(grandchild);

        let scg = serialize_scg(&epic);
        let parsed = parse_scg(&scg).unwrap();

        assert_eq!(parsed.tasks.len(), 4);

        let gc = parsed.get_task("1.2.1").unwrap();
        assert_eq!(gc.parent_id, Some("1.2".to_string()));

        let c2 = parsed.get_task("1.2").unwrap();
        assert!(c2.subtasks.contains(&"1.2.1".to_string()));
    }

    #[test]
    fn test_section_comment_lines_ignored() {
        // Manually create SCG with extra comment lines - they should be ignored
        let content = r#"# SCUD Graph v1
# Epic: test

@meta {
  name test
  # this is a comment
  updated 2025-01-01T00:00:00Z
}

@nodes
# id | title | status | complexity | priority
# another comment
1 | Task | P | 0 | M
"#;
        let epic = parse_scg(content).unwrap();
        assert_eq!(epic.tasks.len(), 1);
        assert_eq!(epic.get_task("1").unwrap().title, "Task");
    }
}
</file>

<file path="scud-cli/src/llm/client.rs">
use anyhow::{Context, Result};
use serde::{Deserialize, Serialize};
use std::env;
use std::path::PathBuf;

use crate::config::Config;
use crate::storage::Storage;

// Anthropic API structures
#[derive(Debug, Serialize)]
struct AnthropicRequest {
    model: String,
    max_tokens: u32,
    messages: Vec<AnthropicMessage>,
}

#[derive(Debug, Serialize)]
struct AnthropicMessage {
    role: String,
    content: String,
}

#[derive(Debug, Deserialize)]
struct AnthropicResponse {
    content: Vec<AnthropicContent>,
}

#[derive(Debug, Deserialize)]
struct AnthropicContent {
    text: String,
}

// OpenAI-compatible API structures (used by xAI, OpenAI, OpenRouter)
#[derive(Debug, Serialize)]
struct OpenAIRequest {
    model: String,
    max_tokens: u32,
    messages: Vec<OpenAIMessage>,
}

#[derive(Debug, Serialize)]
struct OpenAIMessage {
    role: String,
    content: String,
}

#[derive(Debug, Deserialize)]
struct OpenAIResponse {
    choices: Vec<OpenAIChoice>,
}

#[derive(Debug, Deserialize)]
struct OpenAIChoice {
    message: OpenAIMessageResponse,
}

#[derive(Debug, Deserialize)]
struct OpenAIMessageResponse {
    content: String,
}

pub struct LLMClient {
    config: Config,
    api_key: String,
    client: reqwest::Client,
}

impl LLMClient {
    pub fn new() -> Result<Self> {
        let storage = Storage::new(None);
        let config = storage.load_config()?;

        let api_key = if config.requires_api_key() {
            env::var(config.api_key_env_var()).with_context(|| {
                format!("{} environment variable not set", config.api_key_env_var())
            })?
        } else {
            String::new() // Claude CLI doesn't need API key
        };

        Ok(LLMClient {
            config,
            api_key,
            client: reqwest::Client::new(),
        })
    }

    pub fn new_with_project_root(project_root: PathBuf) -> Result<Self> {
        let storage = Storage::new(Some(project_root));
        let config = storage.load_config()?;

        let api_key = if config.requires_api_key() {
            env::var(config.api_key_env_var()).with_context(|| {
                format!("{} environment variable not set", config.api_key_env_var())
            })?
        } else {
            String::new() // Claude CLI doesn't need API key
        };

        Ok(LLMClient {
            config,
            api_key,
            client: reqwest::Client::new(),
        })
    }

    pub async fn complete(&self, prompt: &str) -> Result<String> {
        self.complete_with_model(prompt, None).await
    }

    pub async fn complete_with_model(
        &self,
        prompt: &str,
        model_override: Option<&str>,
    ) -> Result<String> {
        match self.config.llm.provider.as_str() {
            "claude-cli" => self.complete_claude_cli(prompt, model_override).await,
            "anthropic" => {
                self.complete_anthropic_with_model(prompt, model_override)
                    .await
            }
            "xai" | "openai" | "openrouter" => {
                self.complete_openai_compatible_with_model(prompt, model_override)
                    .await
            }
            _ => anyhow::bail!("Unsupported provider: {}", self.config.llm.provider),
        }
    }

    async fn complete_anthropic_with_model(
        &self,
        prompt: &str,
        model_override: Option<&str>,
    ) -> Result<String> {
        let model = model_override.unwrap_or(&self.config.llm.model);
        let request = AnthropicRequest {
            model: model.to_string(),
            max_tokens: self.config.llm.max_tokens,
            messages: vec![AnthropicMessage {
                role: "user".to_string(),
                content: prompt.to_string(),
            }],
        };

        let response = self
            .client
            .post(self.config.api_endpoint())
            .header("x-api-key", &self.api_key)
            .header("anthropic-version", "2023-06-01")
            .header("content-type", "application/json")
            .json(&request)
            .send()
            .await
            .context("Failed to send request to Anthropic API")?;

        if !response.status().is_success() {
            let status = response.status();
            let error_text = response.text().await.unwrap_or_default();
            anyhow::bail!("Anthropic API error ({}): {}", status, error_text);
        }

        let api_response: AnthropicResponse = response
            .json()
            .await
            .context("Failed to parse Anthropic API response")?;

        Ok(api_response
            .content
            .first()
            .map(|c| c.text.clone())
            .unwrap_or_default())
    }

    async fn complete_openai_compatible_with_model(
        &self,
        prompt: &str,
        model_override: Option<&str>,
    ) -> Result<String> {
        let model = model_override.unwrap_or(&self.config.llm.model);
        let request = OpenAIRequest {
            model: model.to_string(),
            max_tokens: self.config.llm.max_tokens,
            messages: vec![OpenAIMessage {
                role: "user".to_string(),
                content: prompt.to_string(),
            }],
        };

        let mut request_builder = self
            .client
            .post(self.config.api_endpoint())
            .header("authorization", format!("Bearer {}", self.api_key))
            .header("content-type", "application/json");

        // OpenRouter requires additional headers
        if self.config.llm.provider == "openrouter" {
            request_builder = request_builder
                .header("HTTP-Referer", "https://github.com/scud-cli")
                .header("X-Title", "SCUD Task Master");
        }

        let response = request_builder
            .json(&request)
            .send()
            .await
            .with_context(|| {
                format!("Failed to send request to {} API", self.config.llm.provider)
            })?;

        if !response.status().is_success() {
            let status = response.status();
            let error_text = response.text().await.unwrap_or_default();
            anyhow::bail!(
                "{} API error ({}): {}",
                self.config.llm.provider,
                status,
                error_text
            );
        }

        let api_response: OpenAIResponse = response.json().await.with_context(|| {
            format!("Failed to parse {} API response", self.config.llm.provider)
        })?;

        Ok(api_response
            .choices
            .first()
            .map(|c| c.message.content.clone())
            .unwrap_or_default())
    }

    pub async fn complete_json<T>(&self, prompt: &str) -> Result<T>
    where
        T: serde::de::DeserializeOwned,
    {
        let response_text = self.complete(prompt).await?;

        // Try to find JSON in the response (LLM might include markdown or explanations)
        let json_str = Self::extract_json(&response_text);

        serde_json::from_str(json_str).with_context(|| {
            // Provide helpful error context
            let preview = if json_str.len() > 500 {
                format!("{}...", &json_str[..500])
            } else {
                json_str.to_string()
            };
            format!(
                "Failed to parse JSON from LLM response. Response preview:\n{}",
                preview
            )
        })
    }

    /// Extract JSON from LLM response, handling markdown code blocks and extra text
    fn extract_json(response: &str) -> &str {
        // First, try to extract from markdown code blocks
        if let Some(start) = response.find("```json") {
            let content_start = start + 7; // Skip "```json"
            if let Some(end) = response[content_start..].find("```") {
                return response[content_start..content_start + end].trim();
            }
        }

        // Try plain code blocks
        if let Some(start) = response.find("```") {
            let content_start = start + 3;
            // Skip language identifier if present (e.g., "```\n")
            let content_start = response[content_start..]
                .find('\n')
                .map(|i| content_start + i + 1)
                .unwrap_or(content_start);
            if let Some(end) = response[content_start..].find("```") {
                return response[content_start..content_start + end].trim();
            }
        }

        // Try to find array JSON
        if let Some(start) = response.find('[') {
            if let Some(end) = response.rfind(']') {
                if end > start {
                    return &response[start..=end];
                }
            }
        }

        // Try to find object JSON
        if let Some(start) = response.find('{') {
            if let Some(end) = response.rfind('}') {
                if end > start {
                    return &response[start..=end];
                }
            }
        }

        response.trim()
    }

    async fn complete_claude_cli(
        &self,
        prompt: &str,
        model_override: Option<&str>,
    ) -> Result<String> {
        use std::process::Stdio;
        use tokio::io::AsyncWriteExt;
        use tokio::process::Command;

        let model = model_override.unwrap_or(&self.config.llm.model);

        // Build the claude command
        let mut cmd = Command::new("claude");
        cmd.arg("-p") // Print mode (headless)
            .arg("--output-format")
            .arg("json")
            .arg("--model")
            .arg(model)
            .stdin(Stdio::piped())
            .stdout(Stdio::piped())
            .stderr(Stdio::piped());

        // Spawn the process
        let mut child = cmd.spawn().context("Failed to spawn 'claude' command. Make sure Claude Code is installed and 'claude' is in your PATH")?;

        // Write prompt to stdin
        if let Some(mut stdin) = child.stdin.take() {
            stdin
                .write_all(prompt.as_bytes())
                .await
                .context("Failed to write prompt to claude stdin")?;
            drop(stdin); // Close stdin
        }

        // Wait for completion
        let output = child
            .wait_with_output()
            .await
            .context("Failed to wait for claude command")?;

        if !output.status.success() {
            let stderr = String::from_utf8_lossy(&output.stderr);
            anyhow::bail!("Claude CLI error: {}", stderr);
        }

        // Parse JSON output
        let stdout =
            String::from_utf8(output.stdout).context("Claude CLI output is not valid UTF-8")?;

        #[derive(Deserialize)]
        struct ClaudeCliResponse {
            result: String,
        }

        let response: ClaudeCliResponse =
            serde_json::from_str(&stdout).context("Failed to parse Claude CLI JSON response")?;

        Ok(response.result)
    }
}
</file>

<file path="scud-cli/src/llm/mod.rs">
pub mod client;
pub mod prompts;

pub use client::LLMClient;
pub use prompts::Prompts;
</file>

<file path="scud-cli/src/llm/prompts.rs">
pub struct Prompts;

impl Prompts {
    pub fn parse_prd(phase_content: &str, num_tasks: u32) -> String {
        format!(
            r#"You are a Scrum Master parsing a phase into actionable development tasks.

Phase Content:
{}

Parse this phase into approximately {} discrete, actionable tasks. Return a JSON array of tasks with the following structure:

[
  {{
    "title": "Task name (concise, action-oriented)",
    "description": "What needs to be done (2-3 sentences)",
    "priority": "high|medium|low",
    "complexity": <1|2|3|5|8|13|21>,
    "dependencies": []
  }}
]

Guidelines:
- Generate approximately {} tasks (can vary by 1-2 if needed for logical breakdown)
- Each task should be atomic and independently testable
- Use Fibonacci complexity scale:
  * 1 = Trivial (~30 min, e.g., update config value)
  * 2 = Simple (30m-1h, e.g., add basic validation)
  * 3 = Moderate (1-2h, e.g., create new API endpoint)
  * 5 = Complex (2-4h, e.g., integrate third-party service)
  * 8 = Very Complex (4-8h, e.g., build feature with multiple components)
  * 13 = Extremely Complex (1 day, SHOULD BE SPLIT)
  * 21 = Too Large (MUST BE SPLIT - only use if absolutely necessary)
- Identify dependencies where tasks must be done in specific order (use task indices, e.g., ["1", "2"])
- Order tasks logically (foundational work first)
- Each task should have clear success criteria

Return ONLY the JSON array, no additional explanation."#,
            phase_content, num_tasks, num_tasks
        )
    }

    pub fn analyze_complexity(
        task_title: &str,
        task_description: &str,
        existing_details: Option<&str>,
    ) -> String {
        let context = existing_details
            .map(|d| format!("\nExisting Technical Details:\n{}\n", d))
            .unwrap_or_default();

        format!(
            r#"You are analyzing the complexity of a development task.

Task: {}
Description: {}{}

Analyze this task and provide:
1. A complexity score (1, 2, 3, 5, 8, 13, or 21) using Fibonacci scale
2. A brief reasoning explaining the score

Consider:
- Technical difficulty and unknowns
- Number of components/files affected
- Testing requirements
- Integration points and dependencies
- Research needed
- Edge cases to handle

Complexity Scale:
- 1 = Trivial (~30 min)
- 2 = Simple (30m-1h)
- 3 = Moderate (1-2h)
- 5 = Complex (2-4h)
- 8 = Very Complex (4-8h)
- 13 = Extremely Complex (1 day) - Should be split
- 21 = Too Large - Must be split

Return a JSON object:
{{
  "complexity": <number>,
  "reasoning": "explanation of the score"
}}

Return ONLY the JSON object, no additional explanation."#,
            task_title, task_description, context
        )
    }

    pub fn expand_task(
        task_title: &str,
        task_description: &str,
        complexity: u32,
        existing_details: Option<&str>,
        recommended_subtasks: usize,
    ) -> String {
        let context = existing_details
            .map(|d| format!("\nExisting Technical Details:\n{}\n", d))
            .unwrap_or_default();

        format!(
            r#"You are breaking down a development task into smaller, manageable subtasks.

Original Task (Complexity {}): {}
Description: {}{}

Break this task down into approximately {} subtasks based on its complexity.

Create subtasks that:
- Are small, focused, and independently completable
- Are independently testable
- Have clear dependencies between them
- Cover all aspects of the original task
- Maintain logical order

Return a JSON array of subtasks:
[
  {{
    "title": "Subtask name",
    "description": "What needs to be done",
    "priority": "high|medium|low",
    "dependencies": []  // Array of strings: ["1", "2", "3"] for subtask dependencies, or ["TASK-123"] for external dependencies
  }}
]

Guidelines:
- Start with foundational work (models, schemas)
- Then build core logic
- Then add UI/API layers
- Finally add tests and documentation
- Each subtask should be independently completable
- Use dependencies to enforce correct order (e.g., ["1"] means depends on first subtask)
- Dependency values MUST be strings, not numbers
- Aim for {} subtasks total (can vary by 1-2 if needed for logical breakdown)
- DO NOT include "complexity" field - subtasks are all assumed to be small and manageable

Return ONLY the JSON array, no additional explanation."#,
            complexity,
            task_title,
            task_description,
            context,
            recommended_subtasks,
            recommended_subtasks
        )
    }

    pub fn reanalyze_dependencies(task_context: &str, phases: &[String]) -> String {
        format!(
            r#"You are analyzing a software project's task dependencies across multiple phases.

## Current Task State

{task_context}

## Your Task

Review the tasks above and suggest dependency changes that would improve execution order. Consider:

1. **Logical ordering**: Tasks that produce artifacts another task needs
2. **Current completion state**: Don't add deps on PENDING tasks for DONE tasks
3. **Cross-phase dependencies**: Tasks in one phase that should wait for tasks in another
4. **Remove redundant deps**: If A depends on B, and B depends on C, A doesn't also need C
5. **Missing dependencies**: If a task clearly requires output from another task, add the dependency

## Rules

- Use full task IDs with phase prefix (e.g., "auth:1", "api:3")
- Only suggest changes for tasks that are PENDING or IN PROGRESS
- Don't modify DONE, EXPANDED, or SKIPPED tasks
- Consider that some tasks may intentionally have no dependencies
- Be conservative - only suggest changes that are clearly needed

## Response Format

Return a JSON array of suggestions:
```json
[
  {{
    "task_id": "api:3",
    "add_dependencies": ["auth:1", "core:2"],
    "remove_dependencies": [],
    "reasoning": "API endpoints need authentication service and core models"
  }}
]
```

Return empty array [] if no changes are needed.

Phases to analyze: {phases:?}
"#,
            task_context = task_context,
            phases = phases
        )
    }
}
</file>

<file path="scud-cli/src/models/mod.rs">
pub mod phase;
pub mod task;

pub use phase::Phase;
pub use task::{Priority, Task, TaskStatus};
</file>

<file path="scud-cli/src/models/phase.rs">
use super::task::Task;
use serde::{Deserialize, Serialize};

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Phase {
    pub name: String,
    pub tasks: Vec<Task>,
}

impl Phase {
    pub fn new(name: String) -> Self {
        Phase {
            name,
            tasks: Vec::new(),
        }
    }

    pub fn add_task(&mut self, task: Task) {
        self.tasks.push(task);
    }

    pub fn get_task(&self, task_id: &str) -> Option<&Task> {
        self.tasks.iter().find(|t| t.id == task_id)
    }

    pub fn get_task_mut(&mut self, task_id: &str) -> Option<&mut Task> {
        self.tasks.iter_mut().find(|t| t.id == task_id)
    }

    pub fn remove_task(&mut self, task_id: &str) -> Option<Task> {
        self.tasks
            .iter()
            .position(|t| t.id == task_id)
            .map(|idx| self.tasks.remove(idx))
    }

    pub fn get_stats(&self) -> PhaseStats {
        let mut total = 0;
        let mut pending = 0;
        let mut in_progress = 0;
        let mut done = 0;
        let mut blocked = 0;
        let mut expanded = 0;
        let mut total_complexity = 0;

        for task in &self.tasks {
            // Don't count subtasks in total (they're part of parent)
            if task.is_subtask() {
                continue;
            }

            total += 1;

            // Only count complexity for non-expanded tasks
            // (expanded tasks have their work represented by subtasks)
            if !task.is_expanded() {
                total_complexity += task.complexity;
            }

            match task.status {
                super::task::TaskStatus::Pending => pending += 1,
                super::task::TaskStatus::InProgress => in_progress += 1,
                super::task::TaskStatus::Done => done += 1,
                super::task::TaskStatus::Blocked => blocked += 1,
                super::task::TaskStatus::Expanded => expanded += 1,
                _ => {}
            }
        }

        PhaseStats {
            total,
            pending,
            in_progress,
            done,
            blocked,
            expanded,
            total_complexity,
        }
    }

    /// Get actionable tasks (not expanded, not subtasks of incomplete parents)
    pub fn get_actionable_tasks(&self) -> Vec<&Task> {
        self.tasks
            .iter()
            .filter(|t| {
                // Exclude expanded parents (work on subtasks instead)
                if t.is_expanded() {
                    return false;
                }
                // Include subtasks only if they're actionable
                if let Some(ref parent_id) = t.parent_id {
                    // Parent must be expanded
                    self.get_task(parent_id)
                        .map(|p| p.is_expanded())
                        .unwrap_or(false)
                } else {
                    // Top-level task
                    true
                }
            })
            .collect()
    }

    /// Find next task using only local phase tasks for dependency checking.
    /// For cross-tag dependency support, use find_next_task_cross_tag instead.
    pub fn find_next_task(&self) -> Option<&Task> {
        self.tasks.iter().find(|task| {
            task.status == super::task::TaskStatus::Pending
                && task.has_dependencies_met(&self.tasks)
        })
    }

    /// Find next task with cross-tag dependency support.
    /// Pass flattened tasks from all phases for proper dependency resolution.
    pub fn find_next_task_cross_tag<'a>(&'a self, all_tasks: &[&Task]) -> Option<&'a Task> {
        self.tasks.iter().find(|task| {
            task.status == super::task::TaskStatus::Pending
                && task.has_dependencies_met_refs(all_tasks)
        })
    }

    pub fn get_tasks_needing_expansion(&self) -> Vec<&Task> {
        self.tasks.iter().filter(|t| t.needs_expansion()).collect()
    }
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct PhaseStats {
    pub total: usize,
    pub pending: usize,
    pub in_progress: usize,
    pub done: usize,
    pub blocked: usize,
    pub expanded: usize,
    pub total_complexity: u32,
}

#[cfg(test)]
mod tests {
    use super::*;
    use crate::models::task::{Task, TaskStatus};

    #[test]
    fn test_phase_creation() {
        let phase = Phase::new("phase-1-auth".to_string());

        assert_eq!(phase.name, "phase-1-auth");
        assert!(phase.tasks.is_empty());
    }

    #[test]
    fn test_add_task() {
        let mut phase = Phase::new("phase-1".to_string());
        let task = Task::new(
            "TASK-1".to_string(),
            "Test Task".to_string(),
            "Description".to_string(),
        );

        phase.add_task(task.clone());

        assert_eq!(phase.tasks.len(), 1);
        assert_eq!(phase.tasks[0].id, "TASK-1");
    }

    #[test]
    fn test_get_task() {
        let mut phase = Phase::new("phase-1".to_string());
        let task = Task::new(
            "TASK-1".to_string(),
            "Test Task".to_string(),
            "Description".to_string(),
        );
        phase.add_task(task);

        let retrieved = phase.get_task("TASK-1");
        assert!(retrieved.is_some());
        assert_eq!(retrieved.unwrap().id, "TASK-1");

        let missing = phase.get_task("TASK-99");
        assert!(missing.is_none());
    }

    #[test]
    fn test_get_task_mut() {
        let mut phase = Phase::new("phase-1".to_string());
        let task = Task::new(
            "TASK-1".to_string(),
            "Test Task".to_string(),
            "Description".to_string(),
        );
        phase.add_task(task);

        {
            let task_mut = phase.get_task_mut("TASK-1").unwrap();
            task_mut.set_status(TaskStatus::InProgress);
        }

        assert_eq!(
            phase.get_task("TASK-1").unwrap().status,
            TaskStatus::InProgress
        );
    }

    #[test]
    fn test_remove_task() {
        let mut phase = Phase::new("phase-1".to_string());
        let task1 = Task::new(
            "TASK-1".to_string(),
            "Task 1".to_string(),
            "Desc".to_string(),
        );
        let task2 = Task::new(
            "TASK-2".to_string(),
            "Task 2".to_string(),
            "Desc".to_string(),
        );
        phase.add_task(task1);
        phase.add_task(task2);

        let removed = phase.remove_task("TASK-1");
        assert!(removed.is_some());
        assert_eq!(removed.unwrap().id, "TASK-1");
        assert_eq!(phase.tasks.len(), 1);
        assert_eq!(phase.tasks[0].id, "TASK-2");

        let missing = phase.remove_task("TASK-99");
        assert!(missing.is_none());
    }

    #[test]
    fn test_get_stats_empty_phase() {
        let phase = Phase::new("phase-1".to_string());
        let stats = phase.get_stats();

        assert_eq!(stats.total, 0);
        assert_eq!(stats.pending, 0);
        assert_eq!(stats.in_progress, 0);
        assert_eq!(stats.done, 0);
        assert_eq!(stats.blocked, 0);
        assert_eq!(stats.total_complexity, 0);
    }

    #[test]
    fn test_get_stats_with_tasks() {
        let mut phase = Phase::new("phase-1".to_string());

        let mut task1 = Task::new(
            "TASK-1".to_string(),
            "Task 1".to_string(),
            "Desc".to_string(),
        );
        task1.complexity = 3;
        task1.set_status(TaskStatus::Done);

        let mut task2 = Task::new(
            "TASK-2".to_string(),
            "Task 2".to_string(),
            "Desc".to_string(),
        );
        task2.complexity = 5;
        task2.set_status(TaskStatus::InProgress);

        let mut task3 = Task::new(
            "TASK-3".to_string(),
            "Task 3".to_string(),
            "Desc".to_string(),
        );
        task3.complexity = 8;
        // Pending by default

        let mut task4 = Task::new(
            "TASK-4".to_string(),
            "Task 4".to_string(),
            "Desc".to_string(),
        );
        task4.complexity = 2;
        task4.set_status(TaskStatus::Blocked);

        phase.add_task(task1);
        phase.add_task(task2);
        phase.add_task(task3);
        phase.add_task(task4);

        let stats = phase.get_stats();

        assert_eq!(stats.total, 4);
        assert_eq!(stats.pending, 1);
        assert_eq!(stats.in_progress, 1);
        assert_eq!(stats.done, 1);
        assert_eq!(stats.blocked, 1);
        assert_eq!(stats.total_complexity, 18); // 3 + 5 + 8 + 2
    }

    #[test]
    fn test_find_next_task_no_dependencies() {
        let mut phase = Phase::new("phase-1".to_string());

        let mut task1 = Task::new(
            "TASK-1".to_string(),
            "Task 1".to_string(),
            "Desc".to_string(),
        );
        task1.set_status(TaskStatus::Done);

        let task2 = Task::new(
            "TASK-2".to_string(),
            "Task 2".to_string(),
            "Desc".to_string(),
        );
        // Pending, no dependencies

        let task3 = Task::new(
            "TASK-3".to_string(),
            "Task 3".to_string(),
            "Desc".to_string(),
        );
        // Pending, no dependencies

        phase.add_task(task1);
        phase.add_task(task2);
        phase.add_task(task3);

        let next = phase.find_next_task();
        assert!(next.is_some());
        assert_eq!(next.unwrap().id, "TASK-2"); // First pending task
    }

    #[test]
    fn test_find_next_task_with_dependencies() {
        let mut phase = Phase::new("phase-1".to_string());

        let mut task1 = Task::new(
            "TASK-1".to_string(),
            "Task 1".to_string(),
            "Desc".to_string(),
        );
        task1.set_status(TaskStatus::Done);

        let task2 = Task::new(
            "TASK-2".to_string(),
            "Task 2".to_string(),
            "Desc".to_string(),
        );
        // Pending, no dependencies

        let mut task3 = Task::new(
            "TASK-3".to_string(),
            "Task 3".to_string(),
            "Desc".to_string(),
        );
        task3.dependencies = vec!["TASK-1".to_string(), "TASK-2".to_string()];
        // Pending, but depends on TASK-2 which is not done

        phase.add_task(task1);
        phase.add_task(task2);
        phase.add_task(task3);

        let next = phase.find_next_task();
        assert!(next.is_some());
        assert_eq!(next.unwrap().id, "TASK-2"); // TASK-3 blocked by dependencies
    }

    #[test]
    fn test_find_next_task_dependencies_met() {
        let mut phase = Phase::new("phase-1".to_string());

        let mut task1 = Task::new(
            "TASK-1".to_string(),
            "Task 1".to_string(),
            "Desc".to_string(),
        );
        task1.set_status(TaskStatus::Done);

        let mut task2 = Task::new(
            "TASK-2".to_string(),
            "Task 2".to_string(),
            "Desc".to_string(),
        );
        task2.set_status(TaskStatus::Done);

        let mut task3 = Task::new(
            "TASK-3".to_string(),
            "Task 3".to_string(),
            "Desc".to_string(),
        );
        task3.dependencies = vec!["TASK-1".to_string(), "TASK-2".to_string()];
        // Pending, dependencies met

        phase.add_task(task1);
        phase.add_task(task2);
        phase.add_task(task3);

        let next = phase.find_next_task();
        assert!(next.is_some());
        assert_eq!(next.unwrap().id, "TASK-3"); // Dependencies met
    }

    #[test]
    fn test_find_next_task_none_available() {
        let mut phase = Phase::new("phase-1".to_string());

        let mut task1 = Task::new(
            "TASK-1".to_string(),
            "Task 1".to_string(),
            "Desc".to_string(),
        );
        task1.set_status(TaskStatus::Done);

        let mut task2 = Task::new(
            "TASK-2".to_string(),
            "Task 2".to_string(),
            "Desc".to_string(),
        );
        task2.set_status(TaskStatus::InProgress);

        phase.add_task(task1);
        phase.add_task(task2);

        let next = phase.find_next_task();
        assert!(next.is_none()); // No pending tasks
    }

    #[test]
    fn test_get_tasks_needing_expansion() {
        let mut phase = Phase::new("phase-1".to_string());

        let mut task1 = Task::new(
            "TASK-1".to_string(),
            "Small Task".to_string(),
            "Desc".to_string(),
        );
        task1.complexity = 5;

        let mut task2 = Task::new(
            "TASK-2".to_string(),
            "Medium Task".to_string(),
            "Desc".to_string(),
        );
        task2.complexity = 13;

        let mut task3 = Task::new(
            "TASK-3".to_string(),
            "Large Task".to_string(),
            "Desc".to_string(),
        );
        task3.complexity = 21;

        let mut task4 = Task::new(
            "TASK-4".to_string(),
            "Huge Task".to_string(),
            "Desc".to_string(),
        );
        task4.complexity = 34;

        phase.add_task(task1);
        phase.add_task(task2);
        phase.add_task(task3);
        phase.add_task(task4);

        let needing_expansion = phase.get_tasks_needing_expansion();

        assert_eq!(needing_expansion.len(), 4); // All tasks with complexity >= 3
        assert!(needing_expansion.iter().any(|t| t.id == "TASK-1"));
        assert!(needing_expansion.iter().any(|t| t.id == "TASK-2"));
        assert!(needing_expansion.iter().any(|t| t.id == "TASK-3"));
        assert!(needing_expansion.iter().any(|t| t.id == "TASK-4"));
    }

    #[test]
    fn test_phase_serialization() {
        let mut phase = Phase::new("phase-1".to_string());
        let task = Task::new(
            "TASK-1".to_string(),
            "Test Task".to_string(),
            "Description".to_string(),
        );
        phase.add_task(task);

        let json = serde_json::to_string(&phase).unwrap();
        let deserialized: Phase = serde_json::from_str(&json).unwrap();

        assert_eq!(phase.name, deserialized.name);
        assert_eq!(phase.tasks.len(), deserialized.tasks.len());
        assert_eq!(phase.tasks[0].id, deserialized.tasks[0].id);
    }
}
</file>

<file path="scud-cli/src/models/task.rs">
use serde::{Deserialize, Serialize};

#[derive(Debug, Clone, Serialize, Deserialize, PartialEq, Default)]
#[serde(rename_all = "kebab-case")]
pub enum TaskStatus {
    #[default]
    Pending,
    InProgress,
    Done,
    Review,
    Blocked,
    Deferred,
    Cancelled,
    Expanded, // Task has been broken into subtasks
}

impl TaskStatus {
    pub fn as_str(&self) -> &'static str {
        match self {
            TaskStatus::Pending => "pending",
            TaskStatus::InProgress => "in-progress",
            TaskStatus::Done => "done",
            TaskStatus::Review => "review",
            TaskStatus::Blocked => "blocked",
            TaskStatus::Deferred => "deferred",
            TaskStatus::Cancelled => "cancelled",
            TaskStatus::Expanded => "expanded",
        }
    }

    #[allow(clippy::should_implement_trait)]
    pub fn from_str(s: &str) -> Option<Self> {
        match s {
            "pending" => Some(TaskStatus::Pending),
            "in-progress" => Some(TaskStatus::InProgress),
            "done" => Some(TaskStatus::Done),
            "review" => Some(TaskStatus::Review),
            "blocked" => Some(TaskStatus::Blocked),
            "deferred" => Some(TaskStatus::Deferred),
            "cancelled" => Some(TaskStatus::Cancelled),
            "expanded" => Some(TaskStatus::Expanded),
            _ => None,
        }
    }

    pub fn all() -> Vec<&'static str> {
        vec![
            "pending",
            "in-progress",
            "done",
            "review",
            "blocked",
            "deferred",
            "cancelled",
            "expanded",
        ]
    }
}

#[derive(Debug, Clone, Serialize, Deserialize, PartialEq, Default)]
#[serde(rename_all = "lowercase")]
pub enum Priority {
    Critical,
    High,
    #[default]
    Medium,
    Low,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Task {
    pub id: String,
    pub title: String,
    pub description: String,

    #[serde(default)]
    pub status: TaskStatus,

    #[serde(default)]
    pub complexity: u32,

    #[serde(default)]
    pub priority: Priority,

    #[serde(default)]
    pub dependencies: Vec<String>,

    // Parent-child relationship for expanded tasks
    #[serde(skip_serializing_if = "Option::is_none")]
    pub parent_id: Option<String>,

    #[serde(default, skip_serializing_if = "Vec::is_empty")]
    pub subtasks: Vec<String>,

    #[serde(skip_serializing_if = "Option::is_none")]
    pub details: Option<String>,

    #[serde(skip_serializing_if = "Option::is_none")]
    pub test_strategy: Option<String>,

    #[serde(skip_serializing_if = "Option::is_none")]
    pub created_at: Option<String>,

    #[serde(skip_serializing_if = "Option::is_none")]
    pub updated_at: Option<String>,

    // Parallel execution support
    #[serde(skip_serializing_if = "Option::is_none")]
    pub assigned_to: Option<String>,

    #[serde(skip_serializing_if = "Option::is_none")]
    pub locked_by: Option<String>,

    #[serde(skip_serializing_if = "Option::is_none")]
    pub locked_at: Option<String>,
}

impl Task {
    // Validation constants
    const MAX_TITLE_LENGTH: usize = 200;
    const MAX_DESCRIPTION_LENGTH: usize = 5000;
    const VALID_FIBONACCI_NUMBERS: &'static [u32] = &[0, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89];

    /// ID separator for namespaced IDs (epic:local_id)
    pub const ID_SEPARATOR: char = ':';

    pub fn new(id: String, title: String, description: String) -> Self {
        let now = chrono::Utc::now().to_rfc3339();
        Task {
            id,
            title,
            description,
            status: TaskStatus::Pending,
            complexity: 0,
            priority: Priority::Medium,
            dependencies: Vec::new(),
            parent_id: None,
            subtasks: Vec::new(),
            details: None,
            test_strategy: None,
            created_at: Some(now.clone()),
            updated_at: Some(now),
            assigned_to: None,
            locked_by: None,
            locked_at: None,
        }
    }

    /// Parse a task ID into (epic_tag, local_id) parts
    /// e.g., "phase1:10.1" -> Some(("phase1", "10.1"))
    /// e.g., "10.1" -> None (legacy format)
    pub fn parse_id(id: &str) -> Option<(&str, &str)> {
        id.split_once(Self::ID_SEPARATOR)
    }

    /// Create a namespaced task ID
    pub fn make_id(epic_tag: &str, local_id: &str) -> String {
        format!("{}{}{}", epic_tag, Self::ID_SEPARATOR, local_id)
    }

    /// Get the local ID part (without epic prefix)
    pub fn local_id(&self) -> &str {
        Self::parse_id(&self.id)
            .map(|(_, local)| local)
            .unwrap_or(&self.id)
    }

    /// Get the epic tag from a namespaced ID
    pub fn epic_tag(&self) -> Option<&str> {
        Self::parse_id(&self.id).map(|(tag, _)| tag)
    }

    /// Check if this is a subtask (has parent)
    pub fn is_subtask(&self) -> bool {
        self.parent_id.is_some()
    }

    /// Check if this task has been expanded into subtasks
    pub fn is_expanded(&self) -> bool {
        self.status == TaskStatus::Expanded || !self.subtasks.is_empty()
    }

    /// Validate task ID - must contain only alphanumeric characters, hyphens, underscores,
    /// colons (for namespacing), and dots (for subtask IDs)
    pub fn validate_id(id: &str) -> Result<(), String> {
        if id.is_empty() {
            return Err("Task ID cannot be empty".to_string());
        }

        if id.len() > 100 {
            return Err("Task ID too long (max 100 characters)".to_string());
        }

        // Allow alphanumeric, hyphen, underscore, colon (namespacing), and dot (subtask IDs)
        let valid_chars = id
            .chars()
            .all(|c| c.is_ascii_alphanumeric() || c == '-' || c == '_' || c == ':' || c == '.');

        if !valid_chars {
            return Err(
                "Task ID can only contain alphanumeric characters, hyphens, underscores, colons, and dots"
                    .to_string(),
            );
        }

        Ok(())
    }

    /// Validate title - must not be empty and within length limit
    pub fn validate_title(title: &str) -> Result<(), String> {
        if title.trim().is_empty() {
            return Err("Task title cannot be empty".to_string());
        }

        if title.len() > Self::MAX_TITLE_LENGTH {
            return Err(format!(
                "Task title too long (max {} characters)",
                Self::MAX_TITLE_LENGTH
            ));
        }

        Ok(())
    }

    /// Validate description - within length limit
    pub fn validate_description(description: &str) -> Result<(), String> {
        if description.len() > Self::MAX_DESCRIPTION_LENGTH {
            return Err(format!(
                "Task description too long (max {} characters)",
                Self::MAX_DESCRIPTION_LENGTH
            ));
        }

        Ok(())
    }

    /// Validate complexity - must be a Fibonacci number
    pub fn validate_complexity(complexity: u32) -> Result<(), String> {
        if !Self::VALID_FIBONACCI_NUMBERS.contains(&complexity) {
            return Err(format!(
                "Complexity must be a Fibonacci number: {:?}",
                Self::VALID_FIBONACCI_NUMBERS
            ));
        }

        Ok(())
    }

    /// Sanitize text by removing potentially dangerous HTML/script tags
    pub fn sanitize_text(text: &str) -> String {
        text.replace('<', "&lt;")
            .replace('>', "&gt;")
            .replace('"', "&quot;")
            .replace('\'', "&#x27;")
    }

    /// Comprehensive validation of all task fields
    pub fn validate(&self) -> Result<(), Vec<String>> {
        let mut errors = Vec::new();

        if let Err(e) = Self::validate_id(&self.id) {
            errors.push(e);
        }

        if let Err(e) = Self::validate_title(&self.title) {
            errors.push(e);
        }

        if let Err(e) = Self::validate_description(&self.description) {
            errors.push(e);
        }

        if self.complexity > 0 {
            if let Err(e) = Self::validate_complexity(self.complexity) {
                errors.push(e);
            }
        }

        if errors.is_empty() {
            Ok(())
        } else {
            Err(errors)
        }
    }

    pub fn set_status(&mut self, status: TaskStatus) {
        self.status = status;
        self.updated_at = Some(chrono::Utc::now().to_rfc3339());
    }

    pub fn update(&mut self) {
        self.updated_at = Some(chrono::Utc::now().to_rfc3339());
    }

    pub fn has_dependencies_met(&self, all_tasks: &[Task]) -> bool {
        self.dependencies.iter().all(|dep_id| {
            all_tasks
                .iter()
                .find(|t| &t.id == dep_id)
                .map(|t| t.status == TaskStatus::Done)
                .unwrap_or(false)
        })
    }

    /// Check if all dependencies are met, searching across provided task references
    /// Supports cross-tag dependencies when passed tasks from all phases
    pub fn has_dependencies_met_refs(&self, all_tasks: &[&Task]) -> bool {
        self.dependencies.iter().all(|dep_id| {
            all_tasks
                .iter()
                .find(|t| &t.id == dep_id)
                .map(|t| t.status == TaskStatus::Done)
                .unwrap_or(false)
        })
    }

    /// Returns whether this task should be expanded into subtasks
    /// All tasks with complexity >= 3 can benefit from expansion
    /// Subtasks and already-expanded tasks don't need expansion
    pub fn needs_expansion(&self) -> bool {
        self.complexity >= 3 && !self.is_expanded() && !self.is_subtask()
    }

    /// Returns the recommended number of subtasks based on complexity
    /// Complexity 1-2: 2 subtasks
    /// Complexity 3: 2-3 subtasks
    /// Complexity 5: 3-4 subtasks
    /// Complexity 8: 4-5 subtasks
    /// Complexity 13: 5-6 subtasks
    /// Complexity 21+: 6-8 subtasks
    pub fn recommended_subtasks(&self) -> usize {
        Self::recommended_subtasks_for_complexity(self.complexity)
    }

    /// Static version for use when we only have complexity value
    pub fn recommended_subtasks_for_complexity(complexity: u32) -> usize {
        match complexity {
            0..=2 => 2,
            3 => 3,
            5 => 4,
            8 => 5,
            13 => 6,
            _ => 8, // 21+
        }
    }

    // Assignment and locking methods
    pub fn assign(&mut self, assignee: &str) {
        self.assigned_to = Some(assignee.to_string());
        self.update();
    }

    pub fn claim(&mut self, assignee: &str) -> Result<(), String> {
        if let Some(ref locked_by) = self.locked_by {
            if locked_by != assignee {
                return Err(format!("Task is locked by {}", locked_by));
            }
        }

        self.assigned_to = Some(assignee.to_string());
        self.locked_by = Some(assignee.to_string());
        self.locked_at = Some(chrono::Utc::now().to_rfc3339());
        self.update();
        Ok(())
    }

    pub fn release(&mut self) {
        self.locked_by = None;
        self.locked_at = None;
        self.update();
    }

    pub fn is_locked(&self) -> bool {
        self.locked_by.is_some()
    }

    pub fn is_locked_by(&self, assignee: &str) -> bool {
        self.locked_by
            .as_ref()
            .map(|s| s == assignee)
            .unwrap_or(false)
    }

    pub fn is_assigned_to(&self, assignee: &str) -> bool {
        self.assigned_to
            .as_ref()
            .map(|s| s == assignee)
            .unwrap_or(false)
    }

    pub fn lock_age_hours(&self) -> Option<f64> {
        self.locked_at.as_ref().and_then(|locked_at| {
            chrono::DateTime::parse_from_rfc3339(locked_at)
                .ok()
                .map(|dt| {
                    let now = chrono::Utc::now();
                    let duration = now.signed_duration_since(dt);
                    duration.num_seconds() as f64 / 3600.0
                })
        })
    }

    pub fn is_stale_lock(&self, hours_threshold: f64) -> bool {
        self.lock_age_hours()
            .map(|hours| hours > hours_threshold)
            .unwrap_or(false)
    }

    /// Check if adding a dependency would create a circular reference
    /// Returns Err with the cycle path if circular dependency detected
    pub fn would_create_cycle(&self, new_dep_id: &str, all_tasks: &[Task]) -> Result<(), String> {
        if self.id == new_dep_id {
            return Err(format!("Self-reference: {} -> {}", self.id, new_dep_id));
        }

        let mut visited = std::collections::HashSet::new();
        let mut path = Vec::new();

        Self::detect_cycle_recursive(new_dep_id, &self.id, all_tasks, &mut visited, &mut path)
    }

    fn detect_cycle_recursive(
        current_id: &str,
        target_id: &str,
        all_tasks: &[Task],
        visited: &mut std::collections::HashSet<String>,
        path: &mut Vec<String>,
    ) -> Result<(), String> {
        if current_id == target_id {
            path.push(current_id.to_string());
            return Err(format!("Circular dependency: {}", path.join(" -> ")));
        }

        if visited.contains(current_id) {
            return Ok(());
        }

        visited.insert(current_id.to_string());
        path.push(current_id.to_string());

        if let Some(task) = all_tasks.iter().find(|t| t.id == current_id) {
            for dep_id in &task.dependencies {
                Self::detect_cycle_recursive(dep_id, target_id, all_tasks, visited, path)?;
            }
        }

        path.pop();
        Ok(())
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_task_creation() {
        let task = Task::new(
            "TASK-1".to_string(),
            "Test Task".to_string(),
            "Description".to_string(),
        );

        assert_eq!(task.id, "TASK-1");
        assert_eq!(task.title, "Test Task");
        assert_eq!(task.description, "Description");
        assert_eq!(task.status, TaskStatus::Pending);
        assert_eq!(task.complexity, 0);
        assert_eq!(task.priority, Priority::Medium);
        assert!(task.dependencies.is_empty());
        assert!(task.created_at.is_some());
        assert!(task.updated_at.is_some());
        assert!(task.assigned_to.is_none());
        assert!(task.locked_by.is_none());
        assert!(task.locked_at.is_none());
    }

    #[test]
    fn test_status_conversion() {
        assert_eq!(TaskStatus::Pending.as_str(), "pending");
        assert_eq!(TaskStatus::InProgress.as_str(), "in-progress");
        assert_eq!(TaskStatus::Done.as_str(), "done");
        assert_eq!(TaskStatus::Review.as_str(), "review");
        assert_eq!(TaskStatus::Blocked.as_str(), "blocked");
        assert_eq!(TaskStatus::Deferred.as_str(), "deferred");
        assert_eq!(TaskStatus::Cancelled.as_str(), "cancelled");
    }

    #[test]
    fn test_status_from_string() {
        assert_eq!(TaskStatus::from_str("pending"), Some(TaskStatus::Pending));
        assert_eq!(
            TaskStatus::from_str("in-progress"),
            Some(TaskStatus::InProgress)
        );
        assert_eq!(TaskStatus::from_str("done"), Some(TaskStatus::Done));
        assert_eq!(TaskStatus::from_str("invalid"), None);
    }

    #[test]
    fn test_set_status_updates_timestamp() {
        let mut task = Task::new("TASK-1".to_string(), "Test".to_string(), "Desc".to_string());
        let initial_updated = task.updated_at.clone();

        std::thread::sleep(std::time::Duration::from_millis(10));
        task.set_status(TaskStatus::InProgress);

        assert_eq!(task.status, TaskStatus::InProgress);
        assert!(task.updated_at > initial_updated);
    }

    #[test]
    fn test_task_assignment() {
        let mut task = Task::new("TASK-1".to_string(), "Test".to_string(), "Desc".to_string());

        task.assign("alice");
        assert_eq!(task.assigned_to, Some("alice".to_string()));
        assert!(task.is_assigned_to("alice"));
        assert!(!task.is_assigned_to("bob"));
    }

    #[test]
    fn test_task_claim_success() {
        let mut task = Task::new("TASK-1".to_string(), "Test".to_string(), "Desc".to_string());

        let result = task.claim("alice");
        assert!(result.is_ok());
        assert_eq!(task.assigned_to, Some("alice".to_string()));
        assert_eq!(task.locked_by, Some("alice".to_string()));
        assert!(task.locked_at.is_some());
        assert!(task.is_locked());
        assert!(task.is_locked_by("alice"));
    }

    #[test]
    fn test_task_claim_already_locked_by_same_user() {
        let mut task = Task::new("TASK-1".to_string(), "Test".to_string(), "Desc".to_string());

        task.claim("alice").unwrap();
        let result = task.claim("alice");
        assert!(result.is_ok()); // Same user can re-claim
    }

    #[test]
    fn test_task_claim_already_locked_by_different_user() {
        let mut task = Task::new("TASK-1".to_string(), "Test".to_string(), "Desc".to_string());

        task.claim("alice").unwrap();
        let result = task.claim("bob");
        assert!(result.is_err());
        assert_eq!(result.unwrap_err(), "Task is locked by alice");
    }

    #[test]
    fn test_task_release() {
        let mut task = Task::new("TASK-1".to_string(), "Test".to_string(), "Desc".to_string());

        task.claim("alice").unwrap();
        assert!(task.is_locked());

        task.release();
        assert!(!task.is_locked());
        assert_eq!(task.locked_by, None);
        assert_eq!(task.locked_at, None);
        assert_eq!(task.assigned_to, Some("alice".to_string())); // Assignment persists
    }

    #[test]
    fn test_lock_age_calculation() {
        let mut task = Task::new("TASK-1".to_string(), "Test".to_string(), "Desc".to_string());

        task.claim("alice").unwrap();

        let age = task.lock_age_hours();
        assert!(age.is_some());
        assert!(age.unwrap() < 0.001); // Should be very recent (< 1 minute)
    }

    #[test]
    fn test_stale_lock_detection() {
        let mut task = Task::new("TASK-1".to_string(), "Test".to_string(), "Desc".to_string());

        task.claim("alice").unwrap();

        // Not stale immediately
        assert!(!task.is_stale_lock(24.0));

        // Simulate old lock by setting locked_at to 48 hours ago
        let two_days_ago = chrono::Utc::now() - chrono::Duration::hours(48);
        task.locked_at = Some(two_days_ago.to_rfc3339());

        assert!(task.is_stale_lock(24.0));
        assert!(!task.is_stale_lock(72.0));
    }

    #[test]
    fn test_has_dependencies_met_all_done() {
        let mut task = Task::new("TASK-3".to_string(), "Test".to_string(), "Desc".to_string());
        task.dependencies = vec!["TASK-1".to_string(), "TASK-2".to_string()];

        let mut task1 = Task::new(
            "TASK-1".to_string(),
            "Dep 1".to_string(),
            "Desc".to_string(),
        );
        task1.set_status(TaskStatus::Done);

        let mut task2 = Task::new(
            "TASK-2".to_string(),
            "Dep 2".to_string(),
            "Desc".to_string(),
        );
        task2.set_status(TaskStatus::Done);

        let all_tasks = vec![task1, task2];
        assert!(task.has_dependencies_met(&all_tasks));
    }

    #[test]
    fn test_has_dependencies_met_some_pending() {
        let mut task = Task::new("TASK-3".to_string(), "Test".to_string(), "Desc".to_string());
        task.dependencies = vec!["TASK-1".to_string(), "TASK-2".to_string()];

        let mut task1 = Task::new(
            "TASK-1".to_string(),
            "Dep 1".to_string(),
            "Desc".to_string(),
        );
        task1.set_status(TaskStatus::Done);

        let task2 = Task::new(
            "TASK-2".to_string(),
            "Dep 2".to_string(),
            "Desc".to_string(),
        );
        // task2 is pending

        let all_tasks = vec![task1, task2];
        assert!(!task.has_dependencies_met(&all_tasks));
    }

    #[test]
    fn test_has_dependencies_met_missing_dependency() {
        let mut task = Task::new("TASK-3".to_string(), "Test".to_string(), "Desc".to_string());
        task.dependencies = vec!["TASK-1".to_string(), "TASK-MISSING".to_string()];

        let mut task1 = Task::new(
            "TASK-1".to_string(),
            "Dep 1".to_string(),
            "Desc".to_string(),
        );
        task1.set_status(TaskStatus::Done);

        let all_tasks = vec![task1];
        assert!(!task.has_dependencies_met(&all_tasks));
    }

    #[test]
    fn test_needs_expansion() {
        let mut task = Task::new("TASK-1".to_string(), "Test".to_string(), "Desc".to_string());

        // Complexity < 3 should not need expansion
        task.complexity = 1;
        assert!(!task.needs_expansion());

        task.complexity = 2;
        assert!(!task.needs_expansion());

        // Complexity >= 3 should need expansion
        task.complexity = 3;
        assert!(task.needs_expansion());

        task.complexity = 8;
        assert!(task.needs_expansion());

        task.complexity = 13;
        assert!(task.needs_expansion());

        task.complexity = 21;
        assert!(task.needs_expansion());

        // Already expanded tasks (with Expanded status) should not need expansion
        task.status = TaskStatus::Expanded;
        assert!(!task.needs_expansion());

        // Reset status and test subtask case
        task.status = TaskStatus::Pending;
        task.parent_id = Some("parent:1".to_string());
        assert!(!task.needs_expansion()); // Subtasks don't need expansion

        // Reset and test tasks with subtasks
        task.parent_id = None;
        task.subtasks = vec!["TASK-1.1".to_string()];
        assert!(!task.needs_expansion()); // Already has subtasks
    }

    #[test]
    fn test_task_serialization() {
        let task = Task::new(
            "TASK-1".to_string(),
            "Test Task".to_string(),
            "Description".to_string(),
        );

        let json = serde_json::to_string(&task).unwrap();
        let deserialized: Task = serde_json::from_str(&json).unwrap();

        assert_eq!(task.id, deserialized.id);
        assert_eq!(task.title, deserialized.title);
        assert_eq!(task.description, deserialized.description);
    }

    #[test]
    fn test_task_serialization_with_optional_fields() {
        let mut task = Task::new("TASK-1".to_string(), "Test".to_string(), "Desc".to_string());
        task.details = Some("Detailed info".to_string());
        task.test_strategy = Some("Test plan".to_string());
        task.claim("alice").unwrap();

        let json = serde_json::to_string(&task).unwrap();
        let deserialized: Task = serde_json::from_str(&json).unwrap();

        assert_eq!(task.details, deserialized.details);
        assert_eq!(task.test_strategy, deserialized.test_strategy);
        assert_eq!(task.locked_by, deserialized.locked_by);
        assert_eq!(task.locked_at, deserialized.locked_at);
    }

    #[test]
    fn test_priority_default() {
        let default_priority = Priority::default();
        assert_eq!(default_priority, Priority::Medium);
    }

    #[test]
    fn test_status_all() {
        let all_statuses = TaskStatus::all();
        assert_eq!(all_statuses.len(), 8);
        assert!(all_statuses.contains(&"pending"));
        assert!(all_statuses.contains(&"in-progress"));
        assert!(all_statuses.contains(&"done"));
        assert!(all_statuses.contains(&"review"));
        assert!(all_statuses.contains(&"blocked"));
        assert!(all_statuses.contains(&"deferred"));
        assert!(all_statuses.contains(&"cancelled"));
        assert!(all_statuses.contains(&"expanded"));
    }

    #[test]
    fn test_circular_dependency_self_reference() {
        let task = Task::new("TASK-1".to_string(), "Test".to_string(), "Desc".to_string());
        let all_tasks = vec![task.clone()];

        let result = task.would_create_cycle("TASK-1", &all_tasks);
        assert!(result.is_err());
        assert!(result.unwrap_err().contains("Self-reference"));
    }

    #[test]
    fn test_circular_dependency_direct_cycle() {
        let mut task1 = Task::new(
            "TASK-1".to_string(),
            "Task 1".to_string(),
            "Desc".to_string(),
        );
        task1.dependencies = vec!["TASK-2".to_string()];

        let task2 = Task::new(
            "TASK-2".to_string(),
            "Task 2".to_string(),
            "Desc".to_string(),
        );

        let all_tasks = vec![task1.clone(), task2.clone()];

        // Trying to add TASK-1 as dependency of TASK-2 would create cycle: TASK-2 -> TASK-1 -> TASK-2
        let result = task2.would_create_cycle("TASK-1", &all_tasks);
        assert!(result.is_err());
        assert!(result.unwrap_err().contains("Circular dependency"));
    }

    #[test]
    fn test_circular_dependency_indirect_cycle() {
        let mut task1 = Task::new(
            "TASK-1".to_string(),
            "Task 1".to_string(),
            "Desc".to_string(),
        );
        task1.dependencies = vec!["TASK-2".to_string()];

        let mut task2 = Task::new(
            "TASK-2".to_string(),
            "Task 2".to_string(),
            "Desc".to_string(),
        );
        task2.dependencies = vec!["TASK-3".to_string()];

        let task3 = Task::new(
            "TASK-3".to_string(),
            "Task 3".to_string(),
            "Desc".to_string(),
        );

        let all_tasks = vec![task1.clone(), task2, task3.clone()];

        // Trying to add TASK-1 as dependency of TASK-3 would create cycle:
        // TASK-3 -> TASK-1 -> TASK-2 -> TASK-3
        let result = task3.would_create_cycle("TASK-1", &all_tasks);
        assert!(result.is_err());
        assert!(result.unwrap_err().contains("Circular dependency"));
    }

    #[test]
    fn test_circular_dependency_no_cycle() {
        let mut task1 = Task::new(
            "TASK-1".to_string(),
            "Task 1".to_string(),
            "Desc".to_string(),
        );
        task1.dependencies = vec!["TASK-3".to_string()];

        let task2 = Task::new(
            "TASK-2".to_string(),
            "Task 2".to_string(),
            "Desc".to_string(),
        );

        let task3 = Task::new(
            "TASK-3".to_string(),
            "Task 3".to_string(),
            "Desc".to_string(),
        );

        let all_tasks = vec![task1.clone(), task2.clone(), task3];

        // Adding TASK-2 as dependency of TASK-1 is fine (no cycle)
        let result = task1.would_create_cycle("TASK-2", &all_tasks);
        assert!(result.is_ok());
    }

    #[test]
    fn test_circular_dependency_complex_graph() {
        let mut task1 = Task::new(
            "TASK-1".to_string(),
            "Task 1".to_string(),
            "Desc".to_string(),
        );
        task1.dependencies = vec!["TASK-2".to_string(), "TASK-3".to_string()];

        let mut task2 = Task::new(
            "TASK-2".to_string(),
            "Task 2".to_string(),
            "Desc".to_string(),
        );
        task2.dependencies = vec!["TASK-4".to_string()];

        let mut task3 = Task::new(
            "TASK-3".to_string(),
            "Task 3".to_string(),
            "Desc".to_string(),
        );
        task3.dependencies = vec!["TASK-4".to_string()];

        let task4 = Task::new(
            "TASK-4".to_string(),
            "Task 4".to_string(),
            "Desc".to_string(),
        );

        let all_tasks = vec![task1.clone(), task2, task3, task4.clone()];

        // Adding TASK-1 as dependency of TASK-4 would create a cycle
        let result = task4.would_create_cycle("TASK-1", &all_tasks);
        assert!(result.is_err());
        assert!(result.unwrap_err().contains("Circular dependency"));
    }

    // Validation tests
    #[test]
    fn test_validate_id_success() {
        assert!(Task::validate_id("TASK-123").is_ok());
        assert!(Task::validate_id("task_456").is_ok());
        assert!(Task::validate_id("Feature-789").is_ok());
        // Namespaced IDs
        assert!(Task::validate_id("phase1:10").is_ok());
        assert!(Task::validate_id("phase1:10.1").is_ok());
        assert!(Task::validate_id("my-epic:subtask-1.2.3").is_ok());
    }

    #[test]
    fn test_validate_id_empty() {
        let result = Task::validate_id("");
        assert!(result.is_err());
        assert_eq!(result.unwrap_err(), "Task ID cannot be empty");
    }

    #[test]
    fn test_validate_id_too_long() {
        let long_id = "A".repeat(101);
        let result = Task::validate_id(&long_id);
        assert!(result.is_err());
        assert!(result.unwrap_err().contains("too long"));
    }

    #[test]
    fn test_validate_id_invalid_characters() {
        assert!(Task::validate_id("TASK@123").is_err());
        assert!(Task::validate_id("TASK 123").is_err());
        assert!(Task::validate_id("TASK#123").is_err());
        // Note: dot and colon are now valid for namespaced IDs
        assert!(Task::validate_id("TASK.123").is_ok()); // Valid for subtask IDs like "10.1"
        assert!(Task::validate_id("epic:TASK-1").is_ok()); // Valid namespaced ID
    }

    #[test]
    fn test_validate_title_success() {
        assert!(Task::validate_title("Valid title").is_ok());
        assert!(Task::validate_title("A").is_ok());
    }

    #[test]
    fn test_validate_title_empty() {
        let result = Task::validate_title("");
        assert!(result.is_err());
        assert_eq!(result.unwrap_err(), "Task title cannot be empty");

        let result = Task::validate_title("   ");
        assert!(result.is_err());
        assert_eq!(result.unwrap_err(), "Task title cannot be empty");
    }

    #[test]
    fn test_validate_title_too_long() {
        let long_title = "A".repeat(201);
        let result = Task::validate_title(&long_title);
        assert!(result.is_err());
        assert!(result.unwrap_err().contains("too long"));
    }

    #[test]
    fn test_validate_description_success() {
        assert!(Task::validate_description("Valid description").is_ok());
        assert!(Task::validate_description("").is_ok());
    }

    #[test]
    fn test_validate_description_too_long() {
        let long_desc = "A".repeat(5001);
        let result = Task::validate_description(&long_desc);
        assert!(result.is_err());
        assert!(result.unwrap_err().contains("too long"));
    }

    #[test]
    fn test_validate_complexity_success() {
        assert!(Task::validate_complexity(0).is_ok());
        assert!(Task::validate_complexity(1).is_ok());
        assert!(Task::validate_complexity(2).is_ok());
        assert!(Task::validate_complexity(3).is_ok());
        assert!(Task::validate_complexity(5).is_ok());
        assert!(Task::validate_complexity(8).is_ok());
        assert!(Task::validate_complexity(13).is_ok());
        assert!(Task::validate_complexity(21).is_ok());
    }

    #[test]
    fn test_validate_complexity_invalid() {
        assert!(Task::validate_complexity(4).is_err());
        assert!(Task::validate_complexity(6).is_err());
        assert!(Task::validate_complexity(7).is_err());
        assert!(Task::validate_complexity(100).is_err());
    }

    #[test]
    fn test_sanitize_text() {
        assert_eq!(
            Task::sanitize_text("<script>alert('xss')</script>"),
            "&lt;script&gt;alert(&#x27;xss&#x27;)&lt;/script&gt;"
        );
        assert_eq!(Task::sanitize_text("Normal text"), "Normal text");
        assert_eq!(
            Task::sanitize_text("<div>Content</div>"),
            "&lt;div&gt;Content&lt;/div&gt;"
        );
    }

    #[test]
    fn test_validate_success() {
        let task = Task::new(
            "TASK-1".to_string(),
            "Valid title".to_string(),
            "Valid description".to_string(),
        );
        assert!(task.validate().is_ok());
    }

    #[test]
    fn test_validate_multiple_errors() {
        let mut task = Task::new("TASK@INVALID".to_string(), "".to_string(), "A".repeat(5001));
        task.complexity = 100; // Invalid Fibonacci number

        let result = task.validate();
        assert!(result.is_err());
        let errors = result.unwrap_err();
        assert_eq!(errors.len(), 4);
        assert!(errors.iter().any(|e| e.contains("ID")));
        assert!(errors.iter().any(|e| e.contains("title")));
        assert!(errors.iter().any(|e| e.contains("description")));
        assert!(errors.iter().any(|e| e.contains("Complexity")));
    }

    // Cross-tag dependency tests
    #[test]
    fn test_cross_tag_dependency_met() {
        let mut task_a = Task::new(
            "auth:1".to_string(),
            "Auth task".to_string(),
            "Desc".to_string(),
        );
        task_a.set_status(TaskStatus::Done);

        let mut task_b = Task::new(
            "api:1".to_string(),
            "API task".to_string(),
            "Desc".to_string(),
        );
        task_b.dependencies = vec!["auth:1".to_string()];

        let all_tasks = vec![&task_a, &task_b];
        assert!(task_b.has_dependencies_met_refs(&all_tasks));
    }

    #[test]
    fn test_cross_tag_dependency_not_met() {
        let task_a = Task::new(
            "auth:1".to_string(),
            "Auth task".to_string(),
            "Desc".to_string(),
        );
        // task_a still pending

        let mut task_b = Task::new(
            "api:1".to_string(),
            "API task".to_string(),
            "Desc".to_string(),
        );
        task_b.dependencies = vec!["auth:1".to_string()];

        let all_tasks = vec![&task_a, &task_b];
        assert!(!task_b.has_dependencies_met_refs(&all_tasks));
    }

    #[test]
    fn test_local_dependency_still_works_with_refs() {
        let mut task_a = Task::new("1".to_string(), "First".to_string(), "Desc".to_string());
        task_a.set_status(TaskStatus::Done);

        let mut task_b = Task::new("2".to_string(), "Second".to_string(), "Desc".to_string());
        task_b.dependencies = vec!["1".to_string()];

        let all_tasks = vec![&task_a, &task_b];
        assert!(task_b.has_dependencies_met_refs(&all_tasks));
    }

    #[test]
    fn test_has_dependencies_met_refs_missing_dependency() {
        let mut task = Task::new("api:1".to_string(), "Test".to_string(), "Desc".to_string());
        task.dependencies = vec!["auth:1".to_string(), "auth:MISSING".to_string()];

        let mut dep1 = Task::new(
            "auth:1".to_string(),
            "Dep 1".to_string(),
            "Desc".to_string(),
        );
        dep1.set_status(TaskStatus::Done);

        let all_tasks = vec![&dep1];
        assert!(!task.has_dependencies_met_refs(&all_tasks));
    }
}
</file>

<file path="scud-cli/src/storage/mod.rs">
use anyhow::{Context, Result};
use fs2::FileExt;
use std::collections::HashMap;
use std::fs::{self, File, OpenOptions};
use std::path::{Path, PathBuf};
use std::sync::RwLock;
use std::thread;
use std::time::Duration;

use crate::config::Config;
use crate::formats::{parse_scg, serialize_scg};
use crate::models::Phase;

pub struct Storage {
    project_root: PathBuf,
    /// Cache for active group to avoid repeated workflow state loads
    /// Option<Option<String>> represents: None = not cached, Some(None) = no active group, Some(Some(tag)) = cached tag
    /// Uses RwLock for thread safety (useful for tests and potential daemon mode)
    active_group_cache: RwLock<Option<Option<String>>>,
}

impl Storage {
    pub fn new(project_root: Option<PathBuf>) -> Self {
        let root = project_root.unwrap_or_else(|| std::env::current_dir().unwrap());
        Storage {
            project_root: root,
            active_group_cache: RwLock::new(None),
        }
    }

    /// Acquire an exclusive file lock with retry logic
    fn acquire_lock_with_retry(&self, file: &File, max_retries: u32) -> Result<()> {
        let mut retries = 0;
        let mut delay_ms = 10;

        loop {
            match file.try_lock_exclusive() {
                Ok(_) => return Ok(()),
                Err(_) if retries < max_retries => {
                    retries += 1;
                    thread::sleep(Duration::from_millis(delay_ms));
                    delay_ms = (delay_ms * 2).min(1000); // Exponential backoff, max 1s
                }
                Err(e) => {
                    anyhow::bail!(
                        "Failed to acquire file lock after {} retries: {}",
                        max_retries,
                        e
                    )
                }
            }
        }
    }

    /// Perform a locked write operation on a file
    fn write_with_lock<F>(&self, path: &Path, writer: F) -> Result<()>
    where
        F: FnOnce() -> Result<String>,
    {
        use std::io::Write;

        let dir = path.parent().unwrap();
        if !dir.exists() {
            fs::create_dir_all(dir)?;
        }

        // Open file for writing
        let mut file = OpenOptions::new()
            .write(true)
            .create(true)
            .truncate(true)
            .open(path)
            .with_context(|| format!("Failed to open file for writing: {}", path.display()))?;

        // Acquire lock with retry
        self.acquire_lock_with_retry(&file, 10)?;

        // Generate content and write through the locked handle
        let content = writer()?;
        file.write_all(content.as_bytes())
            .with_context(|| format!("Failed to write to {}", path.display()))?;
        file.flush()
            .with_context(|| format!("Failed to flush {}", path.display()))?;

        // Lock is automatically released when file is dropped
        Ok(())
    }

    /// Perform a locked read operation on a file
    fn read_with_lock(&self, path: &Path) -> Result<String> {
        use std::io::Read;

        if !path.exists() {
            anyhow::bail!("File not found: {}", path.display());
        }

        // Open file for reading
        let mut file = OpenOptions::new()
            .read(true)
            .open(path)
            .with_context(|| format!("Failed to open file for reading: {}", path.display()))?;

        // Acquire shared lock (allows multiple readers)
        file.lock_shared()
            .with_context(|| format!("Failed to acquire read lock on {}", path.display()))?;

        // Read content through the locked handle
        let mut content = String::new();
        file.read_to_string(&mut content)
            .with_context(|| format!("Failed to read from {}", path.display()))?;

        // Lock is automatically released when file is dropped
        Ok(content)
    }

    pub fn scud_dir(&self) -> PathBuf {
        self.project_root.join(".scud")
    }

    pub fn tasks_file(&self) -> PathBuf {
        self.scud_dir().join("tasks").join("tasks.scg")
    }

    fn active_tag_file(&self) -> PathBuf {
        self.scud_dir().join("active-tag")
    }

    pub fn config_file(&self) -> PathBuf {
        self.scud_dir().join("config.toml")
    }

    pub fn docs_dir(&self) -> PathBuf {
        self.scud_dir().join("docs")
    }

    pub fn is_initialized(&self) -> bool {
        self.scud_dir().exists() && self.tasks_file().exists()
    }

    pub fn initialize(&self) -> Result<()> {
        let config = Config::default();
        self.initialize_with_config(&config)
    }

    pub fn initialize_with_config(&self, config: &Config) -> Result<()> {
        // Create .scud directory structure
        let scud_dir = self.scud_dir();
        fs::create_dir_all(scud_dir.join("tasks"))
            .context("Failed to create .scud/tasks directory")?;

        // Initialize config.toml
        let config_file = self.config_file();
        if !config_file.exists() {
            config.save(&config_file)?;
        }

        // Initialize tasks.scg with empty content
        let tasks_file = self.tasks_file();
        if !tasks_file.exists() {
            let empty_tasks: HashMap<String, Phase> = HashMap::new();
            self.save_tasks(&empty_tasks)?;
        }

        // Create docs directories
        let docs = self.docs_dir();
        fs::create_dir_all(docs.join("prd"))?;
        fs::create_dir_all(docs.join("phases"))?;
        fs::create_dir_all(docs.join("architecture"))?;
        fs::create_dir_all(docs.join("retrospectives"))?;

        // Create CLAUDE.md with agent instructions
        self.create_agent_instructions()?;

        Ok(())
    }

    pub fn load_config(&self) -> Result<Config> {
        let config_file = self.config_file();
        if !config_file.exists() {
            return Ok(Config::default());
        }
        Config::load(&config_file)
    }

    pub fn load_tasks(&self) -> Result<HashMap<String, Phase>> {
        let path = self.tasks_file();
        if !path.exists() {
            anyhow::bail!("Tasks file not found: {}\nRun: scud init", path.display());
        }

        let content = self.read_with_lock(&path)?;
        self.parse_multi_phase_scg(&content)
    }

    /// Parse multi-phase SCG format (multiple phases separated by ---)
    fn parse_multi_phase_scg(&self, content: &str) -> Result<HashMap<String, Phase>> {
        let mut phases = HashMap::new();

        // Empty file returns empty map
        if content.trim().is_empty() {
            return Ok(phases);
        }

        // Split by phase separator (---)
        let sections: Vec<&str> = content.split("\n---\n").collect();

        for section in sections {
            let section = section.trim();
            if section.is_empty() {
                continue;
            }

            // Parse the phase section
            let phase = parse_scg(section).with_context(|| "Failed to parse SCG section")?;

            phases.insert(phase.name.clone(), phase);
        }

        Ok(phases)
    }

    pub fn save_tasks(&self, tasks: &HashMap<String, Phase>) -> Result<()> {
        let path = self.tasks_file();
        self.write_with_lock(&path, || {
            // Sort phases by tag for consistent output
            let mut sorted_tags: Vec<_> = tasks.keys().collect();
            sorted_tags.sort();

            let mut output = String::new();
            for (i, tag) in sorted_tags.iter().enumerate() {
                if i > 0 {
                    output.push_str("\n---\n\n");
                }
                let phase = tasks.get(*tag).unwrap();
                output.push_str(&serialize_scg(phase));
            }

            Ok(output)
        })
    }

    pub fn get_active_group(&self) -> Result<Option<String>> {
        // Check cache first (read lock)
        {
            let cache = self.active_group_cache.read().unwrap();
            if let Some(cached) = cache.as_ref() {
                return Ok(cached.clone());
            }
        }

        // Load from active-tag file
        let active_tag_path = self.active_tag_file();
        let active = if active_tag_path.exists() {
            let content = fs::read_to_string(&active_tag_path)
                .with_context(|| format!("Failed to read {}", active_tag_path.display()))?;
            let tag = content.trim();
            if tag.is_empty() {
                None
            } else {
                Some(tag.to_string())
            }
        } else {
            None
        };

        // Store in cache
        *self.active_group_cache.write().unwrap() = Some(active.clone());

        Ok(active)
    }

    pub fn set_active_group(&self, group_tag: &str) -> Result<()> {
        let tasks = self.load_tasks()?;
        if !tasks.contains_key(group_tag) {
            anyhow::bail!("Task group '{}' not found", group_tag);
        }

        // Write to active-tag file
        let active_tag_path = self.active_tag_file();
        fs::write(&active_tag_path, group_tag)
            .with_context(|| format!("Failed to write {}", active_tag_path.display()))?;

        // Update cache
        *self.active_group_cache.write().unwrap() = Some(Some(group_tag.to_string()));

        Ok(())
    }

    /// Clear the active group cache
    /// Useful when workflow state is modified externally or for testing
    pub fn clear_cache(&self) {
        *self.active_group_cache.write().unwrap() = None;
    }

    /// Load a single task group by tag
    /// Parses the SCG file and extracts the requested group
    pub fn load_group(&self, group_tag: &str) -> Result<Phase> {
        let path = self.tasks_file();
        let content = self.read_with_lock(&path)?;

        let groups = self.parse_multi_phase_scg(&content)?;

        groups
            .get(group_tag)
            .cloned()
            .ok_or_else(|| anyhow::anyhow!("Task group '{}' not found", group_tag))
    }

    /// Load the active task group directly (optimized)
    /// Combines get_active_group() and load_group() in one call
    pub fn load_active_group(&self) -> Result<Phase> {
        let active_tag = self
            .get_active_group()?
            .ok_or_else(|| anyhow::anyhow!("No active task group. Run: scud use-tag <tag>"))?;

        self.load_group(&active_tag)
    }

    /// Update a single task group atomically
    /// Holds exclusive lock across read-modify-write cycle to prevent races
    pub fn update_group(&self, group_tag: &str, group: &Phase) -> Result<()> {
        use std::io::{Read, Seek, SeekFrom, Write};

        let path = self.tasks_file();

        let dir = path.parent().unwrap();
        if !dir.exists() {
            fs::create_dir_all(dir)?;
        }

        // Open file for read+write with exclusive lock held throughout
        // Note: truncate(false) is explicit - we read first, then truncate manually after
        let mut file = OpenOptions::new()
            .read(true)
            .write(true)
            .create(true)
            .truncate(false)
            .open(&path)
            .with_context(|| format!("Failed to open file: {}", path.display()))?;

        // Acquire exclusive lock with retry (held for entire operation)
        self.acquire_lock_with_retry(&file, 10)?;

        // Read current content while holding lock
        let mut content = String::new();
        file.read_to_string(&mut content)
            .with_context(|| format!("Failed to read from {}", path.display()))?;

        // Parse, modify, and serialize
        let mut groups = self.parse_multi_phase_scg(&content)?;
        groups.insert(group_tag.to_string(), group.clone());

        let mut sorted_tags: Vec<_> = groups.keys().collect();
        sorted_tags.sort();

        let mut output = String::new();
        for (i, tag) in sorted_tags.iter().enumerate() {
            if i > 0 {
                output.push_str("\n---\n\n");
            }
            let grp = groups.get(*tag).unwrap();
            output.push_str(&serialize_scg(grp));
        }

        // Truncate and write back while still holding lock
        file.seek(SeekFrom::Start(0))
            .with_context(|| "Failed to seek to beginning of file")?;
        file.set_len(0).with_context(|| "Failed to truncate file")?;
        file.write_all(output.as_bytes())
            .with_context(|| format!("Failed to write to {}", path.display()))?;
        file.flush()
            .with_context(|| format!("Failed to flush {}", path.display()))?;

        // Lock released when file is dropped
        Ok(())
    }

    pub fn read_file(&self, path: &Path) -> Result<String> {
        fs::read_to_string(path).with_context(|| format!("Failed to read file: {}", path.display()))
    }

    /// Create or update CLAUDE.md with SCUD agent instructions
    fn create_agent_instructions(&self) -> Result<()> {
        let claude_md_path = self.project_root.join("CLAUDE.md");

        let scud_instructions = r#"
## SCUD Task Management

This project uses SCUD (Sprint Cycle Unified Development) for task management.

### Session Workflow

1. **Start of session**: Run `scud warmup` to orient yourself
   - Shows current working directory and recent git history
   - Displays active tag, task counts, and any stale locks
   - Identifies the next available task

2. **Claim a task**: Use `/scud:task-next` or `scud next --claim --name "Claude"`
   - Always claim before starting work to prevent conflicts
   - Task context is stored in `.scud/current-task`

3. **Work on the task**: Implement the requirements
   - Reference task details with `/scud:task-show <id>`
   - Dependencies are automatically tracked by the DAG

4. **Commit with context**: Use `scud commit -m "message"` or `scud commit -a -m "message"`
   - Automatically prefixes commits with `[TASK-ID]`
   - Uses task title as default commit message if none provided

5. **Complete the task**: Mark done with `/scud:task-status <id> done`
   - The stop hook will prompt for task completion

### Progress Journaling

Keep a brief progress log during complex tasks:

```
## Progress Log

### Session: 2025-01-15
- Investigated auth module, found issue in token refresh
- Updated refresh logic to handle edge case
- Tests passing, ready for review
```

This helps maintain continuity across sessions and provides context for future work.

### Key Commands

- `scud warmup` - Session orientation
- `scud next` - Find next available task
- `scud show <id>` - View task details
- `scud set-status <id> <status>` - Update task status
- `scud commit` - Task-aware git commit
- `scud stats` - View completion statistics
"#;

        if claude_md_path.exists() {
            // Append to existing CLAUDE.md if SCUD section doesn't exist
            let content = fs::read_to_string(&claude_md_path)
                .with_context(|| "Failed to read existing CLAUDE.md")?;

            if !content.contains("## SCUD Task Management") {
                let mut new_content = content;
                new_content.push_str(scud_instructions);
                fs::write(&claude_md_path, new_content)
                    .with_context(|| "Failed to update CLAUDE.md")?;
            }
        } else {
            // Create new CLAUDE.md
            let content = format!("# Project Instructions\n{}", scud_instructions);
            fs::write(&claude_md_path, content).with_context(|| "Failed to create CLAUDE.md")?;
        }

        Ok(())
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use std::collections::HashMap;
    use tempfile::TempDir;

    fn create_test_storage() -> (Storage, TempDir) {
        let temp_dir = TempDir::new().unwrap();
        let storage = Storage::new(Some(temp_dir.path().to_path_buf()));
        storage.initialize().unwrap();
        (storage, temp_dir)
    }

    #[test]
    fn test_write_with_lock_creates_file() {
        let (storage, _temp_dir) = create_test_storage();
        let test_file = storage.scud_dir().join("test.json");

        storage
            .write_with_lock(&test_file, || Ok(r#"{"test": "data"}"#.to_string()))
            .unwrap();

        assert!(test_file.exists());
        let content = fs::read_to_string(&test_file).unwrap();
        assert_eq!(content, r#"{"test": "data"}"#);
    }

    #[test]
    fn test_read_with_lock_reads_existing_file() {
        let (storage, _temp_dir) = create_test_storage();
        let test_file = storage.scud_dir().join("test.json");

        // Create a file
        fs::write(&test_file, r#"{"test": "data"}"#).unwrap();

        // Read with lock
        let content = storage.read_with_lock(&test_file).unwrap();
        assert_eq!(content, r#"{"test": "data"}"#);
    }

    #[test]
    fn test_read_with_lock_fails_on_missing_file() {
        let (storage, _temp_dir) = create_test_storage();
        let test_file = storage.scud_dir().join("nonexistent.json");

        let result = storage.read_with_lock(&test_file);
        assert!(result.is_err());
        assert!(result.unwrap_err().to_string().contains("File not found"));
    }

    #[test]
    fn test_save_and_load_tasks_with_locking() {
        let (storage, _temp_dir) = create_test_storage();
        let mut tasks = HashMap::new();

        let epic = crate::models::Phase::new("TEST-1".to_string());
        tasks.insert("TEST-1".to_string(), epic);

        // Save tasks
        storage.save_tasks(&tasks).unwrap();

        // Load tasks
        let loaded_tasks = storage.load_tasks().unwrap();

        assert_eq!(tasks.len(), loaded_tasks.len());
        assert!(loaded_tasks.contains_key("TEST-1"));
        assert_eq!(loaded_tasks.get("TEST-1").unwrap().name, "TEST-1");
    }

    #[test]
    fn test_concurrent_writes_dont_corrupt_data() {
        use std::sync::Arc;
        use std::thread;

        let (storage, _temp_dir) = create_test_storage();
        let storage = Arc::new(storage);
        let mut handles = vec![];

        // Spawn 10 threads that each write tasks
        for i in 0..10 {
            let storage_clone = Arc::clone(&storage);
            let handle = thread::spawn(move || {
                let mut tasks = HashMap::new();
                let epic = crate::models::Phase::new(format!("EPIC-{}", i));
                tasks.insert(format!("EPIC-{}", i), epic);

                // Each thread writes multiple times
                for _ in 0..5 {
                    storage_clone.save_tasks(&tasks).unwrap();
                    thread::sleep(Duration::from_millis(1));
                }
            });
            handles.push(handle);
        }

        // Wait for all threads to complete
        for handle in handles {
            handle.join().unwrap();
        }

        // Verify that the file is still valid JSON
        let tasks = storage.load_tasks().unwrap();
        // Should have the last written data (from one of the threads)
        assert_eq!(tasks.len(), 1);
    }

    #[test]
    fn test_lock_retry_on_contention() {
        use std::sync::Arc;

        let (storage, _temp_dir) = create_test_storage();
        let storage = Arc::new(storage);
        let test_file = storage.scud_dir().join("lock-test.json");

        // Create file
        storage
            .write_with_lock(&test_file, || Ok(r#"{"initial": "data"}"#.to_string()))
            .unwrap();

        // Open and lock the file
        let file = OpenOptions::new().write(true).open(&test_file).unwrap();
        file.lock_exclusive().unwrap();

        // Try to acquire lock with retry in another thread
        let storage_clone = Arc::clone(&storage);
        let test_file_clone = test_file.clone();
        let handle = thread::spawn(move || {
            // This should retry and succeed after lock release
            storage_clone.write_with_lock(&test_file_clone, || {
                Ok(r#"{"updated": "data"}"#.to_string())
            })
        });

        // Keep lock for a bit
        thread::sleep(Duration::from_millis(200));

        // Release lock
        file.unlock().unwrap();
        drop(file);

        // The write should have succeeded after retrying
        let result = handle.join().unwrap();
        assert!(result.is_ok());
    }

    // ==================== Error Handling Tests ====================

    #[test]
    fn test_load_tasks_with_malformed_json() {
        let (storage, _temp_dir) = create_test_storage();
        let tasks_file = storage.tasks_file();

        // Write malformed JSON
        fs::write(&tasks_file, r#"{"invalid": json here}"#).unwrap();

        // Should return error
        let result = storage.load_tasks();
        assert!(result.is_err());
    }

    #[test]
    fn test_load_tasks_with_empty_file() {
        let (storage, _temp_dir) = create_test_storage();
        let tasks_file = storage.tasks_file();

        // Write empty file
        fs::write(&tasks_file, "").unwrap();

        // Empty SCG file is valid and returns empty HashMap
        let result = storage.load_tasks();
        assert!(result.is_ok());
        assert!(result.unwrap().is_empty());
    }

    #[test]
    fn test_load_tasks_missing_file_creates_default() {
        let (storage, _temp_dir) = create_test_storage();
        // Don't create tasks file

        // Should return empty HashMap (default)
        let tasks = storage.load_tasks().unwrap();
        assert_eq!(tasks.len(), 0);
    }

    #[test]
    fn test_save_tasks_creates_directory_if_missing() {
        let temp_dir = TempDir::new().unwrap();
        let storage = Storage::new(Some(temp_dir.path().to_path_buf()));
        // Don't call initialize()

        let mut tasks = HashMap::new();
        let epic = crate::models::Phase::new("TEST-1".to_string());
        tasks.insert("TEST-1".to_string(), epic);

        // Should create directory and file
        let result = storage.save_tasks(&tasks);
        assert!(result.is_ok());

        assert!(storage.scud_dir().exists());
        assert!(storage.tasks_file().exists());
    }

    #[test]
    fn test_write_with_lock_handles_directory_creation() {
        let temp_dir = TempDir::new().unwrap();
        let storage = Storage::new(Some(temp_dir.path().to_path_buf()));

        let nested_file = temp_dir
            .path()
            .join("deeply")
            .join("nested")
            .join("test.json");

        // Should create all parent directories
        let result = storage.write_with_lock(&nested_file, || Ok("{}".to_string()));
        assert!(result.is_ok());
        assert!(nested_file.exists());
    }

    #[test]
    fn test_load_tasks_with_invalid_structure() {
        let (storage, _temp_dir) = create_test_storage();
        let tasks_file = storage.tasks_file();

        // Write valid JSON but invalid structure (array instead of object)
        fs::write(&tasks_file, r#"["not", "an", "object"]"#).unwrap();

        // Should return error
        let result = storage.load_tasks();
        assert!(result.is_err());
    }

    #[test]
    fn test_save_and_load_with_unicode_content() {
        let (storage, _temp_dir) = create_test_storage();

        let mut tasks = HashMap::new();
        let mut epic = crate::models::Phase::new("TEST-UNICODE".to_string());

        // Add task with unicode content
        let task = crate::models::Task::new(
            "task-1".to_string(),
            "æµ‹è¯• Unicode ðŸš€".to_string(),
            "DescripciÃ³n en espaÃ±ol æ—¥æœ¬èªž".to_string(),
        );
        epic.add_task(task);

        tasks.insert("TEST-UNICODE".to_string(), epic);

        // Save and load
        storage.save_tasks(&tasks).unwrap();
        let loaded_tasks = storage.load_tasks().unwrap();

        let loaded_epic = loaded_tasks.get("TEST-UNICODE").unwrap();
        let loaded_task = loaded_epic.get_task("task-1").unwrap();
        assert_eq!(loaded_task.title, "æµ‹è¯• Unicode ðŸš€");
        assert_eq!(loaded_task.description, "DescripciÃ³n en espaÃ±ol æ—¥æœ¬èªž");
    }

    #[test]
    fn test_save_and_load_with_large_dataset() {
        let (storage, _temp_dir) = create_test_storage();

        let mut tasks = HashMap::new();

        // Create 100 epics with 50 tasks each
        for i in 0..100 {
            let mut epic = crate::models::Phase::new(format!("EPIC-{}", i));

            for j in 0..50 {
                let task = crate::models::Task::new(
                    format!("task-{}-{}", i, j),
                    format!("Task {} of Epic {}", j, i),
                    format!("Description for task {}-{}", i, j),
                );
                epic.add_task(task);
            }

            tasks.insert(format!("EPIC-{}", i), epic);
        }

        // Save and load
        storage.save_tasks(&tasks).unwrap();
        let loaded_tasks = storage.load_tasks().unwrap();

        assert_eq!(loaded_tasks.len(), 100);
        for i in 0..100 {
            let epic = loaded_tasks.get(&format!("EPIC-{}", i)).unwrap();
            assert_eq!(epic.tasks.len(), 50);
        }
    }

    #[test]
    fn test_concurrent_read_and_write() {
        use std::sync::Arc;
        use std::thread;

        let (storage, _temp_dir) = create_test_storage();
        let storage = Arc::new(storage);

        // Initialize with some data
        let mut tasks = HashMap::new();
        let epic = crate::models::Phase::new("INITIAL".to_string());
        tasks.insert("INITIAL".to_string(), epic);
        storage.save_tasks(&tasks).unwrap();

        let mut handles = vec![];

        // Spawn 5 readers
        for _ in 0..5 {
            let storage_clone = Arc::clone(&storage);
            let handle = thread::spawn(move || {
                for _ in 0..10 {
                    let _ = storage_clone.load_tasks();
                    thread::sleep(Duration::from_millis(1));
                }
            });
            handles.push(handle);
        }

        // Spawn 2 writers
        for i in 0..2 {
            let storage_clone = Arc::clone(&storage);
            let handle = thread::spawn(move || {
                for j in 0..5 {
                    let mut tasks = HashMap::new();
                    let epic = crate::models::Phase::new(format!("WRITER-{}-{}", i, j));
                    tasks.insert(format!("WRITER-{}-{}", i, j), epic);
                    storage_clone.save_tasks(&tasks).unwrap();
                    thread::sleep(Duration::from_millis(2));
                }
            });
            handles.push(handle);
        }

        // Wait for all threads
        for handle in handles {
            handle.join().unwrap();
        }

        // File should still be valid
        let tasks = storage.load_tasks().unwrap();
        assert_eq!(tasks.len(), 1); // Last write wins
    }

    // ==================== Active Epic Cache Tests ====================

    #[test]
    fn test_active_epic_cached_on_second_call() {
        let (storage, _temp_dir) = create_test_storage();

        // Set active epic
        let mut tasks = HashMap::new();
        tasks.insert("TEST-1".to_string(), Phase::new("TEST-1".to_string()));
        storage.save_tasks(&tasks).unwrap();
        storage.set_active_group("TEST-1").unwrap();

        // First call - loads from file
        let active1 = storage.get_active_group().unwrap();
        assert_eq!(active1, Some("TEST-1".to_string()));

        // Modify file directly (bypass storage methods)
        let active_tag_file = storage.active_tag_file();
        fs::write(&active_tag_file, "DIFFERENT").unwrap();

        // Second call - should return cached value (not file value)
        let active2 = storage.get_active_group().unwrap();
        assert_eq!(active2, Some("TEST-1".to_string())); // Still cached

        // After cache clear - should reload from file
        storage.clear_cache();
        let active3 = storage.get_active_group().unwrap();
        assert_eq!(active3, Some("DIFFERENT".to_string())); // From file
    }

    #[test]
    fn test_cache_invalidated_on_set_active_epic() {
        let (storage, _temp_dir) = create_test_storage();

        let mut tasks = HashMap::new();
        tasks.insert("EPIC-1".to_string(), Phase::new("EPIC-1".to_string()));
        tasks.insert("EPIC-2".to_string(), Phase::new("EPIC-2".to_string()));
        storage.save_tasks(&tasks).unwrap();

        storage.set_active_group("EPIC-1").unwrap();
        assert_eq!(
            storage.get_active_group().unwrap(),
            Some("EPIC-1".to_string())
        );

        // Change active epic - should update cache
        storage.set_active_group("EPIC-2").unwrap();
        assert_eq!(
            storage.get_active_group().unwrap(),
            Some("EPIC-2".to_string())
        );
    }

    #[test]
    fn test_cache_with_no_active_epic() {
        let (storage, _temp_dir) = create_test_storage();

        // Load when no active epic is set
        let active = storage.get_active_group().unwrap();
        assert_eq!(active, None);

        // Should cache the None value
        let active2 = storage.get_active_group().unwrap();
        assert_eq!(active2, None);
    }

    // ==================== Lazy Epic Loading Tests ====================

    #[test]
    fn test_load_single_epic_from_many() {
        let (storage, _temp_dir) = create_test_storage();

        // Create 50 epics
        let mut tasks = HashMap::new();
        for i in 0..50 {
            tasks.insert(format!("EPIC-{}", i), Phase::new(format!("EPIC-{}", i)));
        }
        storage.save_tasks(&tasks).unwrap();

        // Load single epic - should only deserialize that one
        let epic = storage.load_group("EPIC-25").unwrap();
        assert_eq!(epic.name, "EPIC-25");
    }

    #[test]
    fn test_load_epic_not_found() {
        let (storage, _temp_dir) = create_test_storage();

        let tasks = HashMap::new();
        storage.save_tasks(&tasks).unwrap();

        let result = storage.load_group("NONEXISTENT");
        assert!(result.is_err());
        assert!(result.unwrap_err().to_string().contains("not found"));
    }

    #[test]
    fn test_load_epic_matches_full_load() {
        let (storage, _temp_dir) = create_test_storage();

        let mut tasks = HashMap::new();
        let mut epic = Phase::new("TEST-1".to_string());
        epic.add_task(crate::models::Task::new(
            "task-1".to_string(),
            "Test".to_string(),
            "Desc".to_string(),
        ));
        tasks.insert("TEST-1".to_string(), epic.clone());
        storage.save_tasks(&tasks).unwrap();

        // Load via both methods
        let epic_lazy = storage.load_group("TEST-1").unwrap();
        let tasks_full = storage.load_tasks().unwrap();
        let epic_full = tasks_full.get("TEST-1").unwrap();

        // Should be identical
        assert_eq!(epic_lazy.name, epic_full.name);
        assert_eq!(epic_lazy.tasks.len(), epic_full.tasks.len());
    }

    #[test]
    fn test_load_active_epic() {
        let (storage, _temp_dir) = create_test_storage();

        let mut tasks = HashMap::new();
        let mut epic = Phase::new("ACTIVE-1".to_string());
        epic.add_task(crate::models::Task::new(
            "task-1".to_string(),
            "Test".to_string(),
            "Desc".to_string(),
        ));
        tasks.insert("ACTIVE-1".to_string(), epic);
        storage.save_tasks(&tasks).unwrap();
        storage.set_active_group("ACTIVE-1").unwrap();

        // Load active epic directly
        let epic = storage.load_active_group().unwrap();
        assert_eq!(epic.name, "ACTIVE-1");
        assert_eq!(epic.tasks.len(), 1);
    }

    #[test]
    fn test_load_active_epic_when_none_set() {
        let (storage, _temp_dir) = create_test_storage();

        // Should error when no active epic
        let result = storage.load_active_group();
        assert!(result.is_err());
        assert!(result
            .unwrap_err()
            .to_string()
            .contains("No active task group"));
    }

    #[test]
    fn test_update_epic_without_loading_all() {
        let (storage, _temp_dir) = create_test_storage();

        let mut tasks = HashMap::new();
        tasks.insert("EPIC-1".to_string(), Phase::new("EPIC-1".to_string()));
        tasks.insert("EPIC-2".to_string(), Phase::new("EPIC-2".to_string()));
        storage.save_tasks(&tasks).unwrap();

        // Update only EPIC-1
        let mut epic1 = storage.load_group("EPIC-1").unwrap();
        epic1.add_task(crate::models::Task::new(
            "new-task".to_string(),
            "New".to_string(),
            "Desc".to_string(),
        ));
        storage.update_group("EPIC-1", &epic1).unwrap();

        // Verify update
        let loaded = storage.load_group("EPIC-1").unwrap();
        assert_eq!(loaded.tasks.len(), 1);

        // Verify EPIC-2 unchanged
        let epic2 = storage.load_group("EPIC-2").unwrap();
        assert_eq!(epic2.tasks.len(), 0);
    }
}
</file>

<file path="scud-cli/src/config.rs">
use anyhow::{Context, Result};
use serde::{Deserialize, Serialize};
use std::fs;
use std::path::Path;

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Config {
    pub llm: LLMConfig,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct LLMConfig {
    pub provider: String,
    pub model: String,
    #[serde(default)]
    pub max_tokens: u32,
}

impl Default for Config {
    fn default() -> Self {
        Config {
            llm: LLMConfig {
                provider: "xai".to_string(),
                model: "grok-code-fast-1".to_string(),
                max_tokens: 4096,
            },
        }
    }
}

impl Config {
    pub fn load(path: &Path) -> Result<Self> {
        let content = fs::read_to_string(path)
            .with_context(|| format!("Failed to read config file: {}", path.display()))?;

        toml::from_str(&content)
            .with_context(|| format!("Failed to parse config file: {}", path.display()))
    }

    pub fn save(&self, path: &Path) -> Result<()> {
        let content = toml::to_string_pretty(self).context("Failed to serialize config to TOML")?;

        if let Some(parent) = path.parent() {
            fs::create_dir_all(parent).with_context(|| {
                format!("Failed to create config directory: {}", parent.display())
            })?;
        }

        fs::write(path, content)
            .with_context(|| format!("Failed to write config file: {}", path.display()))
    }

    pub fn api_key_env_var(&self) -> &str {
        match self.llm.provider.as_str() {
            "anthropic" => "ANTHROPIC_API_KEY",
            "xai" => "XAI_API_KEY",
            "openai" => "OPENAI_API_KEY",
            "openrouter" => "OPENROUTER_API_KEY",
            "claude-cli" => "NONE", // Claude CLI doesn't need API key
            _ => "API_KEY",
        }
    }

    pub fn requires_api_key(&self) -> bool {
        self.llm.provider != "claude-cli"
    }

    pub fn api_endpoint(&self) -> &str {
        match self.llm.provider.as_str() {
            "anthropic" => "https://api.anthropic.com/v1/messages",
            "xai" => "https://api.x.ai/v1/chat/completions",
            "openai" => "https://api.openai.com/v1/chat/completions",
            "openrouter" => "https://openrouter.ai/api/v1/chat/completions",
            _ => "https://api.anthropic.com/v1/messages",
        }
    }

    pub fn default_model_for_provider(provider: &str) -> &str {
        match provider {
            "xai" => "grok-code-fast-1",
            "anthropic" => "claude-sonnet-4-5-20250929",
            "openai" => "o3-mini",
            "openrouter" => "anthropic/claude-sonnet-4.5",
            "claude-cli" => "sonnet", // Claude CLI model names: sonnet, opus, haiku
            _ => "grok-code-fast-1",
        }
    }

    /// Get suggested models for a provider (for display in init)
    pub fn suggested_models_for_provider(provider: &str) -> Vec<&str> {
        match provider {
            "xai" => vec![
                "grok-code-fast-1",
                "grok-4-1-fast-reasoning",
                "grok-4-1-fast",
                "grok-3-fast",
            ],
            "anthropic" => vec![
                "claude-sonnet-4-5-20250929",
                "claude-opus-4-5-20251101",
                "claude-haiku-4-5-20251001",
                "claude-opus-4-1-20250805",
            ],
            "openai" => vec![
                "gpt-5.1",
                "gpt-5.1-mini",
                "o3-mini",
                "o3",
                "o4-mini",
                "gpt-4.1",
            ],
            "openrouter" => vec![
                "anthropic/claude-sonnet-4.5",
                "anthropic/claude-opus-4.5",
                "openai/o3-mini",
                "openai/gpt-4.1",
                "xai/grok-4-1-fast-reasoning",
            ],
            _ => vec![],
        }
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use tempfile::TempDir;

    #[test]
    fn test_default_config() {
        let config = Config::default();
        assert_eq!(config.llm.provider, "xai");
        assert_eq!(config.llm.model, "grok-code-fast-1");
        assert_eq!(config.llm.max_tokens, 4096);
    }

    #[test]
    fn test_api_key_env_vars() {
        let mut config = Config::default();

        config.llm.provider = "anthropic".to_string();
        assert_eq!(config.api_key_env_var(), "ANTHROPIC_API_KEY");

        config.llm.provider = "xai".to_string();
        assert_eq!(config.api_key_env_var(), "XAI_API_KEY");

        config.llm.provider = "openai".to_string();
        assert_eq!(config.api_key_env_var(), "OPENAI_API_KEY");
    }

    #[test]
    fn test_api_endpoints() {
        let mut config = Config::default();

        config.llm.provider = "anthropic".to_string();
        assert_eq!(
            config.api_endpoint(),
            "https://api.anthropic.com/v1/messages"
        );

        config.llm.provider = "xai".to_string();
        assert_eq!(
            config.api_endpoint(),
            "https://api.x.ai/v1/chat/completions"
        );

        config.llm.provider = "openai".to_string();
        assert_eq!(
            config.api_endpoint(),
            "https://api.openai.com/v1/chat/completions"
        );
    }

    #[test]
    fn test_save_and_load_config() {
        let temp_dir = TempDir::new().unwrap();
        let config_path = temp_dir.path().join("config.toml");

        let config = Config {
            llm: LLMConfig {
                provider: "xai".to_string(),
                model: "grok-code-fast-1".to_string(),
                max_tokens: 8192,
            },
        };

        config.save(&config_path).unwrap();
        assert!(config_path.exists());

        let loaded = Config::load(&config_path).unwrap();
        assert_eq!(loaded.llm.provider, "xai");
        assert_eq!(loaded.llm.model, "grok-code-fast-1");
        assert_eq!(loaded.llm.max_tokens, 8192);
    }

    #[test]
    fn test_default_models() {
        assert_eq!(
            Config::default_model_for_provider("xai"),
            "grok-code-fast-1"
        );
        assert_eq!(
            Config::default_model_for_provider("anthropic"),
            "claude-sonnet-4-5-20250929"
        );
        assert_eq!(Config::default_model_for_provider("openai"), "o3-mini");
    }
}
</file>

<file path="scud-cli/src/lib.rs">
// Library entry point for SCUD
// This allows testing and using SCUD as a library

pub mod commands;
pub mod config;
pub mod formats;
pub mod llm;
pub mod models;
pub mod storage;
</file>

<file path="scud-cli/src/main.rs">
use anyhow::Result;
use clap::{Parser, Subcommand};
use scud::commands;
use std::path::PathBuf;

#[derive(Subcommand)]
enum ConfigCommands {
    /// Show current configuration
    Show,

    /// Set LLM provider
    SetProvider {
        /// Provider name (xai, anthropic, openai, openrouter)
        provider: String,

        /// Optional model name (defaults to provider's default)
        #[arg(short, long)]
        model: Option<String>,
    },

    /// Manage SCUD workflow agents (Claude Code slash commands)
    Agents {
        #[command(subcommand)]
        command: AgentsCommands,
    },
}

#[derive(Subcommand)]
enum AgentsCommands {
    /// List installed SCUD agents
    List,

    /// Add SCUD agent(s) to the project
    Add {
        /// Agent name (pm, sm, architect, dev, retrospective, status) or use --all
        name: Option<String>,

        /// Add all SCUD agents
        #[arg(long)]
        all: bool,
    },

    /// Remove SCUD agent(s) from the project
    Remove {
        /// Agent name (pm, sm, architect, dev, retrospective, status) or use --all
        name: Option<String>,

        /// Remove all SCUD agents
        #[arg(long)]
        all: bool,
    },
}

#[derive(Parser)]
#[command(name = "scud")]
#[command(about = "Fast, simple task master for AI-driven development", long_about = None)]
#[command(version)]
struct Cli {
    #[command(subcommand)]
    command: Commands,

    /// Project root directory
    #[arg(short, long, global = true)]
    project: Option<PathBuf>,
}

#[derive(Subcommand)]
enum Commands {
    /// Initialize SCUD in current directory
    Init {
        /// LLM provider to use (xai, anthropic, openai, openrouter)
        #[arg(long)]
        provider: Option<String>,
    },

    /// List phase tags or set active tag
    Tags {
        /// Tag to set as active (lists tags if not provided)
        tag: Option<String>,
    },

    /// List tasks in active phase
    List {
        /// Filter by status
        #[arg(short, long)]
        status: Option<String>,

        /// Phase tag (uses active phase if not provided)
        #[arg(short, long)]
        tag: Option<String>,

        /// Output as JSON instead of SCG format
        #[arg(long)]
        json: bool,
    },

    /// Show detailed task information
    Show {
        /// Task ID
        task_id: String,

        /// Phase tag (uses active phase if not provided)
        #[arg(short, long)]
        tag: Option<String>,
    },

    /// Update task status
    SetStatus {
        /// Task ID
        task_id: String,
        /// New status
        status: String,

        /// Phase tag (uses active phase if not provided)
        #[arg(short, long)]
        tag: Option<String>,
    },

    /// Find next available task (EXPERIMENTAL: use --claim for dynamic-wave mode)
    Next {
        /// Phase tag (uses active phase if not provided)
        #[arg(short, long)]
        tag: Option<String>,

        /// [EXPERIMENTAL] Auto-claim the task for the specified agent
        #[arg(long, requires = "name")]
        claim: bool,

        /// Agent/developer name (required with --claim)
        #[arg(short, long)]
        name: Option<String>,

        /// [EXPERIMENTAL] Release the currently claimed task for this agent
        #[arg(long, conflicts_with = "claim", requires = "name")]
        release: bool,

        /// Output machine-readable JSON for orchestrators
        #[arg(long)]
        spawn: bool,
    },

    /// Show phase statistics
    Stats {
        /// Phase tag (uses active phase if not provided)
        #[arg(short, long)]
        tag: Option<String>,
    },

    /// Migrate task data to new format (namespaced IDs, parent-child relationships)
    Migrate {
        /// Show what would change without making changes
        #[arg(long)]
        dry_run: bool,
    },

    /// Plan parallel execution waves based on task dependencies
    Waves {
        /// Phase tag (uses active phase if not provided)
        #[arg(short, long)]
        tag: Option<String>,

        /// Maximum parallel tasks per round (default: 5, min: 1)
        #[arg(short = 'n', long, default_value = "5")]
        max_parallel: usize,

        /// Plan across all phases
        #[arg(long)]
        all_tags: bool,
    },

    /// Configuration management
    Config {
        #[command(subcommand)]
        command: ConfigCommands,
    },

    /// Parse PRD/phase markdown into tasks (AI-powered)
    ParsePrd {
        /// Path to PRD/phase markdown file
        file: PathBuf,

        /// Phase tag to create
        #[arg(short, long)]
        tag: String,

        /// Number of tasks to generate (default: 10)
        #[arg(short = 'n', long, default_value = "10")]
        num_tasks: u32,
    },

    /// Analyze task complexity (AI-powered)
    AnalyzeComplexity {
        /// Specific task ID (analyzes all if not provided)
        #[arg(short = 'i', long)]
        task: Option<String>,

        /// Phase tag (uses active phase if not provided)
        #[arg(short, long)]
        tag: Option<String>,
    },

    /// Expand complex task into subtasks (AI-powered)
    Expand {
        /// Task ID to expand
        task_id: Option<String>,

        /// Expand all tasks with complexity > 13
        #[arg(short, long)]
        all: bool,

        /// Phase tag (uses active phase if not provided)
        #[arg(short, long)]
        tag: Option<String>,
    },

    /// Re-analyze and suggest cross-tag dependencies (AI-powered)
    ReanalyzeDeps {
        /// Tag to analyze (default: all tags)
        #[arg(short, long)]
        tag: Option<String>,

        /// Analyze all tags (default if no tag specified)
        #[arg(long)]
        all_tags: bool,

        /// Automatically apply suggestions without prompting
        #[arg(long)]
        apply: bool,

        /// Show suggestions without applying
        #[arg(long)]
        dry_run: bool,
    },

    // Task Assignment commands
    /// Assign task to a developer
    Assign {
        /// Task ID
        task_id: String,

        /// Assignee name
        assignee: String,

        /// Phase tag (uses active phase if not provided)
        #[arg(short, long)]
        tag: Option<String>,
    },

    /// Claim a task for yourself
    Claim {
        /// Task ID
        task_id: String,

        /// Your name/identifier
        #[arg(short, long)]
        name: String,

        /// Phase tag (uses active phase if not provided)
        #[arg(short = 'e', long)]
        tag: Option<String>,
    },

    /// Release task assignment/lock
    Release {
        /// Task ID
        task_id: String,

        /// Force release even if locked by someone else
        #[arg(short, long)]
        force: bool,

        /// Phase tag (uses active phase if not provided)
        #[arg(short = 'e', long)]
        tag: Option<String>,
    },

    /// Show who is working on what
    WhoIs {
        /// Phase tag (uses active phase if not provided)
        #[arg(short, long)]
        tag: Option<String>,
    },

    /// Get multiple ready tasks at once (for orchestrators)
    NextBatch {
        /// Phase tag (uses active phase if not provided)
        #[arg(short, long)]
        tag: Option<String>,

        /// Maximum number of tasks to return
        #[arg(short, long, default_value = "5")]
        limit: usize,
    },

    /// Show active task sessions (claimed/locked tasks)
    Sessions {
        /// Phase tag (checks all phases if not provided)
        #[arg(short, long)]
        tag: Option<String>,
    },

    /// Convert task storage format between JSON and SCG
    Convert {
        /// Source format (json, scg)
        #[arg(long)]
        from: String,

        /// Target format (json, scg)
        #[arg(long)]
        to: String,

        /// Create backup of source file (default: true)
        #[arg(long, default_value = "true")]
        backup: bool,
    },

    /// [EXPERIMENTAL] Diagnose stuck workflow states
    Doctor {
        /// Phase tag (checks all phases if not provided)
        #[arg(short, long)]
        tag: Option<String>,

        /// Stale lock threshold in hours (default: 24)
        #[arg(long, default_value = "24")]
        stale_hours: f64,

        /// Attempt auto-fix for recoverable issues
        #[arg(long)]
        fix: bool,
    },

    /// Generate Mermaid diagram of task graph
    Mermaid {
        /// Phase tag (uses active phase if not provided)
        #[arg(short, long)]
        tag: Option<String>,

        /// Include all phases in the diagram
        #[arg(long)]
        all_tags: bool,
    },

    /// Manage Claude Code hooks for automatic task completion
    Hooks {
        /// Action: install, uninstall, or status
        action: Option<String>,
    },

    /// Internal: Called by Claude Code Stop hook
    #[command(hide = true)]
    HookComplete,

    /// Quick orientation for new session (show recent commits, active sessions, next task)
    Warmup,

    /// Create a git commit with task context
    Commit {
        /// Commit message (uses task title if not provided)
        #[arg(short, long)]
        message: Option<String>,

        /// Stage all changes before committing
        #[arg(short, long)]
        all: bool,
    },
}

#[tokio::main]
async fn main() -> Result<()> {
    let cli = Cli::parse();

    match cli.command {
        Commands::Init { provider } => commands::init::run(cli.project, provider),
        Commands::Tags { tag } => commands::tags::run(cli.project, tag.as_deref()),
        Commands::List { status, tag, json } => {
            commands::list::run(cli.project, status.as_deref(), tag.as_deref(), json)
        }
        Commands::Show { task_id, tag } => {
            commands::show::run(cli.project, &task_id, tag.as_deref())
        }
        Commands::SetStatus {
            task_id,
            status,
            tag,
        } => commands::set_status::run(cli.project, &task_id, &status, tag.as_deref()),
        Commands::Next {
            tag,
            claim,
            name,
            release,
            spawn,
        } => commands::next::run(
            cli.project,
            tag.as_deref(),
            claim,
            name.as_deref(),
            release,
            spawn,
        ),
        Commands::Stats { tag } => commands::stats::run(cli.project, tag.as_deref()),
        Commands::Migrate { dry_run } => commands::migrate::run(cli.project, dry_run),
        Commands::Waves {
            tag,
            max_parallel,
            all_tags,
        } => commands::waves::run(cli.project, tag.as_deref(), max_parallel, all_tags),
        Commands::Config { command } => match command {
            ConfigCommands::Show => commands::config::show(cli.project),
            ConfigCommands::SetProvider { provider, model } => {
                commands::config::set_provider(cli.project, &provider, model)
            }
            ConfigCommands::Agents { command } => match command {
                AgentsCommands::List => commands::config::agents_list(cli.project),
                AgentsCommands::Add { name, all } => {
                    commands::config::agents_add(cli.project, name, all)
                }
                AgentsCommands::Remove { name, all } => {
                    commands::config::agents_remove(cli.project, name, all)
                }
            },
        },
        Commands::ParsePrd {
            file,
            tag,
            num_tasks,
        } => commands::ai::parse_prd::run(cli.project, &file, &tag, num_tasks).await,
        Commands::AnalyzeComplexity { task, tag } => {
            commands::ai::analyze_complexity::run(cli.project, task.as_deref(), tag.as_deref())
                .await
        }
        Commands::Expand { task_id, all, tag } => {
            commands::ai::expand::run(cli.project, task_id.as_deref(), all, tag.as_deref()).await
        }
        Commands::ReanalyzeDeps {
            tag,
            all_tags,
            apply,
            dry_run,
        } => {
            commands::ai::reanalyze_deps::run(cli.project, tag.as_deref(), all_tags, apply, dry_run)
                .await
        }
        Commands::Assign {
            task_id,
            assignee,
            tag,
        } => commands::assign::run(cli.project, &task_id, &assignee, tag.as_deref()),
        Commands::Claim { task_id, name, tag } => {
            commands::claim::run(cli.project, &task_id, &name, tag.as_deref())
        }
        Commands::Release {
            task_id,
            force,
            tag,
        } => commands::release::run(cli.project, &task_id, force, tag.as_deref()),
        Commands::WhoIs { tag } => commands::whois::run(cli.project, tag.as_deref()),
        Commands::NextBatch { tag, limit } => {
            commands::next_batch::run(cli.project, tag.as_deref(), limit)
        }
        Commands::Sessions { tag } => commands::sessions::run(cli.project, tag.as_deref()),
        Commands::Convert { from, to, backup } => {
            commands::convert::run(cli.project, &from, &to, backup)
        }
        Commands::Doctor {
            tag,
            stale_hours,
            fix,
        } => commands::doctor::run(cli.project, tag.as_deref(), stale_hours, fix),
        Commands::Mermaid { tag, all_tags } => {
            commands::mermaid::run(cli.project, tag.as_deref(), all_tags)
        }
        Commands::Hooks { action } => {
            commands::hooks::run(cli.project, action.as_deref().unwrap_or("status"))
        }
        Commands::HookComplete => commands::hook_complete::run(cli.project),
        Commands::Warmup => commands::warmup::run(cli.project),
        Commands::Commit { message, all } => {
            commands::commit::run(cli.project, message.as_deref(), all)
        }
    }
}
</file>

<file path="scud-cli/.gitignore">
# SCUD
.taskmaster/

# npm
*.tgz
node_modules/

# SCUD
.scud/
</file>

<file path="scud-cli/.npmignore">
# Rust debug build artifacts (keep release binary)
target/debug/
*.rlib
*.rmeta

# Source files (keep for users who want to build)
# src/ - keeping source for cargo build
# benches/ - not needed
benches/

# Development and test files
tests/
.taskmaster/
.claude/

# Documentation that's not needed for npm users
COVERAGE.md
TESTING.md
PROVIDERS.md
docs/

# Git files
.git/
.gitignore

# IDE files
.vscode/
.idea/
*.swp
*.swo

# OS files
.DS_Store
Thumbs.db

# Keep these files
!README.md
!package.json
!index.js
!install.js
!Cargo.toml
!Cargo.lock
</file>

<file path="scud-cli/Cargo.toml">
[package]
name = "scud-cli"
version = "1.19.3"
edition = "2021"
authors = ["SCUD Team"]
description = "Fast, simple task master for AI-driven development"
license = "MIT"

[lib]
name = "scud"
path = "src/lib.rs"

[[bin]]
name = "scud"
path = "src/main.rs"

[dependencies]
clap = { version = "4.5", features = ["derive", "cargo"] }
serde = { version = "1.0", features = ["derive"] }
serde_json = "1.0"
tokio = { version = "1.0", features = ["full"] }
reqwest = { version = "0.11", features = ["json", "rustls-tls"], default-features = false }
anyhow = "1.0"
thiserror = "1.0"
chrono = { version = "0.4", features = ["serde"] }
colored = "2.1"
indicatif = "0.17"
dirs = "5.0"
fs2 = "0.4"  # File locking for concurrent access
toml = "0.8"  # TOML parsing for config files
dialoguer = "0.11"  # Interactive CLI prompts
atty = "0.2"  # TTY detection for interactive mode
futures = "0.3"  # Async utilities for parallel execution

[dev-dependencies]
tempfile = "3.8"  # Temporary directories for tests
mockall = "0.12"  # Mock objects for testing
tokio-test = "0.4"  # Testing utilities for tokio
criterion = "0.5"  # Benchmarking framework
</file>

<file path="scud-cli/COVERAGE.md">
# Test Coverage Report

**Generated:** November 16, 2025
**Tool:** cargo-tarpaulin v0.34.1
**Overall Coverage:** 30.87% (360/1166 lines)

## Coverage Breakdown by Module

### Models (Excellent Coverage: 87.8%)

| Module | Coverage | Lines Covered | Total Lines | Status |
|--------|----------|--------------|-------------|---------|
| **workflow.rs** | 100.0% | 72/72 | 72 | âœ… Perfect |
| **task.rs** | 99.3% | 141/142 | 142 | âœ… Excellent |
| **epic.rs** | 97.0% | 32/33 | 33 | âœ… Excellent |
| **group.rs** | 30.8% | 8/26 | 26 | ðŸŸ¡ Needs tests |

**Models Total:** 253/273 lines (92.7%)

### Storage (Good Coverage: 79.9%)

| Module | Coverage | Lines Covered | Total Lines | Status |
|--------|----------|--------------|-------------|---------|
| **storage/mod.rs** | 79.9% | 107/134 | 134 | âœ… Good |

**Storage Total:** 107/134 lines (79.9%)

### Commands (No Coverage: 0%)

| Module | Coverage | Lines Covered | Total Lines | Status |
|--------|----------|--------------|-------------|---------|
| **init.rs** | 0% | 0/12 | 12 | âŒ No tests |
| **use_tag.rs** | 0% | 0/12 | 12 | âŒ No tests |
| **ai/research.rs** | 0% | 0/19 | 19 | âŒ No tests |
| **add_to_group.rs** | 0% | 0/19 | 19 | âŒ No tests |
| **assign.rs** | 0% | 0/19 | 19 | âŒ No tests |
| **tags.rs** | 0% | 0/22 | 22 | âŒ No tests |
| **set_status.rs** | 0% | 0/23 | 23 | âŒ No tests |
| **list_groups.rs** | 0% | 0/23 | 23 | âŒ No tests |
| **claim.rs** | 0% | 0/30 | 30 | âŒ No tests |
| **release.rs** | 0% | 0/30 | 30 | âŒ No tests |
| **list.rs** | 0% | 0/35 | 35 | âŒ No tests |
| **create_group.rs** | 0% | 0/36 | 36 | âŒ No tests |
| **next.rs** | 0% | 0/36 | 36 | âŒ No tests |
| **show.rs** | 0% | 0/37 | 37 | âŒ No tests |
| **stats.rs** | 0% | 0/44 | 44 | âŒ No tests |
| **whois.rs** | 0% | 0/45 | 45 | âŒ No tests |
| **group_status.rs** | 0% | 0/54 | 54 | âŒ No tests |
| **ai/parse_prd.rs** | 0% | 0/55 | 55 | âŒ No tests |
| **ai/analyze_complexity.rs** | 0% | 0/66 | 66 | âŒ No tests |
| **ai/expand.rs** | 0% | 0/91 | 91 | âŒ No tests |

**Commands Total:** 0/728 lines (0%)

### LLM (No Coverage: 0%)

| Module | Coverage | Lines Covered | Total Lines | Status |
|--------|----------|--------------|-------------|---------|
| **llm/prompts.rs** | 0% | 0/12 | 12 | âŒ No tests |
| **llm/client.rs** | 0% | 0/39 | 39 | âŒ No tests |

**LLM Total:** 0/51 lines (0%)

## Summary

### By Category

| Category | Coverage | Lines | Tested Lines | Status |
|----------|----------|-------|--------------|---------|
| Models | 92.7% | 273 | 253 | âœ… Excellent |
| Storage | 79.9% | 134 | 107 | âœ… Good |
| Commands | 0% | 728 | 0 | âŒ Needs integration tests |
| LLM | 0% | 51 | 0 | âŒ Needs mocking |
| **Overall** | **30.9%** | **1166** | **360** | ðŸŸ¡ **Good core coverage** |

### Test Count: 94 Tests

- Task Model: 36 tests
- Epic Model: 13 tests
- Workflow State: 18 tests
- Storage Layer: 27 tests
- Pass Rate: 100%

## Analysis

### Strengths âœ…

1. **Workflow State:** 100% coverage - Perfect implementation
2. **Task Model:** 99.3% coverage - Comprehensive testing
3. **Epic Model:** 97% coverage - Excellent coverage
4. **Storage Layer:** 79.9% coverage - Good error handling tests

### Gaps Identified ðŸŸ¡

1. **Commands:** 0% coverage (728 uncovered lines)
   - Need integration tests
   - CLI interface testing required
   - Mock LLM interactions needed

2. **LLM Module:** 0% coverage (51 uncovered lines)
   - Need mock Anthropic API
   - Test prompt generation
   - Test response parsing

3. **Epic Groups:** 30.8% coverage (18 uncovered lines)
   - Need group-specific tests
   - Group lifecycle tests
   - Group validation tests

## Next Steps to Improve Coverage

### High Priority (Target: 60% total coverage)

1. **Add Integration Tests for Commands** (~400 lines)
   - Create `scud-cli/tests/integration_test.rs`
   - Mock file system operations
   - Test CLI argument parsing
   - Test command execution flow
   - Estimated impact: +35% coverage

2. **Mock LLM Client Tests** (~50 lines)
   - Create mock Anthropic responses
   - Test prompt generation
   - Test response parsing
   - Estimated impact: +4% coverage

### Medium Priority (Target: 75% coverage)

3. **Epic Groups Tests** (~20 lines)
   - Group creation/deletion
   - Epic assignment to groups
   - Group validation
   - Estimated impact: +2% coverage

4. **Command Error Handling** (~100 lines)
   - Invalid input scenarios
   - Missing dependencies
   - File permission errors
   - Estimated impact: +9% coverage

### Low Priority (Target: 85%+)

5. **Property-Based Testing**
   - Use `proptest` crate
   - Generate random valid tasks/epics
   - Fuzz test validation logic

6. **End-to-End Tests**
   - Full workflow cycles
   - Real file system operations
   - Multi-user scenarios

## Coverage Goals

| Milestone | Target | Current | Gap | ETA |
|-----------|--------|---------|-----|-----|
| v0.0.3-beta | 60% | 30.9% | +29% | 1 week |
| v0.0.4-beta | 75% | 30.9% | +44% | 2 weeks |
| v1.0.0 | 85% | 30.9% | +54% | 3-4 weeks |

## How to Run Coverage

```bash
# Install tarpaulin
cargo install cargo-tarpaulin

# Generate coverage report
cargo tarpaulin --out Html --output-dir ../coverage

# View HTML report
open ../coverage/tarpaulin-report.html
```

## CI/CD Integration

Coverage reporting is configured in `.github/workflows/coverage.yml`:
- Runs on every push/PR
- Uploads to codecov.io
- Fails if coverage drops below threshold (currently disabled)

## Notes

- **30.9% overall coverage** is accurate for current state
- Core business logic (models + storage) has **excellent coverage (85%)**
- Commands are 0% covered because they need **integration tests**
- Coverage will significantly improve with command integration tests
- Current test suite is robust for the implemented test types

---

**Report Generated:** November 16, 2025
**Next Update:** After integration tests implementation
</file>

<file path="scud-cli/index.js">
#!/usr/bin/env node

const { spawn } = require('child_process');
const path = require('path');

const platform = process.platform;
const binaryName = platform === 'win32' ? 'scud.exe' : 'scud';
const binaryPath = path.join(__dirname, 'bin', binaryName);

const args = process.argv.slice(2);

const child = spawn(binaryPath, args, {
  stdio: 'inherit'
});

child.on('exit', (code) => {
  process.exit(code);
});
</file>

<file path="scud-cli/install.js">
#!/usr/bin/env node

const fs = require('fs');
const path = require('path');
const { execSync } = require('child_process');

const platform = process.platform;
const arch = process.arch;

console.log(`Installing SCUD for ${platform}-${arch}...`);

// Check if cargo is available
try {
  execSync('cargo --version', { stdio: 'ignore' });
} catch (error) {
  console.error('Error: Cargo (Rust toolchain) is not installed.');
  console.error('Please install Rust from https://rustup.rs/');
  process.exit(1);
}

// Build the Rust binary
try {
  console.log('Building SCUD from source...');
  execSync('cargo build --release', {
    stdio: 'inherit',
    cwd: __dirname
  });

  // Copy binary to bin directory
  const binDir = path.join(__dirname, 'bin');
  const binaryName = platform === 'win32' ? 'scud.exe' : 'scud';
  const sourcePath = path.join(__dirname, 'target', 'release', binaryName);
  const destPath = path.join(binDir, binaryName);

  // Verify source binary exists
  if (!fs.existsSync(sourcePath)) {
    throw new Error(`Compiled binary not found at: ${sourcePath}`);
  }

  // Create bin directory if it doesn't exist
  if (!fs.existsSync(binDir)) {
    fs.mkdirSync(binDir, { recursive: true });
  }

  // Remove any existing binary (in case of reinstall)
  if (fs.existsSync(destPath)) {
    fs.unlinkSync(destPath);
  }

  // Copy the compiled binary
  fs.copyFileSync(sourcePath, destPath);

  // Make executable on Unix-like systems
  if (platform !== 'win32') {
    fs.chmodSync(destPath, 0o755);
  }

  console.log(`âœ“ Binary installed at: ${destPath}`);

  console.log('âœ… SCUD installed successfully!');
  console.log(`Run 'scud --help' to get started.`);
} catch (error) {
  console.error('Error building SCUD:', error.message);
  process.exit(1);
}
</file>

<file path="scud-cli/package.json">
{
  "name": "scud-task",
  "version": "1.7.0",
  "description": "Fast, simple task master for AI-driven development - BMAD-TM workflow automation",
  "main": "index.js",
  "bin": {
    "scud": "./bin/scud.js"
  },
  "scripts": {
    "postinstall": "node install.js",
    "build": "cargo build --release",
    "test": "cargo test"
  },
  "keywords": [
    "task-management",
    "ai",
    "development",
    "workflow",
    "automation",
    "bmad-tm",
    "cli"
  ],
  "author": "SCUD Team",
  "license": "MIT",
  "repository": {
    "type": "git",
    "url": "https://github.com/pyrex41/scud.git"
  },
  "engines": {
    "node": ">=14.0.0"
  },
  "os": [
    "darwin",
    "linux",
    "win32"
  ],
  "files": [
    "bin/scud.js",
    "install.js",
    "index.js",
    "README.md",
    "Cargo.toml",
    "Cargo.lock",
    "src/"
  ]
}
</file>

<file path="scud-cli/PROVIDERS.md">
# LLM Provider Configuration

SCUD supports multiple LLM providers. You can configure your preferred provider during initialization or by editing the config file.

## Supported Providers

### xAI (Grok)
**Default Model:** `grok-code-fast-1`

```bash
# Initialize with xAI
scud init --provider xai

# Set API key
export XAI_API_KEY=your-xai-api-key
```

### Anthropic (Claude)
**Default Model:** `claude-sonnet-4-20250514`

```bash
# Initialize with Anthropic
scud init --provider anthropic

# Set API key
export ANTHROPIC_API_KEY=sk-ant-your-key
```

### OpenAI (GPT)
**Default Model:** `gpt-4-turbo`

```bash
# Initialize with OpenAI
scud init --provider openai

# Set API key
export OPENAI_API_KEY=sk-your-key
```

### OpenRouter
**Default Model:** `anthropic/claude-sonnet-4`

```bash
# Initialize with OpenRouter
scud init --provider openrouter

# Set API key
export OPENROUTER_API_KEY=sk-or-your-key
```

## Configuration File

The configuration is stored in `.taskmaster/config.toml`:

```toml
[llm]
provider = "xai"
model = "grok-code-fast-1"
max_tokens = 4096
```

## Changing Providers

To change providers after initialization:

1. Edit `.taskmaster/config.toml`
2. Update the `provider` and `model` fields
3. Set the appropriate API key environment variable

## Interactive Mode

If you don't specify a provider, SCUD will prompt you interactively:

```bash
scud init
# Select your LLM provider:
# > xAI (Grok)
#   Anthropic (Claude)
#   OpenAI (GPT)
#   OpenRouter
```

## API Endpoints

Each provider uses a different API endpoint:

- **xAI:** `https://api.x.ai/v1/chat/completions`
- **Anthropic:** `https://api.anthropic.com/v1/messages`
- **OpenAI:** `https://api.openai.com/v1/chat/completions`
- **OpenRouter:** `https://openrouter.ai/api/v1/chat/completions`

## Custom Models

You can override the default model by editing the config file:

```toml
[llm]
provider = "xai"
model = "grok-2-latest"  # Use a different model
max_tokens = 8192        # Increase token limit
```

## Environment Variables

SCUD reads API keys from environment variables for security:

| Provider | Environment Variable |
|----------|---------------------|
| xAI | `XAI_API_KEY` |
| Anthropic | `ANTHROPIC_API_KEY` |
| OpenAI | `OPENAI_API_KEY` |
| OpenRouter | `OPENROUTER_API_KEY` |

Set these in your shell profile (`.bashrc`, `.zshrc`, etc.) for persistence:

```bash
# Add to ~/.zshrc or ~/.bashrc
export XAI_API_KEY="your-key-here"
```

## Troubleshooting

### Authentication Error
```
Error: XAI_API_KEY environment variable not set
```

**Solution:** Set the appropriate API key for your provider.

### Invalid Provider
```
Error: Invalid provider: xxx. Valid options: xai, anthropic, openai, openrouter
```

**Solution:** Use one of the supported provider names.

### API Error
```
xai API error (401 Unauthorized): {"error": {"message": "Invalid API key"}}
```

**Solution:** Verify your API key is correct and has not expired.
</file>

<file path="scud-cli/README.md">
# SCUD CLI (Rust)

Fast, simple task master for AI-driven development - Rust implementation.

## Overview

This is a high-performance Rust rewrite of the SCUD task management system. It replaces the external `task-master` CLI with a fast, single-binary solution that:

- âš¡ **50x faster** startup time (~10ms vs ~500ms)
- ðŸŽ¯ **42x token reduction** (~500 tokens vs ~21k tokens per operation)
- ðŸ“¦ **Simple distribution** - single binary, no dependencies
- ðŸ”§ **Direct LLM integration** - no MCP overhead

## Architecture

```
scud (Rust Binary)
â”œâ”€â”€ Core Commands (No AI - Instant)
â”‚   â”œâ”€â”€ init               # Initialize .taskmaster/
â”‚   â”œâ”€â”€ tags               # List tags
â”‚   â”œâ”€â”€ use-tag            # Switch active tag
â”‚   â”œâ”€â”€ list               # List tasks with filters
â”‚   â”œâ”€â”€ show               # Show task details
â”‚   â”œâ”€â”€ set-status         # Update task status
â”‚   â”œâ”€â”€ next               # Find next available task (--claim for dynamic-wave)
â”‚   â”œâ”€â”€ stats              # Show statistics
â”‚   â””â”€â”€ doctor             # [EXPERIMENTAL] Diagnose stuck states
â”‚
â”œâ”€â”€ AI Commands (Direct Anthropic API)
â”‚   â”œâ”€â”€ parse-prd          # Parse PRD markdown into tasks
â”‚   â”œâ”€â”€ analyze-complexity # Analyze task complexity
â”‚   â”œâ”€â”€ expand             # Break down complex tasks
â”‚   â””â”€â”€ research           # AI-powered research
â”‚
â””â”€â”€ Storage (SCG)
    â””â”€â”€ .scud/tasks/tasks.scg
```

## Building

### Development
```bash
cargo build
```

### Release (Optimized)
```bash
cargo build --release
```

## Usage

### Core Commands

```bash
# Initialize SCUD
scud init

# List tags
scud tags

# Switch to a tag
scud use-tag auth

# List tasks
scud list
scud list --status pending

# Show task details
scud show 3

# Update task status
scud set-status 3 in-progress

# Find next available task
scud next

# Show statistics
scud stats
```

### [EXPERIMENTAL] Dynamic-Wave Mode

Dynamic-wave mode allows agents to auto-claim tasks and maintain workflow health:

```bash
# Find and auto-claim the next available task
scud next --claim --name agent-1

# Release all tasks claimed by an agent
scud next --release --name agent-1
```

**IMPORTANT:** When using `--claim`, agents MUST run `scud set-status <id> done` when finishing a task. This ensures dependent tasks become unblocked.

### [EXPERIMENTAL] Doctor Command

Diagnose stuck workflow states:

```bash
# Check for issues in all tags
scud doctor

# Check specific tag with custom stale threshold
scud doctor --tag auth --stale-hours 12

# Auto-fix recoverable issues (stale locks, orphan tasks)
scud doctor --fix
```

The doctor command detects:
- Stale locks (tasks locked >24h by default)
- Tasks blocked by cancelled/missing dependencies
- Orphan in-progress tasks (not locked, stale)
- Missing active tag
- Corrupt storage files

### AI Commands

**Requires:** API key environment variable (see [Provider Configuration](#provider-configuration))

```bash
# Parse PRD into tasks
scud parse-prd docs/features/auth.md --tag auth

# Analyze complexity
scud analyze-complexity                # All tasks
scud analyze-complexity --task 5       # Specific task

# Expand complex tasks
scud expand 7                          # Specific task
scud expand --all                      # All tasks >13 complexity

# Research a topic
scud research "OAuth 2.0 best practices"
```

## Performance Comparison

| Operation | Old (task-master) | New (Rust) | Improvement |
|-----------|------------------|------------|-------------|
| Startup | ~500ms | ~10ms | **50x faster** |
| List tasks | ~100ms | ~5ms | **20x faster** |
| Parse PRD | ~3-5s | ~2-3s | ~40% faster |
| Token overhead | ~21k | ~500 | **42x reduction** |

## Provider Configuration

SCUD supports multiple LLM providers: **xAI (Grok)**, **Anthropic (Claude)**, **OpenAI (GPT)**, and **OpenRouter**.

### Quick Start

```bash
# Initialize with xAI (Grok) - recommended for fast code generation
scud init --provider xai
export XAI_API_KEY=your-key

# Or initialize with Anthropic (Claude)
scud init --provider anthropic
export ANTHROPIC_API_KEY=your-key

# Interactive mode - prompt for provider
scud init
```

### Configuration File

The configuration is stored in `.taskmaster/config.toml`:

```toml
[llm]
provider = "xai"
model = "grok-code-fast-1"
max_tokens = 4096
```

For complete provider documentation, see [PROVIDERS.md](./PROVIDERS.md).

### Supported Providers

| Provider | Environment Variable | Default Model |
|----------|---------------------|---------------|
| xAI | `XAI_API_KEY` | `grok-code-fast-1` |
| Anthropic | `ANTHROPIC_API_KEY` | `claude-sonnet-4-20250514` |
| OpenAI | `OPENAI_API_KEY` | `gpt-4-turbo` |
| OpenRouter | `OPENROUTER_API_KEY` | `anthropic/claude-sonnet-4` |

## Data Models

### Task
```rust
struct Task {
    id: String,
    title: String,
    description: String,
    status: TaskStatus,         // pending, in-progress, done, etc.
    complexity: u32,            // Fibonacci scale: 1,2,3,5,8,13,21
    priority: Priority,         // high, medium, low
    dependencies: Vec<String>,  // Task IDs this depends on
    details: Option<String>,    // Technical details
    test_strategy: Option<String>,
    complexity_analysis: Option<String>,
    created_at: Option<String>,
    updated_at: Option<String>,
}
```

### Phase
```rust
struct Phase {
    name: String,
    tasks: Vec<Task>,
}
```

### Config
```toml
[llm]
provider = "xai"
model = "grok-code-fast-1"
max_tokens = 4096
```

## LLM Integration

### Direct Anthropic API
- No MCP server overhead
- Simple HTTP requests
- Minimal token usage
- Fast response times

### Prompt Templates

Located in `src/llm/prompts.rs`:
- `parse_prd()` - Converts markdown to structured tasks
- `analyze_complexity()` - Scores task difficulty
- `expand_task()` - Breaks down complex tasks
- `research_topic()` - AI research assistant

## Integration with SCUD

The Rust CLI integrates seamlessly with the existing SCUD system:

1. `bin/scud.js` detects and delegates to Rust binary
2. Falls back to debug build if release not available
3. Auto-builds if binary not found
4. All agents and slash commands work unchanged

## Development

### Project Structure

```
scud-cli/
â”œâ”€â”€ Cargo.toml
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ main.rs              # CLI entry point
â”‚   â”œâ”€â”€ commands/
â”‚   â”‚   â”œâ”€â”€ mod.rs
â”‚   â”‚   â”œâ”€â”€ init.rs          # Core commands
â”‚   â”‚   â”œâ”€â”€ tags.rs
â”‚   â”‚   â”œâ”€â”€ ...
â”‚   â”‚   â””â”€â”€ ai/              # AI commands
â”‚   â”‚       â”œâ”€â”€ parse_prd.rs
â”‚   â”‚       â”œâ”€â”€ analyze_complexity.rs
â”‚   â”‚       â”œâ”€â”€ expand.rs
â”‚   â”‚       â””â”€â”€ research.rs
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ task.rs
â”‚   â”‚   â””â”€â”€ phase.rs
â”‚   â”œâ”€â”€ storage/
â”‚   â”‚   â””â”€â”€ mod.rs           # JSON I/O
â”‚   â””â”€â”€ llm/
â”‚       â”œâ”€â”€ client.rs        # Anthropic API
â”‚       â””â”€â”€ prompts.rs       # Prompt templates
```

### Adding New Commands

1. Add command to `Commands` enum in `main.rs`
2. Create handler in `src/commands/`
3. Add to `rustCommands` array in `bin/scud.js`
4. Update help text

### Adding New LLM Prompts

1. Add prompt function to `src/llm/prompts.rs`
2. Create command handler in `src/commands/ai/`
3. Use `LLMClient::complete()` or `complete_json()`

## Testing

```bash
# Build and test
cargo build
cargo test

# Test specific command
cargo run -- init
cargo run -- tags
cargo run -- --help

# Test AI commands (requires API key)
export ANTHROPIC_API_KEY=sk-...
cargo run -- parse-prd test.md --tag test
```

## Distribution

### As Standalone Binary

```bash
cargo build --release
# Binary: target/release/scud
# Copy to /usr/local/bin or similar
```

### As Part of npm Package

The SCUD npm package includes the Rust binary:
- Pre-built binaries for major platforms
- Auto-built on first use if needed
- Seamless integration via `bin/scud.js`

## Future Enhancements

- [ ] Cross-compilation for multiple platforms
- [ ] Pre-built binaries in npm package
- [ ] Configuration file support
- [ ] Additional LLM providers (OpenAI, etc.)
- [ ] Offline mode for core commands
- [ ] Task export/import
- [ ] Custom prompt templates
- [ ] Parallel task execution analysis
- [ ] Integration tests with real API calls

## License

MIT

## Contributing

See main SCUD repository for contribution guidelines.
</file>

<file path="scud-cli/TESTING.md">
# SCUD Testing Guide

This document explains how to run and write tests for the SCUD Rust CLI.

## Table of Contents

1. [Running Tests](#running-tests)
2. [Test Structure](#test-structure)
3. [Writing New Tests](#writing-new-tests)
4. [Test Coverage](#test-coverage)
5. [Continuous Integration](#continuous-integration)

---

## Running Tests

### Run All Tests

```bash
cd scud-cli
cargo test
```

### Run Specific Test Module

```bash
# Task model tests only
cargo test --lib models::task::tests

# Epic model tests only
cargo test --lib models::epic::tests

# All model tests
cargo test --lib models
```

### Run Specific Test

```bash
cargo test --lib test_circular_dependency_self_reference
```

### Run Tests with Output

```bash
# Show println! output from passing tests
cargo test -- --nocapture

# Show output only from failing tests (default)
cargo test
```

### Run Tests in Release Mode

```bash
cargo test --release
```

---

## Test Structure

### Unit Tests

Unit tests are located in the same file as the code they test, in a `#[cfg(test)]` module:

```rust
// src/models/task.rs

impl Task {
    pub fn claim(&mut self, assignee: &str) -> Result<(), String> {
        // implementation
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_task_claim_success() {
        let mut task = Task::new("TASK-1".to_string(), "Test".to_string(), "Desc".to_string());
        let result = task.claim("alice");
        assert!(result.is_ok());
    }
}
```

### Integration Tests

Integration tests are in the `tests/` directory:

```
scud-cli/
â”œâ”€â”€ src/
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ integration_test.rs      # Command integration tests
â”‚   â”œâ”€â”€ error_handling_test.rs   # Error scenario tests
â”‚   â””â”€â”€ fixtures/                 # Test data
â”‚       â”œâ”€â”€ sample_prd.md
â”‚       â””â”€â”€ complex_epic.json
```

---

## Writing New Tests

### Test Naming Convention

- Test functions start with `test_`
- Use descriptive names: `test_task_claim_already_locked_by_different_user`
- Group related tests with common prefix: `test_circular_dependency_*`

### Assertion Macros

```rust
// Equality
assert_eq!(actual, expected);
assert_ne!(actual, not_expected);

// Boolean
assert!(condition);
assert!(!condition);

// Result types
assert!(result.is_ok());
assert!(result.is_err());

// Option types
assert!(option.is_some());
assert!(option.is_none());

// String contains
assert!(string.contains("substring"));
```

### Testing Async Code

```rust
#[tokio::test]
async fn test_async_function() {
    let result = some_async_function().await;
    assert!(result.is_ok());
}
```

### Using Test Fixtures

```rust
use std::fs;

#[test]
fn test_parse_prd() {
    let prd_content = fs::read_to_string("tests/fixtures/sample_prd.md").unwrap();
    // Test PRD parsing
}
```

### Testing Error Cases

```rust
#[test]
fn test_invalid_input_returns_error() {
    let result = function_that_should_fail("invalid input");

    assert!(result.is_err());
    assert_eq!(result.unwrap_err(), "Expected error message");
}
```

### Testing Panics

```rust
#[test]
#[should_panic(expected = "task ID cannot be empty")]
fn test_empty_task_id_panics() {
    Task::new("".to_string(), "Title".to_string(), "Desc".to_string());
}
```

---

## Test Coverage

### Measuring Coverage

Install `cargo-tarpaulin`:

```bash
cargo install cargo-tarpaulin
```

Generate coverage report:

```bash
cd scud-cli
cargo tarpaulin --out Html --output-dir ../coverage
```

View report:

```bash
open ../coverage/index.html
```

### Coverage Goals

- **Unit tests**: 80%+ coverage for core logic
- **Integration tests**: All commands tested end-to-end
- **Error handling**: 90%+ coverage for error paths

### Current Coverage

Run to see current coverage:

```bash
cd scud-cli
cargo tarpaulin
```

Example output:
```
|| Tested/Total Lines:
|| src/models/task.rs: 95.2% (120/126)
|| src/models/epic.rs: 92.5% (74/80)
||
|| Total: 87.3% (1234/1413)
```

---

## Continuous Integration

### GitHub Actions

Tests run automatically on:
- Every push to `master` or `main`
- Every pull request

Workflows:
- **`.github/workflows/test.yml`** - Runs tests on Ubuntu and macOS
- **`.github/workflows/coverage.yml`** - Generates coverage reports

### CI Commands

The CI runs:

```bash
# Lint
cargo clippy --all-targets --all-features -- -D warnings

# Format check
cargo fmt -- --check

# Tests
cargo test --all-features

# Build
cargo build --release
```

### Local CI Simulation

Run the same checks locally before pushing:

```bash
./scripts/ci-check.sh
```

Or manually:

```bash
cd scud-cli

# 1. Lint
cargo clippy --all-targets --all-features -- -D warnings

# 2. Format
cargo fmt --check

# 3. Test
cargo test

# 4. Build
cargo build --release
```

---

## Test Categories

### Unit Tests (37 tests)

**Task Model (24 tests):**
- Task creation and defaults
- Status transitions
- Task assignment and claiming
- Lock management and stale detection
- Dependency resolution
- Circular dependency detection
- Serialization

**Epic Model (13 tests):**
- Epic creation and task management
- Statistics calculation
- Next task finder (dependency-aware)
- Task expansion detection
- Serialization

### Integration Tests (Planned)

**Command Tests:**
- `init` - Directory structure creation
- `parse-prd` - PRD to tasks (with mock LLM)
- `list` - Task filtering
- `next` - Dependency-aware next task
- `set-status` - Status transitions
- `claim/release` - Task locking
- `stats` - Statistics calculation

**Error Handling Tests:**
- Invalid task IDs
- Missing files
- Malformed JSON
- Concurrent access conflicts

---

## Debugging Tests

### Run Single Test with Output

```bash
cargo test test_name -- --nocapture
```

### Show All Test Names

```bash
cargo test -- --list
```

### Run Tests Matching Pattern

```bash
# All circular dependency tests
cargo test circular_dependency

# All task claim tests
cargo test task_claim
```

### Ignore Slow Tests

```rust
#[test]
#[ignore]
fn test_slow_operation() {
    // This test is skipped by default
}
```

Run ignored tests:

```bash
cargo test -- --ignored

# Run all tests including ignored
cargo test -- --include-ignored
```

---

## Best Practices

### 1. Test One Thing Per Test

âŒ Bad:
```rust
#[test]
fn test_task() {
    let task = Task::new(...);
    assert_eq!(task.status, TaskStatus::Pending);
    task.claim("alice").unwrap();
    assert!(task.is_locked());
    task.set_status(TaskStatus::Done);
    // Testing too many things
}
```

âœ… Good:
```rust
#[test]
fn test_task_creation_defaults_to_pending() {
    let task = Task::new(...);
    assert_eq!(task.status, TaskStatus::Pending);
}

#[test]
fn test_task_claim_locks_task() {
    let mut task = Task::new(...);
    task.claim("alice").unwrap();
    assert!(task.is_locked());
}
```

### 2. Use Descriptive Test Names

The test name should describe what it's testing:

```rust
#[test]
fn test_task_claim_already_locked_by_different_user() {
    // Test name explains the scenario
}
```

### 3. Arrange-Act-Assert Pattern

```rust
#[test]
fn test_example() {
    // Arrange: Set up test data
    let mut task = Task::new("TASK-1".to_string(), "Test".to_string(), "Desc".to_string());
    task.claim("alice").unwrap();

    // Act: Perform the action
    let result = task.claim("bob");

    // Assert: Verify the outcome
    assert!(result.is_err());
    assert_eq!(result.unwrap_err(), "Task is locked by alice");
}
```

### 4. Test Edge Cases

Always test:
- Empty inputs
- Null/None values
- Maximum values
- Invalid inputs
- Concurrent access
- Missing dependencies

### 5. Keep Tests Fast

- Avoid `sleep()` in tests
- Use mocks for external dependencies
- Use in-memory data instead of files when possible

---

## Common Issues

### Tests Pass Locally But Fail in CI

Check for:
- Platform-specific code (Unix vs Windows)
- Hardcoded paths
- Race conditions
- Time zone dependencies

### Flaky Tests

If a test sometimes passes and sometimes fails:
- Look for timing issues
- Check for proper cleanup
- Verify no shared state between tests
- Use `cargo test -- --test-threads=1` to run serially

### Slow Test Suite

- Run tests in parallel (default)
- Use `--release` for heavy tests
- Mock expensive operations (API calls, file I/O)
- Use `#[ignore]` for very slow tests

---

## Resources

- [Rust Book - Testing](https://doc.rust-lang.org/book/ch11-00-testing.html)
- [Rust By Example - Testing](https://doc.rust-lang.org/rust-by-example/testing.html)
- [cargo-tarpaulin](https://github.com/xd009642/tarpaulin)
- [mockall crate](https://docs.rs/mockall/latest/mockall/)

---

## Summary

Quick reference:

```bash
# Run all tests
cargo test

# Run with output
cargo test -- --nocapture

# Run specific test
cargo test test_name

# Check coverage
cargo tarpaulin

# Lint
cargo clippy -- -D warnings

# Format
cargo fmt

# Format check
cargo fmt -- --check
```

**Current Test Stats:**
- âœ… 37 unit tests
- âœ… 100% passing
- âœ… ~87% estimated coverage
- âœ… CI/CD configured
- â³ Integration tests (planned)
</file>

<file path="scud-mcp/src/resources/stats.ts">
/**
 * Statistics resources - provides read-only access to phase statistics
 */

import type {
  ReadResourceRequest,
  ReadResourceResult,
  Resource,
} from '@modelcontextprotocol/sdk/types.js';
import { executeScudCommand } from '../utils/exec.js';

export const STATS_RESOURCES: Resource[] = [
  {
    uri: 'scud://stats/phase',
    name: 'Phase statistics',
    description: 'Read statistics for the active phase (task counts, complexity breakdown)',
    mimeType: 'text/plain',
  },
];

export async function handleStatsResource(
  request: ReadResourceRequest
): Promise<ReadResourceResult> {
  const { uri } = request.params;

  if (uri === 'scud://stats/phase') {
    try {
      const result = await executeScudCommand(['stats']);

      if (result.exitCode !== 0) {
        return {
          contents: [{
            uri,
            mimeType: 'text/plain',
            text: `Error reading stats: ${result.stderr || result.stdout}`,
          }],
        };
      }

      return {
        contents: [{
          uri,
          mimeType: 'text/plain',
          text: result.stdout,
        }],
      };
    } catch (error: any) {
      return {
        contents: [{
          uri,
          mimeType: 'text/plain',
          text: `Error reading stats: ${error.message}`,
        }],
      };
    }
  }

  throw new Error(`Unknown stats resource: ${uri}`);
}
</file>

<file path="scud-mcp/src/resources/tasks.ts">
/**
 * Tasks resources - provides read-only access to tasks
 * Uses scud CLI to read task data (SCG format is authoritative)
 */

import type {
  ReadResourceRequest,
  ReadResourceResult,
  Resource,
} from '@modelcontextprotocol/sdk/types.js';
import { executeScudCommand } from '../utils/exec.js';

export const TASK_RESOURCES: Resource[] = [
  {
    uri: 'scud://tasks/list',
    name: 'All tasks in active phase',
    description: 'Read all tasks for the currently active phase',
    mimeType: 'application/json',
  },
];

export async function handleTaskResource(
  request: ReadResourceRequest
): Promise<ReadResourceResult> {
  const { uri } = request.params;

  if (uri === 'scud://tasks/list') {
    try {
      // Use scud CLI to get task list (SCG is authoritative format)
      const result = await executeScudCommand(['list']);

      if (result.exitCode !== 0) {
        return {
          contents: [{
            uri,
            mimeType: 'text/plain',
            text: `Error reading tasks: ${result.stderr || result.stdout}`,
          }],
        };
      }

      return {
        contents: [{
          uri,
          mimeType: 'text/plain',
          text: result.stdout,
        }],
      };
    } catch (error: any) {
      return {
        contents: [{
          uri,
          mimeType: 'text/plain',
          text: `Error reading tasks: ${error.message}`,
        }],
      };
    }
  }

  throw new Error(`Unknown task resource: ${uri}`);
}
</file>

<file path="scud-mcp/src/tools/ai.ts">
/**
 * AI-powered tools - require ANTHROPIC_API_KEY
 */

import type {
  CallToolRequest,
  CallToolResult,
  Tool,
} from '@modelcontextprotocol/sdk/types.js';
import { executeScudCommand } from '../utils/exec.js';

export const AI_TOOLS: Tool[] = [
  {
    name: 'scud_parse_prd',
    description: 'Parse a PRD markdown file into tasks using AI. Requires ANTHROPIC_API_KEY environment variable.',
    inputSchema: {
      type: 'object',
      properties: {
        file: {
          type: 'string',
          description: 'Path to PRD markdown file (e.g., "docs/phases/phase-1-auth.md")',
        },
        tag: {
          type: 'string',
          description: 'Phase tag to create (e.g., "phase-1-auth")',
        },
      },
      required: ['file', 'tag'],
    },
  },
  {
    name: 'scud_analyze_complexity',
    description: 'Analyze task complexity using AI. Returns Fibonacci complexity score (1,2,3,5,8,13,21) with reasoning. Requires ANTHROPIC_API_KEY.',
    inputSchema: {
      type: 'object',
      properties: {
        task: {
          type: 'string',
          description: 'Specific task ID to analyze (analyzes all tasks if not provided)',
        },
      },
    },
  },
  {
    name: 'scud_expand',
    description: 'Break down complex tasks (>13 complexity) into smaller subtasks using AI. Requires ANTHROPIC_API_KEY.',
    inputSchema: {
      type: 'object',
      properties: {
        task_id: {
          type: 'string',
          description: 'Task ID to expand (expands all tasks >13 if not provided)',
        },
        all: {
          type: 'boolean',
          description: 'Expand all tasks with complexity > 13',
          default: false,
        },
      },
    },
  },
  {
    name: 'scud_research',
    description: 'Perform AI-powered research on a topic and save findings. Requires ANTHROPIC_API_KEY.',
    inputSchema: {
      type: 'object',
      properties: {
        query: {
          type: 'string',
          description: 'Research query or question',
        },
      },
      required: ['query'],
    },
  },
];

export async function handleAITool(
  request: CallToolRequest
): Promise<CallToolResult> {
  const { name, arguments: args } = request.params;

  // Check for API key
  if (!process.env.ANTHROPIC_API_KEY) {
    return {
      content: [{
        type: 'text',
        text: 'Error: ANTHROPIC_API_KEY environment variable not set. AI tools require this API key.',
      }],
      isError: true,
    };
  }

  switch (name) {
    case 'scud_parse_prd': {
      if (!args?.file || !args?.tag) {
        return {
          content: [{
            type: 'text',
            text: 'Error: file and tag are required',
          }],
          isError: true,
        };
      }

      const result = await executeScudCommand([
        'parse-prd',
        args.file as string,
        '--tag',
        args.tag as string,
      ]);

      if (result.exitCode !== 0) {
        return {
          content: [{
            type: 'text',
            text: `Error parsing PRD: ${result.stderr || result.stdout}`,
          }],
          isError: true,
        };
      }

      return {
        content: [{
          type: 'text',
          text: result.stdout,
        }],
      };
    }

    case 'scud_analyze_complexity': {
      const cmdArgs = ['analyze-complexity'];
      if (args?.task) {
        cmdArgs.push('--task', args.task as string);
      }

      const result = await executeScudCommand(cmdArgs);

      if (result.exitCode !== 0) {
        return {
          content: [{
            type: 'text',
            text: `Error analyzing complexity: ${result.stderr || result.stdout}`,
          }],
          isError: true,
        };
      }

      return {
        content: [{
          type: 'text',
          text: result.stdout,
        }],
      };
    }

    case 'scud_expand': {
      const cmdArgs = ['expand'];

      if (args?.task_id) {
        cmdArgs.push(args.task_id as string);
      } else if (args?.all) {
        cmdArgs.push('--all');
      }

      const result = await executeScudCommand(cmdArgs);

      if (result.exitCode !== 0) {
        return {
          content: [{
            type: 'text',
            text: `Error expanding tasks: ${result.stderr || result.stdout}`,
          }],
          isError: true,
        };
      }

      return {
        content: [{
          type: 'text',
          text: result.stdout,
        }],
      };
    }

    case 'scud_research': {
      if (!args?.query) {
        return {
          content: [{
            type: 'text',
            text: 'Error: query is required',
          }],
          isError: true,
        };
      }

      const result = await executeScudCommand([
        'research',
        args.query as string,
      ]);

      if (result.exitCode !== 0) {
        return {
          content: [{
            type: 'text',
            text: `Error performing research: ${result.stderr || result.stdout}`,
          }],
          isError: true,
        };
      }

      return {
        content: [{
          type: 'text',
          text: result.stdout,
        }],
      };
    }

    default:
      return {
        content: [{
          type: 'text',
          text: `Unknown AI tool: ${name}`,
        }],
        isError: true,
      };
  }
}
</file>

<file path="scud-mcp/src/tools/core.ts">
/**
 * Core SCUD tools - basic commands that don't require AI
 */

import type { Server } from '@modelcontextprotocol/sdk/server/index.js';
import type {
  CallToolRequest,
  CallToolResult,
  Tool,
} from '@modelcontextprotocol/sdk/types.js';
import { executeScudCommand } from '../utils/exec.js';

export const CORE_TOOLS: Tool[] = [
  {
    name: 'scud_init',
    description: 'Initialize SCUD in the current directory. Creates .scud/ directory structure.',
    inputSchema: {
      type: 'object',
      properties: {
        provider: {
          type: 'string',
          description: 'LLM provider to use (xai, anthropic, openai, openrouter). Defaults to anthropic.',
          enum: ['xai', 'anthropic', 'openai', 'openrouter'],
        },
      },
    },
  },
  {
    name: 'scud_list',
    description: 'List all tasks in the active phase. Optionally filter by status.',
    inputSchema: {
      type: 'object',
      properties: {
        status: {
          type: 'string',
          description: 'Filter by task status',
          enum: ['pending', 'in-progress', 'done', 'review', 'blocked', 'deferred', 'cancelled'],
        },
      },
    },
  },
  {
    name: 'scud_next',
    description: 'Find the next available task to work on. Respects dependencies and current status.',
    inputSchema: {
      type: 'object',
      properties: {},
    },
  },
  {
    name: 'scud_stats',
    description: 'Show statistics for the active phase (task counts, complexity breakdown).',
    inputSchema: {
      type: 'object',
      properties: {},
    },
  },
];

export async function handleCoreTool(
  request: CallToolRequest
): Promise<CallToolResult> {
  const { name, arguments: args } = request.params;

  switch (name) {
    case 'scud_init': {
      const cmdArgs = ['init'];
      if (args?.provider) {
        cmdArgs.push('--provider', args.provider as string);
      }

      const result = await executeScudCommand(cmdArgs);

      if (result.exitCode !== 0) {
        return {
          content: [{
            type: 'text',
            text: `Error initializing SCUD: ${result.stderr || result.stdout}`,
          }],
          isError: true,
        };
      }

      return {
        content: [{
          type: 'text',
          text: result.stdout || 'SCUD initialized successfully',
        }],
      };
    }

    case 'scud_list': {
      const cmdArgs = ['list'];
      if (args?.status) {
        cmdArgs.push('--status', args.status as string);
      }

      const result = await executeScudCommand(cmdArgs);

      if (result.exitCode !== 0) {
        return {
          content: [{
            type: 'text',
            text: `Error listing tasks: ${result.stderr || result.stdout}`,
          }],
          isError: true,
        };
      }

      return {
        content: [{
          type: 'text',
          text: result.stdout || 'No tasks found',
        }],
      };
    }

    case 'scud_next': {
      const result = await executeScudCommand(['next']);

      if (result.exitCode !== 0) {
        return {
          content: [{
            type: 'text',
            text: `Error finding next task: ${result.stderr || result.stdout}`,
          }],
          isError: true,
        };
      }

      return {
        content: [{
          type: 'text',
          text: result.stdout || 'No available tasks',
        }],
      };
    }

    case 'scud_stats': {
      const result = await executeScudCommand(['stats']);

      if (result.exitCode !== 0) {
        return {
          content: [{
            type: 'text',
            text: `Error getting stats: ${result.stderr || result.stdout}`,
          }],
          isError: true,
        };
      }

      return {
        content: [{
          type: 'text',
          text: result.stdout || 'No statistics available',
        }],
      };
    }

    default:
      return {
        content: [{
          type: 'text',
          text: `Unknown core tool: ${name}`,
        }],
        isError: true,
      };
  }
}
</file>

<file path="scud-mcp/src/tools/parallel.ts">
/**
 * Parallel development tools - phase groups and task assignments
 */

import type {
  CallToolRequest,
  CallToolResult,
  Tool,
} from '@modelcontextprotocol/sdk/types.js';
import { executeScudCommand } from '../utils/exec.js';

export const PARALLEL_TOOLS: Tool[] = [
  {
    name: 'scud_create_group',
    description: 'Create a phase group for parallel development across multiple phases.',
    inputSchema: {
      type: 'object',
      properties: {
        name: {
          type: 'string',
          description: 'Group name (e.g., "sprint-1")',
        },
        phases: {
          type: 'string',
          description: 'Comma-separated list of phase tags (e.g., "phase-1,phase-2,phase-3")',
        },
        description: {
          type: 'string',
          description: 'Optional group description',
        },
      },
      required: ['name', 'phases'],
    },
  },
  {
    name: 'scud_list_groups',
    description: 'List all phase groups.',
    inputSchema: {
      type: 'object',
      properties: {},
    },
  },
  {
    name: 'scud_group_status',
    description: 'Show status and progress for a phase group.',
    inputSchema: {
      type: 'object',
      properties: {
        group_id: {
          type: 'string',
          description: 'Group ID or name',
        },
      },
      required: ['group_id'],
    },
  },
  {
    name: 'scud_assign',
    description: 'Assign a task to a developer.',
    inputSchema: {
      type: 'object',
      properties: {
        task_id: {
          type: 'string',
          description: 'Task ID to assign',
        },
        assignee: {
          type: 'string',
          description: 'Developer name or username',
        },
      },
      required: ['task_id', 'assignee'],
    },
  },
  {
    name: 'scud_claim',
    description: 'Claim a task for yourself. Prevents others from working on it.',
    inputSchema: {
      type: 'object',
      properties: {
        task_id: {
          type: 'string',
          description: 'Task ID to claim',
        },
        name: {
          type: 'string',
          description: 'Your name or username',
        },
      },
      required: ['task_id', 'name'],
    },
  },
  {
    name: 'scud_release',
    description: 'Release a claimed task so others can work on it.',
    inputSchema: {
      type: 'object',
      properties: {
        task_id: {
          type: 'string',
          description: 'Task ID to release',
        },
        force: {
          type: 'boolean',
          description: 'Force release even if claimed by someone else',
          default: false,
        },
      },
      required: ['task_id'],
    },
  },
  {
    name: 'scud_whois',
    description: 'Show task assignments and who is working on what.',
    inputSchema: {
      type: 'object',
      properties: {},
    },
  },
];

export async function handleParallelTool(
  request: CallToolRequest
): Promise<CallToolResult> {
  const { name, arguments: args } = request.params;

  switch (name) {
    case 'scud_create_group': {
      if (!args?.name || !args?.phases) {
        return {
          content: [{
            type: 'text',
            text: 'Error: name and phases are required',
          }],
          isError: true,
        };
      }

      const cmdArgs = [
        'create-group',
        args.name as string,
        '--phases',
        args.phases as string,
      ];

      if (args.description) {
        cmdArgs.push('--description', args.description as string);
      }

      const result = await executeScudCommand(cmdArgs);

      if (result.exitCode !== 0) {
        return {
          content: [{
            type: 'text',
            text: `Error creating group: ${result.stderr || result.stdout}`,
          }],
          isError: true,
        };
      }

      return {
        content: [{
          type: 'text',
          text: result.stdout,
        }],
      };
    }

    case 'scud_list_groups': {
      const result = await executeScudCommand(['list-groups']);

      if (result.exitCode !== 0) {
        return {
          content: [{
            type: 'text',
            text: `Error listing groups: ${result.stderr || result.stdout}`,
          }],
          isError: true,
        };
      }

      return {
        content: [{
          type: 'text',
          text: result.stdout || 'No phase groups found',
        }],
      };
    }

    case 'scud_group_status': {
      if (!args?.group_id) {
        return {
          content: [{
            type: 'text',
            text: 'Error: group_id is required',
          }],
          isError: true,
        };
      }

      const result = await executeScudCommand(['group-status', args.group_id as string]);

      if (result.exitCode !== 0) {
        return {
          content: [{
            type: 'text',
            text: `Error getting group status: ${result.stderr || result.stdout}`,
          }],
          isError: true,
        };
      }

      return {
        content: [{
          type: 'text',
          text: result.stdout,
        }],
      };
    }

    case 'scud_assign': {
      if (!args?.task_id || !args?.assignee) {
        return {
          content: [{
            type: 'text',
            text: 'Error: task_id and assignee are required',
          }],
          isError: true,
        };
      }

      const result = await executeScudCommand([
        'assign',
        args.task_id as string,
        args.assignee as string,
      ]);

      if (result.exitCode !== 0) {
        return {
          content: [{
            type: 'text',
            text: `Error assigning task: ${result.stderr || result.stdout}`,
          }],
          isError: true,
        };
      }

      return {
        content: [{
          type: 'text',
          text: result.stdout || `Task ${args.task_id} assigned to ${args.assignee}`,
        }],
      };
    }

    case 'scud_claim': {
      if (!args?.task_id || !args?.name) {
        return {
          content: [{
            type: 'text',
            text: 'Error: task_id and name are required',
          }],
          isError: true,
        };
      }

      const result = await executeScudCommand([
        'claim',
        args.task_id as string,
        '--name',
        args.name as string,
      ]);

      if (result.exitCode !== 0) {
        return {
          content: [{
            type: 'text',
            text: `Error claiming task: ${result.stderr || result.stdout}`,
          }],
          isError: true,
        };
      }

      return {
        content: [{
          type: 'text',
          text: result.stdout || `Task ${args.task_id} claimed by ${args.name}`,
        }],
      };
    }

    case 'scud_release': {
      if (!args?.task_id) {
        return {
          content: [{
            type: 'text',
            text: 'Error: task_id is required',
          }],
          isError: true,
        };
      }

      const cmdArgs = ['release', args.task_id as string];
      if (args.force) {
        cmdArgs.push('--force');
      }

      const result = await executeScudCommand(cmdArgs);

      if (result.exitCode !== 0) {
        return {
          content: [{
            type: 'text',
            text: `Error releasing task: ${result.stderr || result.stdout}`,
          }],
          isError: true,
        };
      }

      return {
        content: [{
          type: 'text',
          text: result.stdout || `Task ${args.task_id} released`,
        }],
      };
    }

    case 'scud_whois': {
      const result = await executeScudCommand(['whois']);

      if (result.exitCode !== 0) {
        return {
          content: [{
            type: 'text',
            text: `Error getting assignments: ${result.stderr || result.stdout}`,
          }],
          isError: true,
        };
      }

      return {
        content: [{
          type: 'text',
          text: result.stdout || 'No task assignments found',
        }],
      };
    }

    default:
      return {
        content: [{
          type: 'text',
          text: `Unknown parallel tool: ${name}`,
        }],
        isError: true,
      };
  }
}
</file>

<file path="scud-mcp/src/tools/phase.ts">
/**
 * Phase management tools - working with phase tags
 */

import type {
  CallToolRequest,
  CallToolResult,
  Tool,
} from '@modelcontextprotocol/sdk/types.js';
import { executeScudCommand } from '../utils/exec.js';

export const PHASE_TOOLS: Tool[] = [
  {
    name: 'scud_tags',
    description: 'List all available phase tags in the project.',
    inputSchema: {
      type: 'object',
      properties: {},
    },
  },
  {
    name: 'scud_use_tag',
    description: 'Set the active phase tag to work with.',
    inputSchema: {
      type: 'object',
      properties: {
        tag: {
          type: 'string',
          description: 'The phase tag to activate (e.g., "phase-1-auth")',
        },
      },
      required: ['tag'],
    },
  },
];

export async function handlePhaseTool(
  request: CallToolRequest
): Promise<CallToolResult> {
  const { name, arguments: args } = request.params;

  switch (name) {
    case 'scud_tags': {
      const result = await executeScudCommand(['tags']);

      if (result.exitCode !== 0) {
        return {
          content: [{
            type: 'text',
            text: `Error listing tags: ${result.stderr || result.stdout}`,
          }],
          isError: true,
        };
      }

      return {
        content: [{
          type: 'text',
          text: result.stdout || 'No phase tags found',
        }],
      };
    }

    case 'scud_use_tag': {
      if (!args?.tag) {
        return {
          content: [{
            type: 'text',
            text: 'Error: tag is required',
          }],
          isError: true,
        };
      }

      const result = await executeScudCommand(['use-tag', args.tag as string]);

      if (result.exitCode !== 0) {
        return {
          content: [{
            type: 'text',
            text: `Error setting active tag: ${result.stderr || result.stdout}`,
          }],
          isError: true,
        };
      }

      return {
        content: [{
          type: 'text',
          text: result.stdout || `Active phase set to: ${args.tag}`,
        }],
      };
    }

    default:
      return {
        content: [{
          type: 'text',
          text: `Unknown phase tool: ${name}`,
        }],
        isError: true,
      };
  }
}
</file>

<file path="scud-mcp/src/tools/task.ts">
/**
 * Task operation tools - working with individual tasks
 */

import type {
  CallToolRequest,
  CallToolResult,
  Tool,
} from '@modelcontextprotocol/sdk/types.js';
import { executeScudCommand } from '../utils/exec.js';

export const TASK_TOOLS: Tool[] = [
  {
    name: 'scud_show',
    description: 'Show detailed information about a specific task.',
    inputSchema: {
      type: 'object',
      properties: {
        task_id: {
          type: 'string',
          description: 'The task ID to show details for (e.g., "TASK-1")',
        },
      },
      required: ['task_id'],
    },
  },
  {
    name: 'scud_set_status',
    description: 'Update the status of a task.',
    inputSchema: {
      type: 'object',
      properties: {
        task_id: {
          type: 'string',
          description: 'The task ID to update',
        },
        status: {
          type: 'string',
          description: 'New status for the task',
          enum: ['pending', 'in-progress', 'done', 'review', 'blocked', 'deferred', 'cancelled'],
        },
      },
      required: ['task_id', 'status'],
    },
  },
];

export async function handleTaskTool(
  request: CallToolRequest
): Promise<CallToolResult> {
  const { name, arguments: args } = request.params;

  switch (name) {
    case 'scud_show': {
      if (!args?.task_id) {
        return {
          content: [{
            type: 'text',
            text: 'Error: task_id is required',
          }],
          isError: true,
        };
      }

      const result = await executeScudCommand(['show', args.task_id as string]);

      if (result.exitCode !== 0) {
        return {
          content: [{
            type: 'text',
            text: `Error showing task: ${result.stderr || result.stdout}`,
          }],
          isError: true,
        };
      }

      return {
        content: [{
          type: 'text',
          text: result.stdout,
        }],
      };
    }

    case 'scud_set_status': {
      if (!args?.task_id || !args?.status) {
        return {
          content: [{
            type: 'text',
            text: 'Error: task_id and status are required',
          }],
          isError: true,
        };
      }

      const result = await executeScudCommand([
        'set-status',
        args.task_id as string,
        args.status as string,
      ]);

      if (result.exitCode !== 0) {
        return {
          content: [{
            type: 'text',
            text: `Error setting task status: ${result.stderr || result.stdout}`,
          }],
          isError: true,
        };
      }

      return {
        content: [{
          type: 'text',
          text: result.stdout || `Task ${args.task_id} status updated to ${args.status}`,
        }],
      };
    }

    default:
      return {
        content: [{
          type: 'text',
          text: `Unknown task tool: ${name}`,
        }],
        isError: true,
      };
  }
}
</file>

<file path="scud-mcp/src/utils/exec.ts">
/**
 * CLI execution wrapper for SCUD commands
 *
 * Uses execFile (not exec) to avoid shell injection vulnerabilities.
 * Arguments are passed as an array, not concatenated into a shell string.
 */

import { execFile } from 'child_process';
import { promisify } from 'util';
import type { ScudCommandResult } from '../types.js';

const execFileAsync = promisify(execFile);

export interface ExecOptions {
  cwd?: string;
  timeout?: number;
}

/**
 * Execute a SCUD CLI command and return the result
 *
 * Uses execFile to safely pass arguments without shell interpolation.
 * This prevents command injection and properly handles arguments with spaces.
 */
export async function executeScudCommand(
  args: string[],
  options?: ExecOptions
): Promise<ScudCommandResult> {
  try {
    // Use execFile with argument array - no shell interpolation
    const { stdout, stderr } = await execFileAsync('scud', args, {
      cwd: options?.cwd || process.cwd(),
      timeout: options?.timeout || 30000, // 30 second default timeout
      env: {
        ...process.env,
        // Inherit ANTHROPIC_API_KEY and other env vars
      },
      // Increase buffer size for large outputs
      maxBuffer: 10 * 1024 * 1024, // 10MB
    });

    return {
      stdout: stdout.trim(),
      stderr: stderr.trim(),
      exitCode: 0,
    };
  } catch (error: any) {
    return {
      stdout: error.stdout?.trim() || '',
      stderr: error.stderr?.trim() || error.message,
      exitCode: error.code || 1,
    };
  }
}

/**
 * Parse JSON output from SCUD command
 */
export function parseJsonOutput<T>(stdout: string): T {
  try {
    return JSON.parse(stdout);
  } catch (error) {
    throw new Error(`Failed to parse SCUD output as JSON: ${error}`);
  }
}

/**
 * Check if SCUD CLI is available in PATH
 */
export async function checkScudAvailable(): Promise<boolean> {
  try {
    const result = await executeScudCommand(['--version']);
    return result.exitCode === 0;
  } catch {
    return false;
  }
}

/**
 * Validate that a command succeeded
 */
export function ensureSuccess(result: ScudCommandResult, context: string): void {
  if (result.exitCode !== 0) {
    throw new Error(
      `SCUD command failed (${context}): ${result.stderr || result.stdout}`
    );
  }
}
</file>

<file path="scud-mcp/src/index.ts">
#!/usr/bin/env node

/**
 * SCUD MCP Server - Model Context Protocol server for SCUD task management
 *
 * This server wraps the SCUD CLI and exposes it through the MCP protocol,
 * enabling AI assistants like Claude to interact with SCUD task management.
 */

import { Server } from '@modelcontextprotocol/sdk/server/index.js';
import { StdioServerTransport } from '@modelcontextprotocol/sdk/server/stdio.js';
import {
  CallToolRequestSchema,
  ListToolsRequestSchema,
  ListResourcesRequestSchema,
  ReadResourceRequestSchema,
} from '@modelcontextprotocol/sdk/types.js';

// Import all tool handlers
import { CORE_TOOLS, handleCoreTool } from './tools/core.js';
import { PHASE_TOOLS, handlePhaseTool } from './tools/phase.js';
import { TASK_TOOLS, handleTaskTool } from './tools/task.js';
import { AI_TOOLS, handleAITool } from './tools/ai.js';
import { PARALLEL_TOOLS, handleParallelTool } from './tools/parallel.js';

// Import all resource handlers
import { TASK_RESOURCES, handleTaskResource } from './resources/tasks.js';
import { STATS_RESOURCES, handleStatsResource } from './resources/stats.js';

import { checkScudAvailable } from './utils/exec.js';

// Combine all tools
const ALL_TOOLS = [
  ...CORE_TOOLS,
  ...PHASE_TOOLS,
  ...TASK_TOOLS,
  ...AI_TOOLS,
  ...PARALLEL_TOOLS,
];

// Combine all resources
const ALL_RESOURCES = [
  ...TASK_RESOURCES,
  ...STATS_RESOURCES,
];

// Create MCP server
const server = new Server(
  {
    name: 'scud-mcp',
    version: '1.0.0',
  },
  {
    capabilities: {
      tools: {},
      resources: {},
    },
  }
);

// List available tools
server.setRequestHandler(ListToolsRequestSchema, async () => {
  return {
    tools: ALL_TOOLS,
  };
});

// Handle tool execution
server.setRequestHandler(CallToolRequestSchema, async (request) => {
  const toolName = request.params.name;

  // Route to appropriate handler based on tool name
  if (CORE_TOOLS.some(t => t.name === toolName)) {
    return handleCoreTool(request);
  }

  if (PHASE_TOOLS.some(t => t.name === toolName)) {
    return handlePhaseTool(request);
  }

  if (TASK_TOOLS.some(t => t.name === toolName)) {
    return handleTaskTool(request);
  }

  if (AI_TOOLS.some(t => t.name === toolName)) {
    return handleAITool(request);
  }

  if (PARALLEL_TOOLS.some(t => t.name === toolName)) {
    return handleParallelTool(request);
  }

  // Unknown tool
  return {
    content: [{
      type: 'text',
      text: `Unknown tool: ${toolName}`,
    }],
    isError: true,
  };
});

// List available resources
server.setRequestHandler(ListResourcesRequestSchema, async () => {
  return {
    resources: ALL_RESOURCES,
  };
});

// Handle resource reads
server.setRequestHandler(ReadResourceRequestSchema, async (request) => {
  const uri = request.params.uri;

  // Route to appropriate handler based on URI
  if (uri.startsWith('scud://tasks/')) {
    return handleTaskResource(request);
  }

  if (uri.startsWith('scud://stats/')) {
    return handleStatsResource(request);
  }

  // Unknown resource
  throw new Error(`Unknown resource URI: ${uri}`);
});

// Start server
async function main() {
  // Check if SCUD CLI is available
  const isAvailable = await checkScudAvailable();
  if (!isAvailable) {
    console.error('Error: SCUD CLI not found in PATH');
    console.error('Please install SCUD first: npm install -g scud');
    process.exit(1);
  }

  // Start MCP server with stdio transport
  const transport = new StdioServerTransport();
  await server.connect(transport);

  console.error('SCUD MCP server started successfully');
  console.error(`Exposing ${ALL_TOOLS.length} tools and ${ALL_RESOURCES.length} resources`);
}

main().catch((error) => {
  console.error('Fatal error starting SCUD MCP server:', error);
  process.exit(1);
});
</file>

<file path="scud-mcp/src/types.ts">
/**
 * TypeScript type definitions for SCUD MCP server
 */

export interface ScudCommandResult {
  stdout: string;
  stderr: string;
  exitCode: number;
}

export interface ScudTask {
  id: string;
  title: string;
  description: string;
  status: TaskStatus;
  priority: Priority;
  complexity: number;
  dependencies: string[];
  assigned_to?: string;
  locked_by?: string;
  locked_at?: string;
  created_at: string;
  updated_at: string;
  details?: string;
}

export type TaskStatus =
  | 'pending'
  | 'in-progress'
  | 'done'
  | 'review'
  | 'blocked'
  | 'deferred'
  | 'cancelled';

export type Priority = 'critical' | 'high' | 'medium' | 'low';

export interface ScudPhase {
  tag: string;
  tasks: ScudTask[];
}

export interface PhaseStats {
  total_tasks: number;
  by_status: Record<TaskStatus, number>;
  total_complexity: number;
  completed_complexity: number;
}

export interface PhaseGroup {
  id: string;
  name: string;
  description?: string;
  phase_tags: string[];
  created_at: string;
}
</file>

<file path="scud-mcp/.gitignore">
# Dependencies
node_modules/
package-lock.json

# Build output
dist/
*.tsbuildinfo

# Logs
*.log
npm-debug.log*

# Environment
.env
.env.local

# IDE
.vscode/
.idea/
*.swp
*.swo
*~

# OS
.DS_Store
Thumbs.db
</file>

<file path="scud-mcp/.npmignore">
# Source files (only ship dist/)
src/
tsconfig.json

# Development files
.git/
.gitignore
*.log

# Documentation (keep README)
# README.md is included automatically
</file>

<file path="scud-mcp/EXAMPLE_CONFIG.json">
{
  "mcpServers": {
    "scud": {
      "command": "scud-mcp",
      "env": {
        "ANTHROPIC_API_KEY": "sk-ant-your-api-key-here"
      }
    }
  }
}
</file>

<file path="scud-mcp/package.json">
{
  "name": "scud-mcp",
  "version": "1.8.0",
  "description": "Model Context Protocol server for SCUD task management",
  "type": "module",
  "bin": {
    "scud-mcp": "./dist/index.js"
  },
  "scripts": {
    "build": "tsc",
    "dev": "tsc --watch",
    "prepare": "npm run build"
  },
  "dependencies": {
    "@modelcontextprotocol/sdk": "^1.0.4"
  },
  "devDependencies": {
    "@types/node": "^20.11.0",
    "typescript": "^5.3.3"
  },
  "engines": {
    "node": ">=18.0.0"
  },
  "keywords": [
    "mcp",
    "model-context-protocol",
    "scud",
    "task-management",
    "ai",
    "workflow",
    "sprint",
    "agile"
  ],
  "author": "",
  "license": "MIT",
  "repository": {
    "type": "git",
    "url": "https://github.com/yourusername/bmad-tm"
  }
}
</file>

<file path="scud-mcp/README.md">
# SCUD MCP Server

Model Context Protocol (MCP) server for [SCUD](https://github.com/yourusername/bmad-tm) task management. Enables AI assistants like Claude Desktop to interact with SCUD's task management and AI-powered workflow features.

## Overview

**SCUD MCP** wraps the SCUD CLI and exposes it through the Model Context Protocol, allowing any MCP-compatible client to:
- Parse PRDs into tasks using AI
- Manage tags and task lifecycles
- Track workflow state and progress
- Coordinate parallel development across teams
- Analyze complexity and expand tasks automatically

Perfect for teams using Claude Desktop, Cline, or other MCP clients who want AI-powered task management integrated into their development workflow.

## Features

### ðŸ› ï¸ **20 MCP Tools** (Actions)

#### Core Commands
- `scud_init` - Initialize SCUD in current directory
- `scud_list` - List tasks (optionally filter by status)
- `scud_next` - Find next available task
- `scud_stats` - Show statistics
- `scud_show` - Show task details
- `scud_set_status` - Update task status

#### Tag Management
- `scud_tags` - List all tags
- `scud_use_tag` - Set active tag

#### AI-Powered (Requires ANTHROPIC_API_KEY)
- `scud_parse_prd` - Parse PRD markdown into tasks
- `scud_analyze_complexity` - Analyze task complexity (Fibonacci scale)
- `scud_expand` - Break down complex tasks into subtasks
- `scud_research` - AI-powered research

#### Parallel Development
- `scud_create_group` - Create tag group for parallel work
- `scud_list_groups` - List all tag groups
- `scud_group_status` - Show group progress
- `scud_assign` - Assign task to developer
- `scud_claim` - Claim task for yourself
- `scud_release` - Release claimed task
- `scud_whois` - Show task assignments

### ðŸ“Š **3 MCP Resources** (Read-only Data)
- `scud://workflow/state` - Current workflow state (JSON)
- `scud://tasks/list` - All tasks in active tag (JSON)
- `scud://stats/current` - Tag statistics (text)

## Installation

### Prerequisites

1. **Node.js 18+** - Required for MCP server
2. **SCUD CLI** - The Rust CLI must be installed and in PATH

```bash
# Install SCUD CLI (if not already installed)
npm install -g scud

# Verify installation
scud --version
```

### Install SCUD MCP Server

```bash
npm install -g scud-mcp
```

### Configure in Claude Desktop

Edit your Claude Desktop config file:

**macOS:** `~/Library/Application Support/Claude/claude_desktop_config.json`
**Windows:** `%APPDATA%\Claude\claude_desktop_config.json`
**Linux:** `~/.config/Claude/claude_desktop_config.json`

Add the SCUD MCP server:

```json
{
  "mcpServers": {
    "scud": {
      "command": "scud-mcp",
      "env": {
        "ANTHROPIC_API_KEY": "sk-ant-your-api-key-here"
      }
    }
  }
}
```

**Note:** The `ANTHROPIC_API_KEY` is only required for AI-powered tools (`scud_parse_prd`, `scud_analyze_complexity`, `scud_expand`, `scud_research`). Core task management works without it.

### Restart Claude Desktop

Restart Claude Desktop to load the MCP server. You should see the SCUD tools available in your conversations.

## Usage

### Example 1: Initialize Project and Parse PRD

**You (in Claude Desktop):**
> Initialize SCUD and parse my PRD file at docs/features/auth.md with tag auth

**Claude:**
```
[calls scud_init]
âœ“ Initialized SCUD successfully

[calls scud_parse_prd with file="docs/features/auth.md", tag="auth"]
âœ“ Parsed PRD into 12 tasks
âœ“ Active tag set to: auth
```

### Example 2: Development Workflow

**You:**
> What's my next task?

**Claude:**
```
[calls scud_next]
Next available task:

TASK-1: Set up authentication database schema
Complexity: 5
Priority: high
Status: pending
```

**You:**
> Mark TASK-1 as in-progress and show me the details

**Claude:**
```
[calls scud_set_status with task_id="TASK-1", status="in-progress"]
âœ“ Task TASK-1 updated to in-progress

[calls scud_show with task_id="TASK-1"]

TASK-1: Set up authentication database schema
Description: Create User and Session tables with proper indexes...
Complexity: 5 (Medium)
Priority: high
Status: in-progress
Dependencies: None
```

### Example 3: AI-Powered Complexity Analysis

**You:**
> Analyze the complexity of all tasks

**Claude:**
```
[calls scud_analyze_complexity]

Analyzing 12 tasks...

TASK-3: Implement OAuth2 flow
  Complexity: 21 â†’ Recommended: EXPAND
  Reasoning: Involves multiple components (auth provider integration, token management,
  refresh logic, error handling). Should be broken down.

TASK-7: Add login form UI
  Complexity: 3
  Reasoning: Simple form with email/password fields. Well-defined scope.

[... more analysis ...]
```

**You:**
> Expand TASK-3

**Claude:**
```
[calls scud_expand with task_id="TASK-3"]

Expanded TASK-3 into 5 subtasks:

TASK-3-1: Research OAuth2 providers (complexity: 2)
TASK-3-2: Implement authorization endpoint (complexity: 5)
TASK-3-3: Implement token endpoint (complexity: 5)
TASK-3-4: Add token refresh logic (complexity: 3)
TASK-3-5: Add OAuth2 error handling (complexity: 3)
```

### Example 4: Parallel Development

**You:**
> Create a sprint group with auth, api, and frontend tags

**Claude:**
```
[calls scud_create_group with name="sprint-1", tags="auth,api,frontend"]
âœ“ Created group: sprint-1

[calls scud_group_status with group_id="sprint-1"]

Sprint 1 Status:
  auth: 5/12 tasks done (42%)
  api: 2/8 tasks done (25%)
  frontend: 0/15 tasks done (0%)

Overall: 7/35 tasks completed (20%)
```

### Example 5: Using Resources

**You:**
> Read the current workflow state

**Claude:**
```
[reads scud://workflow/state resource]

{
  "active_tag": "auth",
  "current_phase": "development",
  "phases": {
    "planning": { "status": "completed", "completed_at": "2025-11-15T10:00:00Z" },
    "development": { "status": "in-progress", "started_at": "2025-11-15T14:00:00Z" }
  },
  "completed_epics": []
}
```

## Tool Reference

### Core Tools

| Tool | Parameters | Description |
|------|-----------|-------------|
| `scud_init` | - | Initialize SCUD in current directory |
| `scud_list` | `status?` | List tasks (filter by status) |
| `scud_next` | - | Find next available task |
| `scud_stats` | - | Show statistics |
| `scud_show` | `task_id` | Show task details |
| `scud_set_status` | `task_id`, `status` | Update task status |

### Tag Management

| Tool | Parameters | Description |
|------|-----------|-------------|
| `scud_tags` | - | List all tags |
| `scud_use_tag` | `tag` | Set active tag |

### AI Tools (Require ANTHROPIC_API_KEY)

| Tool | Parameters | Description |
|------|-----------|-------------|
| `scud_parse_prd` | `file`, `tag` | Parse PRD into tasks |
| `scud_analyze_complexity` | `task?` | Analyze task complexity |
| `scud_expand` | `task_id?`, `all?` | Expand complex tasks |
| `scud_research` | `query` | AI-powered research |

### Parallel Development

| Tool | Parameters | Description |
|------|-----------|-------------|
| `scud_create_group` | `name`, `tags`, `description?` | Create tag group |
| `scud_list_groups` | - | List all tag groups |
| `scud_group_status` | `group_id` | Show group status |
| `scud_assign` | `task_id`, `assignee` | Assign task to developer |
| `scud_claim` | `task_id`, `name` | Claim task for yourself |
| `scud_release` | `task_id`, `force?` | Release claimed task |
| `scud_whois` | - | Show task assignments |

### Resource URIs

| Resource | Description | Format |
|----------|-------------|--------|
| `scud://workflow/state` | Current workflow state | JSON |
| `scud://tasks/list` | All tasks in active tag | JSON |
| `scud://stats/current` | Tag statistics | Text |

## Task Status Values

When using `scud_set_status`, valid status values are:
- `pending` - Not started
- `in-progress` - Currently being worked on
- `done` - Completed
- `review` - Ready for review
- `blocked` - Blocked by dependencies or issues
- `deferred` - Postponed to later
- `cancelled` - No longer needed

## Complexity Scale

SCUD uses Fibonacci complexity scores:
- `1` - Trivial (5-15 minutes)
- `2` - Simple (15-30 minutes)
- `3` - Moderate (30-60 minutes)
- `5` - Medium (1-3 hours)
- `8` - Complex (3-8 hours)
- `13` - Large (8+ hours, recommend expanding)
- `21` - Too large (must expand into subtasks)

## Troubleshooting

### "SCUD CLI not found in PATH"

The MCP server couldn't find the `scud` command. Install it:

```bash
npm install -g scud
```

Verify:

```bash
which scud  # macOS/Linux
where scud  # Windows
```

### "ANTHROPIC_API_KEY environment variable not set"

AI tools require an API key. Add it to your Claude Desktop config:

```json
{
  "mcpServers": {
    "scud": {
      "command": "scud-mcp",
      "env": {
        "ANTHROPIC_API_KEY": "sk-ant-your-key"
      }
    }
  }
}
```

### "No active tag set"

Initialize a project and set an active tag:

```
1. Run scud_init
2. Run scud_parse_prd to create tasks with a tag
3. Run scud_use_tag to set the active tag
```

### Tools not showing in Claude Desktop

1. Check that claude_desktop_config.json is valid JSON
2. Restart Claude Desktop completely
3. Check logs: Help â†’ View Logs in Claude Desktop
4. Verify scud-mcp is installed: `which scud-mcp`

## Development

### Build from Source

```bash
git clone https://github.com/yourusername/bmad-tm
cd bmad-tm/scud-mcp
npm install
npm run build
npm link  # Test locally
```

### Project Structure

```
scud-mcp/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ index.ts           # MCP server entry point
â”‚   â”œâ”€â”€ types.ts           # TypeScript types
â”‚   â”œâ”€â”€ tools/             # Tool implementations
â”‚   â”‚   â”œâ”€â”€ core.ts        # Core commands
â”‚   â”‚   â”œâ”€â”€ tags.ts        # Tag management
â”‚   â”‚   â”œâ”€â”€ task.ts        # Task operations
â”‚   â”‚   â”œâ”€â”€ ai.ts          # AI-powered tools
â”‚   â”‚   â””â”€â”€ parallel.ts    # Parallel development
â”‚   â”œâ”€â”€ resources/         # Resource implementations
â”‚   â”‚   â”œâ”€â”€ workflow.ts    # Workflow state
â”‚   â”‚   â”œâ”€â”€ tasks.ts       # Task lists
â”‚   â”‚   â””â”€â”€ stats.ts       # Statistics
â”‚   â””â”€â”€ utils/
â”‚       â””â”€â”€ exec.ts        # CLI execution wrapper
â”œâ”€â”€ package.json
â”œâ”€â”€ tsconfig.json
â””â”€â”€ README.md (this file)
```

## Performance

The SCUD MCP server is a lightweight wrapper around the fast Rust CLI:
- **Startup:** <100ms
- **Tool execution:** Inherits SCUD CLI performance (42ms average)
- **Memory:** <10MB for server process
- **Concurrent requests:** Supported (file locking in CLI prevents corruption)

## Contributing

Contributions welcome! Please:
1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests if applicable
5. Submit a pull request

## License

MIT License - see LICENSE file for details

## Related Projects

- [SCUD CLI](https://github.com/yourusername/bmad-tm/tree/master/scud-cli) - The Rust CLI this server wraps
- [Model Context Protocol](https://modelcontextprotocol.io/) - MCP specification
- [Claude Desktop](https://claude.ai/download) - MCP-compatible AI assistant

## Support

- Issues: https://github.com/yourusername/bmad-tm/issues
- Documentation: https://github.com/yourusername/bmad-tm/tree/master/docs
- SCUD Guide: https://github.com/yourusername/bmad-tm/blob/master/docs/guides/COMPLETE_GUIDE.md

---

**Built with â¤ï¸ for AI-powered development workflows**
</file>

<file path="scud-mcp/tsconfig.json">
{
  "compilerOptions": {
    "target": "ES2022",
    "module": "Node16",
    "moduleResolution": "Node16",
    "lib": ["ES2022"],
    "outDir": "./dist",
    "rootDir": "./src",
    "strict": true,
    "esModuleInterop": true,
    "skipLibCheck": true,
    "forceConsistentCasingInFileNames": true,
    "resolveJsonModule": true,
    "declaration": true,
    "declarationMap": true,
    "sourceMap": true
  },
  "include": ["src/**/*"],
  "exclude": ["node_modules", "dist"]
}
</file>

<file path="src/validators/scud-validator.js">
#!/usr/bin/env node

/**
 * SCUD Validator
 *
 * Validates SCUD state and provides task information.
 * Simplified version - workflow phases removed (use task DAG instead).
 */

const fs = require('fs');
const path = require('path');
const { execSync } = require('child_process');

class ScudValidator {
  constructor(projectRoot = process.cwd()) {
    this.projectRoot = projectRoot;
    this.scudDir = path.join(projectRoot, '.scud');
    this.taskmasterDir = path.join(projectRoot, '.taskmaster');
  }

  resolvePath(...segments) {
    const preferred = path.join(this.scudDir, ...segments);
    if (fs.existsSync(preferred)) {
      return preferred;
    }
    const fallback = path.join(this.taskmasterDir, ...segments);
    if (fs.existsSync(fallback)) {
      return fallback;
    }
    return preferred;
  }

  /**
   * Get active tag from active-tag file
   */
  getActiveTag() {
    const activeTagFile = this.resolvePath('active-tag');
    if (!fs.existsSync(activeTagFile)) {
      return null;
    }
    return fs.readFileSync(activeTagFile, 'utf8').trim() || null;
  }

  /**
   * Validate that SCUD CLI is available
   */
  validateScudCLI() {
    try {
      execSync('scud --version', { stdio: 'ignore' });
      return { valid: true };
    } catch (error) {
      return {
        valid: false,
        error: 'SCUD CLI not found. Install: npm install -g scud-task'
      };
    }
  }

  /**
   * Get command availability (simplified - no workflow phases)
   */
  getCommandAvailability() {
    const activeTag = this.getActiveTag();

    const commands = {
      'scud:task-list': { available: true, reason: 'List tasks in active tag' },
      'scud:task-next': { available: true, reason: 'Find next available task' },
      'scud:task-show': { available: true, reason: 'Show task details' },
      'scud:task-status': { available: true, reason: 'Update task status' },
      'scud:task-claim': { available: true, reason: 'Claim/release task locks' },
    };

    if (!activeTag) {
      for (const cmd of Object.keys(commands)) {
        commands[cmd].available = false;
        commands[cmd].reason = 'No active tag set. Run: scud tags <tag-name>';
      }
    }

    return commands;
  }

  /**
   * List all tags using scud CLI
   */
  listTags() {
    try {
      const result = execSync('scud tags', { encoding: 'utf8' });
      return { valid: true, output: result };
    } catch (error) {
      return { valid: false, error: error.message };
    }
  }
}

// CLI Interface
if (require.main === module) {
  const validator = new ScudValidator();
  const command = process.argv[2];

  try {
    let result;

    switch (command) {
      case 'validate-cli':
        result = validator.validateScudCLI();
        break;

      case 'get-command-availability':
        result = validator.getCommandAvailability();
        break;

      case 'list-tags':
        result = validator.listTags();
        break;

      case 'get-active-tag':
        const tag = validator.getActiveTag();
        result = { valid: true, activeTag: tag };
        break;

      default:
        console.error(`Unknown command: ${command}`);
        console.log(`
Usage: scud-validator.js <command>

Commands:
  validate-cli              Check if SCUD CLI is available
  get-command-availability  Get which commands are available
  list-tags                 List all tags
  get-active-tag            Get currently active tag
        `);
        process.exit(1);
    }

    console.log(JSON.stringify(result, null, 2));
    process.exit(result.valid !== false ? 0 : 1);
  } catch (error) {
    console.error(JSON.stringify({
      valid: false,
      error: error.message
    }, null, 2));
    process.exit(1);
  }
}

module.exports = ScudValidator;
</file>

<file path="src/task-manager.js">
#!/usr/bin/env node

/**
 * Simple Task Manager for SCUD
 * Sprint Cycle Unified Development
 *
 * DEPRECATED: This file is kept for reference only.
 * Use the scud CLI (Rust) for all task operations.
 *
 * The Rust CLI reads SCG format directly and is the authoritative source.
 */

const fs = require('fs');
const path = require('path');
const { execSync } = require('child_process');

class TaskManager {
  constructor(projectRoot = process.cwd()) {
    this.projectRoot = projectRoot;
    this.scudDir = path.join(projectRoot, '.scud');
    this.taskmasterDir = path.join(projectRoot, '.taskmaster');
  }

  /**
   * Get active tag from active-tag file
   */
  getActiveTag() {
    const activeTagFile = this.resolvePath('active-tag');
    if (!fs.existsSync(activeTagFile)) {
      return null;
    }
    return fs.readFileSync(activeTagFile, 'utf8').trim() || null;
  }

  resolvePath(...segments) {
    const preferred = path.join(this.scudDir, ...segments);
    if (fs.existsSync(preferred)) {
      return preferred;
    }
    const fallback = path.join(this.taskmasterDir, ...segments);
    if (fs.existsSync(fallback)) {
      return fallback;
    }
    return preferred;
  }

  /**
   * Run scud CLI command
   */
  runScud(args) {
    try {
      const result = execSync(`scud ${args.join(' ')}`, { encoding: 'utf8' });
      return { success: true, output: result };
    } catch (error) {
      return { success: false, error: error.message };
    }
  }

  /**
   * List all tags using scud CLI
   */
  listTags() {
    return this.runScud(['tags']);
  }

  /**
   * List tasks in active tag
   */
  listTasks(options = {}) {
    const args = ['list'];
    if (options.status) {
      args.push('--status', options.status);
    }
    return this.runScud(args);
  }

  /**
   * Show task details
   */
  showTask(taskId) {
    return this.runScud(['show', taskId]);
  }

  /**
   * Update task status
   */
  setStatus(taskId, status) {
    return this.runScud(['set-status', taskId, status]);
  }

  /**
   * Find next available task
   */
  findNext() {
    return this.runScud(['next']);
  }

  /**
   * Get task statistics
   */
  getStats() {
    return this.runScud(['stats']);
  }
}

// CLI Interface
if (require.main === module) {
  console.log(`
DEPRECATED: Use the scud CLI directly instead.

Examples:
  scud tags                      # List all tags
  scud list                      # List tasks
  scud next                      # Find next task
  scud show 3                    # Show task 3
  scud set-status 3 in-progress  # Start task 3
  scud set-status 3 done         # Complete task 3
  scud stats                     # Show statistics
  `);
  process.exit(0);
}

module.exports = TaskManager;
</file>

<file path=".gitignore">
# Rust build artifacts
scud-cli/target/

# Node.js
node_modules/
npm-debug.log*
yarn-debug.log*
yarn-error.log*
package-lock.json

# SCUD task management (user data)
.taskmaster/
docs/prd/
docs/epics/
docs/architecture/
docs/retrospectives/

# Editor files
.vscode/
.idea/
*.swp
*.swo
*~
.DS_Store

# Temporary files
*.tmp
*.log

# Coverage reports
coverage/

# SCUD
.scud/
</file>

<file path=".npmignore">
# Development files
*.log
npm-debug.log*
node_modules/
.DS_Store

# Git
.git/
.gitignore

# IDE
.vscode/
.idea/

# Project-specific (not needed in package)
.taskmaster/
docs/
log_docs/
repomix-output.xml

# Installation scripts (already in project)
install-claude-code.sh
install-opencode.sh

# Rust build artifacts
scud-cli/target/

# MCP server (separate package)
scud-mcp/

# OpenCode node_modules
.opencode/node_modules/
</file>

<file path="install-claude-code.sh">
#!/bin/bash

# BMAD-TM Lite Installation Script for Claude Code CLI
# This script sets up the workflow orchestration system

set -e

echo "ðŸš€ BMAD-TM Lite Installation for Claude Code CLI"
echo "=================================================="
echo ""

# Colors for output
GREEN='\033[0;32m'
BLUE='\033[0;34m'
YELLOW='\033[1;33m'
RED='\033[0;31m'
NC='\033[0m' # No Color

# Get project root
PROJECT_ROOT="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
echo -e "${BLUE}Project root:${NC} $PROJECT_ROOT"
echo ""

# Check if Task Master CLI is installed
echo -e "${BLUE}Step 1: Checking Task Master CLI...${NC}"
if command -v task-master &> /dev/null; then
    TASKMASTER_VERSION=$(task-master --version 2>&1 || echo "unknown")
    echo -e "${GREEN}âœ“ Task Master CLI found${NC} ($TASKMASTER_VERSION)"
else
    echo -e "${RED}âœ— Task Master CLI not found${NC}"
    echo ""
    echo "Install Task Master CLI:"
    echo "  npm install -g task-master"
    echo ""
    read -p "Would you like to install it now? (y/n) " -n 1 -r
    echo
    if [[ $REPLY =~ ^[Yy]$ ]]; then
        npm install -g task-master
        echo -e "${GREEN}âœ“ Task Master CLI installed${NC}"
    else
        echo -e "${YELLOW}âš  Skipping Task Master CLI installation${NC}"
        echo "You'll need to install it manually before using BMAD-TM Lite"
    fi
fi
echo ""

# Check if Node.js is installed (for validator)
echo -e "${BLUE}Step 2: Checking Node.js...${NC}"
if command -v node &> /dev/null; then
    NODE_VERSION=$(node --version)
    echo -e "${GREEN}âœ“ Node.js found${NC} ($NODE_VERSION)"
else
    echo -e "${RED}âœ— Node.js not found${NC}"
    echo "Node.js is required for the Task Master validator."
    echo "Install from: https://nodejs.org/"
    exit 1
fi
echo ""

# Initialize Task Master if not already done
echo -e "${BLUE}Step 3: Initializing Task Master...${NC}"
cd "$PROJECT_ROOT"
if [ -f ".taskmaster/tasks/tasks.json" ]; then
    echo -e "${GREEN}âœ“ Task Master already initialized${NC}"
else
    mkdir -p .taskmaster/tasks
    echo '{}' > .taskmaster/tasks/tasks.json
    echo -e "${GREEN}âœ“ Task Master initialized${NC}"
fi
echo ""

# Create workflow state file
echo -e "${BLUE}Step 4: Creating workflow state...${NC}"
if [ -f ".taskmaster/workflow-state.json" ]; then
    echo -e "${YELLOW}âš  Workflow state already exists${NC}"
    read -p "Overwrite? (y/n) " -n 1 -r
    echo
    if [[ ! $REPLY =~ ^[Yy]$ ]]; then
        echo -e "${BLUE}â†’ Keeping existing workflow state${NC}"
    else
        cp .taskmaster/workflow-state.json .taskmaster/workflow-state.json.backup
        echo -e "${YELLOW}â†’ Backed up to .taskmaster/workflow-state.json.backup${NC}"
        # Create fresh state
        cat > .taskmaster/workflow-state.json << 'EOF'
{
  "version": "1.0.0",
  "current_phase": "ideation",
  "active_epic": null,
  "phases": {
    "ideation": {
      "status": "active",
      "completed_at": null,
      "agent": "tm-pm",
      "description": "Product definition and PRD creation"
    },
    "planning": {
      "status": "pending",
      "completed_at": null,
      "agent": "tm-pm",
      "description": "Parse PRD into Task Master epics and tasks"
    },
    "architecture": {
      "status": "pending",
      "completed_at": null,
      "agent": "tm-architect",
      "description": "Technical design and architecture planning"
    },
    "implementation": {
      "status": "pending",
      "completed_at": null,
      "agent": "tm-dev",
      "description": "Task execution and development"
    },
    "retrospective": {
      "status": "pending",
      "completed_at": null,
      "agent": "tm-retrospective",
      "description": "Post-epic analysis and learning capture"
    }
  },
  "history": [],
  "completed_epics": [],
  "last_updated": null
}
EOF
        echo -e "${GREEN}âœ“ Workflow state created${NC}"
    fi
else
    cat > .taskmaster/workflow-state.json << 'EOF'
{
  "version": "1.0.0",
  "current_phase": "ideation",
  "active_epic": null,
  "phases": {
    "ideation": {
      "status": "active",
      "completed_at": null,
      "agent": "tm-pm",
      "description": "Product definition and PRD creation"
    },
    "planning": {
      "status": "pending",
      "completed_at": null,
      "agent": "tm-pm",
      "description": "Parse PRD into Task Master epics and tasks"
    },
    "architecture": {
      "status": "pending",
      "completed_at": null,
      "agent": "tm-architect",
      "description": "Technical design and architecture planning"
    },
    "implementation": {
      "status": "pending",
      "completed_at": null,
      "agent": "tm-dev",
      "description": "Task execution and development"
    },
    "retrospective": {
      "status": "pending",
      "completed_at": null,
      "agent": "tm-retrospective",
      "description": "Post-epic analysis and learning capture"
    }
  },
  "history": [],
  "completed_epics": [],
  "last_updated": null
}
EOF
    echo -e "${GREEN}âœ“ Workflow state created${NC}"
fi
echo ""

# Create directory structure
echo -e "${BLUE}Step 5: Creating directory structure...${NC}"
mkdir -p docs/prd
mkdir -p docs/epics
mkdir -p docs/architecture
mkdir -p docs/retrospectives
echo -e "${GREEN}âœ“ Directory structure created${NC}"
echo ""

# Copy slash commands to Claude Code directory
echo -e "${BLUE}Step 6: Installing slash commands...${NC}"
CLAUDE_COMMANDS_DIR="$HOME/.config/claude-code/commands"

if [ -d "$CLAUDE_COMMANDS_DIR" ]; then
    cp -r .claude/commands/* "$CLAUDE_COMMANDS_DIR/"
    echo -e "${GREEN}âœ“ Slash commands installed to $CLAUDE_COMMANDS_DIR${NC}"
    echo "  â€¢ /status"
    echo "  â€¢ /tm-pm"
    echo "  â€¢ /tm-architect"
    echo "  â€¢ /tm-dev"
    echo "  â€¢ /tm-retrospective"
else
    echo -e "${YELLOW}âš  Claude Code commands directory not found${NC}"
    echo "  Expected: $CLAUDE_COMMANDS_DIR"
    echo ""
    echo "Options:"
    echo "  1. Symlink commands to your project (recommended):"
    echo "     ln -s $PROJECT_ROOT/.claude/commands ~/.config/claude-code/commands"
    echo ""
    echo "  2. Copy commands manually when Claude Code is installed"
fi
echo ""

# Make validator executable and add to PATH
echo -e "${BLUE}Step 7: Setting up Task Master validator...${NC}"
chmod +x "$PROJECT_ROOT/src/validators/taskmaster-validator.js"
echo -e "${GREEN}âœ“ Validator made executable${NC}"
echo ""
echo "To use the validator globally, add to your PATH:"
echo "  export PATH=\"\$PATH:$PROJECT_ROOT/src/validators\""
echo ""
echo "Or add this to your ~/.bashrc or ~/.zshrc:"
echo "  echo 'export PATH=\"\$PATH:$PROJECT_ROOT/src/validators\"' >> ~/.bashrc"
echo ""

# Test validator
echo -e "${BLUE}Step 8: Testing validator...${NC}"
if "$PROJECT_ROOT/src/validators/taskmaster-validator.js" get-command-availability &> /dev/null; then
    echo -e "${GREEN}âœ“ Validator working correctly${NC}"
else
    echo -e "${YELLOW}âš  Validator test failed (may need Node.js modules)${NC}"
fi
echo ""

# Create .gitignore if it doesn't exist
echo -e "${BLUE}Step 9: Updating .gitignore...${NC}"
if [ ! -f ".gitignore" ]; then
    touch .gitignore
fi

# Add Task Master files to gitignore if not already present
if ! grep -q ".taskmaster/tasks/tasks.json" .gitignore; then
    echo "" >> .gitignore
    echo "# Task Master state (optional - depends on team workflow)" >> .gitignore
    echo "# .taskmaster/tasks/tasks.json" >> .gitignore
    echo "# .taskmaster/workflow-state.json" >> .gitignore
fi
echo -e "${GREEN}âœ“ .gitignore updated${NC}"
echo ""

# Installation complete
echo ""
echo -e "${GREEN}âœ… BMAD-TM Lite installation complete!${NC}"
echo "=================================================="
echo ""
echo -e "${BLUE}Quick Start:${NC}"
echo ""
echo "  1. Check your workflow status:"
echo "     /status"
echo ""
echo "  2. Start your first epic:"
echo "     /tm-pm"
echo ""
echo "  3. Follow the workflow phases:"
echo "     Ideation â†’ Planning â†’ Architecture â†’ Implementation â†’ Retrospective"
echo ""
echo -e "${BLUE}Documentation:${NC}"
echo "  â€¢ Workflow Guide: src/workflows/workflow-plan-and-build.md"
echo "  â€¢ Quick Start: QUICKSTART.md"
echo ""
echo -e "${BLUE}Slash Commands Available:${NC}"
echo "  /status           - Show workflow status"
echo "  /tm-pm            - Product Manager (create PRD, plan epics)"
echo "  /tm-architect     - Architect (design technical solution)"
echo "  /tm-dev           - Developer (implement tasks)"
echo "  /tm-retrospective - Retrospective (capture learnings)"
echo ""
echo -e "${YELLOW}Next Steps:${NC}"
echo "  â€¢ Read QUICKSTART.md for a guided walkthrough"
echo "  â€¢ Run /status to see your current workflow state"
echo "  â€¢ Start with /tm-pm when ready to create your first epic"
echo ""
echo "Happy building! ðŸš€"
echo ""
</file>

<file path="install-opencode.sh">
#!/bin/bash

# BMAD-TM Lite Installation Script for OpenCode
# This script sets up the workflow orchestration system for OpenCode

set -e

echo "ðŸš€ BMAD-TM Lite Installation for OpenCode"
echo "=========================================="
echo ""

# Colors for output
GREEN='\033[0;32m'
BLUE='\033[0;34m'
YELLOW='\033[1;33m'
RED='\033[0;31m'
NC='\033[0m' # No Color

# Get project root
PROJECT_ROOT="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
echo -e "${BLUE}Project root:${NC} $PROJECT_ROOT"
echo ""

# Check if Task Master CLI is installed
echo -e "${BLUE}Step 1: Checking Task Master CLI...${NC}"
if command -v task-master &> /dev/null; then
    TASKMASTER_VERSION=$(task-master --version 2>&1 || echo "unknown")
    echo -e "${GREEN}âœ“ Task Master CLI found${NC} ($TASKMASTER_VERSION)"
else
    echo -e "${RED}âœ— Task Master CLI not found${NC}"
    echo ""
    echo "Install Task Master CLI:"
    echo "  npm install -g task-master"
    echo ""
    read -p "Would you like to install it now? (y/n) " -n 1 -r
    echo
    if [[ $REPLY =~ ^[Yy]$ ]]; then
        npm install -g task-master
        echo -e "${GREEN}âœ“ Task Master CLI installed${NC}"
    else
        echo -e "${YELLOW}âš  Skipping Task Master CLI installation${NC}"
        echo "You'll need to install it manually before using BMAD-TM Lite"
    fi
fi
echo ""

# Check if Node.js is installed (for validator)
echo -e "${BLUE}Step 2: Checking Node.js...${NC}"
if command -v node &> /dev/null; then
    NODE_VERSION=$(node --version)
    echo -e "${GREEN}âœ“ Node.js found${NC} ($NODE_VERSION)"
else
    echo -e "${RED}âœ— Node.js not found${NC}"
    echo "Node.js is required for the Task Master validator."
    echo "Install from: https://nodejs.org/"
    exit 1
fi
echo ""

# Initialize Task Master if not already done
echo -e "${BLUE}Step 3: Initializing Task Master...${NC}"
cd "$PROJECT_ROOT"
if [ -f ".taskmaster/tasks/tasks.json" ]; then
    echo -e "${GREEN}âœ“ Task Master already initialized${NC}"
else
    mkdir -p .taskmaster/tasks
    echo '{}' > .taskmaster/tasks/tasks.json
    echo -e "${GREEN}âœ“ Task Master initialized${NC}"
fi
echo ""

# Create workflow state file
echo -e "${BLUE}Step 4: Creating workflow state...${NC}"
if [ -f ".taskmaster/workflow-state.json" ]; then
    echo -e "${YELLOW}âš  Workflow state already exists${NC}"
    read -p "Overwrite? (y/n) " -n 1 -r
    echo
    if [[ ! $REPLY =~ ^[Yy]$ ]]; then
        echo -e "${BLUE}â†’ Keeping existing workflow state${NC}"
    else
        cp .taskmaster/workflow-state.json .taskmaster/workflow-state.json.backup
        echo -e "${YELLOW}â†’ Backed up to .taskmaster/workflow-state.json.backup${NC}"
        # Create fresh state
        cat > .taskmaster/workflow-state.json << 'EOF'
{
  "version": "1.0.0",
  "current_phase": "ideation",
  "active_epic": null,
  "phases": {
    "ideation": {
      "status": "active",
      "completed_at": null,
      "agent": "tm-pm",
      "description": "Product definition and PRD creation"
    },
    "planning": {
      "status": "pending",
      "completed_at": null,
      "agent": "tm-pm",
      "description": "Parse PRD into Task Master epics and tasks"
    },
    "architecture": {
      "status": "pending",
      "completed_at": null,
      "agent": "tm-architect",
      "description": "Technical design and architecture planning"
    },
    "implementation": {
      "status": "pending",
      "completed_at": null,
      "agent": "tm-dev",
      "description": "Task execution and development"
    },
    "retrospective": {
      "status": "pending",
      "completed_at": null,
      "agent": "tm-retrospective",
      "description": "Post-epic analysis and learning capture"
    }
  },
  "history": [],
  "completed_epics": [],
  "last_updated": null
}
EOF
        echo -e "${GREEN}âœ“ Workflow state created${NC}"
    fi
else
    cat > .taskmaster/workflow-state.json << 'EOF'
{
  "version": "1.0.0",
  "current_phase": "ideation",
  "active_epic": null,
  "phases": {
    "ideation": {
      "status": "active",
      "completed_at": null,
      "agent": "tm-pm",
      "description": "Product definition and PRD creation"
    },
    "planning": {
      "status": "pending",
      "completed_at": null,
      "agent": "tm-pm",
      "description": "Parse PRD into Task Master epics and tasks"
    },
    "architecture": {
      "status": "pending",
      "completed_at": null,
      "agent": "tm-architect",
      "description": "Technical design and architecture planning"
    },
    "implementation": {
      "status": "pending",
      "completed_at": null,
      "agent": "tm-dev",
      "description": "Task execution and development"
    },
    "retrospective": {
      "status": "pending",
      "completed_at": null,
      "agent": "tm-retrospective",
      "description": "Post-epic analysis and learning capture"
    }
  },
  "history": [],
  "completed_epics": [],
  "last_updated": null
}
EOF
    echo -e "${GREEN}âœ“ Workflow state created${NC}"
fi
echo ""

# Create directory structure
echo -e "${BLUE}Step 5: Creating directory structure...${NC}"
mkdir -p docs/prd
mkdir -p docs/epics
mkdir -p docs/architecture
mkdir -p docs/retrospectives
echo -e "${GREEN}âœ“ Directory structure created${NC}"
echo ""

# Create OpenCode skills directory
echo -e "${BLUE}Step 6: Creating OpenCode skills...${NC}"
mkdir -p .opencode/skills

# Create skill files from slash commands
cat > .opencode/skills/status.md << 'EOF'
# BMAD-TM Workflow Status Skill

Invoke this skill to show the current BMAD-TM workflow status.

## How to Use
User says: "show status" or "what's my workflow status?" or "status"

## Skill Behavior
Load and display:
- Current workflow phase
- Active epic and task progress
- Available commands
- Warnings or blockers
- Next steps guidance

Reference the full command documentation at: .claude/commands/status.md
EOF

cat > .opencode/skills/tm-pm.md << 'EOF'
# Product Manager Skill

Invoke this skill when the user wants to:
- Create a Product Requirements Document (PRD)
- Plan a new epic
- Break down requirements into tasks

## How to Use
User says: "I need to create a PRD" or "start product planning" or "tm-pm"

## Skill Behavior
1. Validate workflow phase (must be ideation or planning)
2. Load Product Manager agent persona from: .claude/commands/tm-pm.md
3. Follow the agent's workflow for current phase

Reference the full agent documentation at: .claude/commands/tm-pm.md
EOF

cat > .opencode/skills/tm-architect.md << 'EOF'
# Architect Skill

Invoke this skill when the user wants to:
- Design technical architecture
- Create technical specifications
- Enhance tasks with implementation details

## How to Use
User says: "design the architecture" or "create technical design" or "tm-architect"

## Skill Behavior
1. Validate workflow phase (must be architecture)
2. Validate active epic exists
3. Load Architect agent persona from: .claude/commands/tm-architect.md
4. Follow the agent's workflow

Reference the full agent documentation at: .claude/commands/tm-architect.md
EOF

cat > .opencode/skills/tm-dev.md << 'EOF'
# Developer Skill

Invoke this skill when the user wants to:
- Implement tasks from Task Master
- Write code following architecture
- Execute the development phase

## How to Use
User says: "start development" or "implement tasks" or "tm-dev"

## Skill Behavior
1. Validate workflow phase (must be implementation)
2. Validate active epic and architecture complete
3. Load Developer agent persona from: .claude/commands/tm-dev.md
4. Follow dependency-aware implementation workflow

Reference the full agent documentation at: .claude/commands/tm-dev.md
EOF

cat > .opencode/skills/tm-retrospective.md << 'EOF'
# Retrospective Skill

Invoke this skill when the user wants to:
- Conduct post-epic retrospective
- Capture learnings and insights
- Analyze completed work

## How to Use
User says: "run retrospective" or "review the epic" or "tm-retrospective"

## Skill Behavior
1. Validate all tasks in epic are complete
2. Load Retrospective agent persona from: .claude/commands/tm-retrospective.md
3. Follow retrospective workflow
4. Create comprehensive retrospective document

Reference the full agent documentation at: .claude/commands/tm-retrospective.md
EOF

echo -e "${GREEN}âœ“ OpenCode skills created${NC}"
echo "  â€¢ status"
echo "  â€¢ tm-pm"
echo "  â€¢ tm-architect"
echo "  â€¢ tm-dev"
echo "  â€¢ tm-retrospective"
echo ""

# Make validator executable and add to PATH
echo -e "${BLUE}Step 7: Setting up Task Master validator...${NC}"
chmod +x "$PROJECT_ROOT/src/validators/taskmaster-validator.js"
echo -e "${GREEN}âœ“ Validator made executable${NC}"
echo ""
echo "To use the validator globally, add to your PATH:"
echo "  export PATH=\"\$PATH:$PROJECT_ROOT/src/validators\""
echo ""
echo "Or add this to your ~/.bashrc or ~/.zshrc:"
echo "  echo 'export PATH=\"\$PATH:$PROJECT_ROOT/src/validators\"' >> ~/.bashrc"
echo ""

# Test validator
echo -e "${BLUE}Step 8: Testing validator...${NC}"
if "$PROJECT_ROOT/src/validators/taskmaster-validator.js" get-command-availability &> /dev/null; then
    echo -e "${GREEN}âœ“ Validator working correctly${NC}"
else
    echo -e "${YELLOW}âš  Validator test failed (may need Node.js modules)${NC}"
fi
echo ""

# Create .gitignore if it doesn't exist
echo -e "${BLUE}Step 9: Updating .gitignore...${NC}"
if [ ! -f ".gitignore" ]; then
    touch .gitignore
fi

# Add Task Master files to gitignore if not already present
if ! grep -q ".taskmaster/tasks/tasks.json" .gitignore; then
    echo "" >> .gitignore
    echo "# Task Master state (optional - depends on team workflow)" >> .gitignore
    echo "# .taskmaster/tasks/tasks.json" >> .gitignore
    echo "# .taskmaster/workflow-state.json" >> .gitignore
fi
echo -e "${GREEN}âœ“ .gitignore updated${NC}"
echo ""

# Installation complete
echo ""
echo -e "${GREEN}âœ… BMAD-TM Lite installation for OpenCode complete!${NC}"
echo "========================================================="
echo ""
echo -e "${BLUE}Quick Start:${NC}"
echo ""
echo "  1. Tell OpenCode: 'show status'"
echo "     This will display your current workflow state"
echo ""
echo "  2. Start your first epic: 'start product planning'"
echo "     This will activate the Product Manager skill"
echo ""
echo "  3. Follow the workflow phases:"
echo "     Ideation â†’ Planning â†’ Architecture â†’ Implementation â†’ Retrospective"
echo ""
echo -e "${BLUE}Documentation:${NC}"
echo "  â€¢ Workflow Guide: src/workflows/workflow-plan-and-build.md"
echo "  â€¢ Quick Start: QUICKSTART.md"
echo ""
echo -e "${BLUE}Skills Available:${NC}"
echo "  status           - Show workflow status"
echo "  tm-pm            - Product Manager (create PRD, plan epics)"
echo "  tm-architect     - Architect (design technical solution)"
echo "  tm-dev           - Developer (implement tasks)"
echo "  tm-retrospective - Retrospective (capture learnings)"
echo ""
echo -e "${BLUE}How to Invoke Skills:${NC}"
echo "  Just describe what you want in natural language:"
echo "  â€¢ 'show me the current status'"
echo "  â€¢ 'I need to create a product requirements document'"
echo "  â€¢ 'let's design the architecture'"
echo "  â€¢ 'start implementing tasks'"
echo "  â€¢ 'run a retrospective'"
echo ""
echo -e "${YELLOW}Next Steps:${NC}"
echo "  â€¢ Read QUICKSTART.md for a guided walkthrough"
echo "  â€¢ Tell OpenCode 'show status' to see your workflow state"
echo "  â€¢ Start with 'create a PRD' when ready for your first epic"
echo ""
echo "Happy building! ðŸš€"
echo ""
</file>

<file path="LICENSE">
MIT License

Copyright (c) 2025 pyrex41

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
</file>

<file path="package.json">
{
  "name": "scud-task",
  "version": "1.19.3",
  "description": "Sprint Cycle Unified Development - Lightweight workflow orchestration for building software with AI agents",
  "main": "src/validators/scud-validator.js",
  "bin": {
    "scud": "bin/scud.js"
  },
  "scripts": {
    "postinstall": "node bin/postinstall.js",
    "test": "echo \"Error: no test specified\" && exit 1"
  },
  "keywords": [
    "scud",
    "sprint-cycle",
    "workflow",
    "orchestration",
    "ai-agents",
    "project-management",
    "agile",
    "scrum",
    "claude-code"
  ],
  "author": "pyrex41",
  "license": "MIT",
  "repository": {
    "type": "git",
    "url": "git+https://github.com/pyrex41/scud.git"
  },
  "bugs": {
    "url": "https://github.com/pyrex41/scud/issues"
  },
  "homepage": "https://github.com/pyrex41/scud#readme",
  "dependencies": {
    "open": "^10.0.3"
  },
  "engines": {
    "node": ">=16.0.0"
  },
  "files": [
    "bin/",
    "src/",
    "scud-cli/Cargo.toml",
    "scud-cli/src/",
    "scud-cli/PROVIDERS.md",
    ".claude/commands/",
    ".claude/settings.local.json",
    ".opencode/command/",
    ".opencode/skills/",
    ".opencode/scud-commands.md",
    ".opencode/package.json",
    "README.md",
    "QUICKSTART.md",
    "COMPLETE_GUIDE.md",
    "QUICK_REFERENCE.md",
    "PARALLEL_FEATURES.md",
    "LICENSE"
  ]
}
</file>

<file path="prd-hook.md">
### SCUD v2.0 â€œBulletproof Completesâ€ PRD  
**Feature:** Enforced task completion via Claude Code Stop hooks  
**Goal:** Make it physically impossible for a Claude sub-agent to finish a task without calling `scud complete <id>` â€” zero reliance on LLM memory or politeness.  
**Target release:** v2.0.0 (post-beta)

#### 1. Why this feature
- Current reality: Claude Code sub-agents forget to mark tasks done ~12â€“18% of the time in real waves.
- Beads wins agent love because its issues are the single source of truth and agents canâ€™t â€œwalk awayâ€ without closing them.
- We want the same guarantee with zero wrapper scripts, zero background loops, and zero extra dependencies.
- Claude Codeâ€™s Stop hook (June 2025) gives us the perfect enforcement point: it fires on every clean exit and can block the session from ending until the task is marked complete.

#### 2. User stories (prioritised)

| Priority | Story                                                                                            | Acceptance criteria                                                                                              |
|----------|--------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------|
| 1        | As an orchestrator, I want every Claude sub-agent to be forced to run `scud complete` before it can end a task. | `scud complete <id>` is executed automatically on every clean Stop hook. If it fails, the session is blocked.    |
| 2        | As a human, I want `scud wave start` to auto-install the enforcement hook so I never forget.     | Running `scud wave start â€¦` writes the correct Stop hook into `.claude/settings.local.json` (project-scoped).    |
| 3        | As a human, I want to disable enforcement temporarily for debugging.                            | `scud wave start --no-enforce` skips hook installation. Hook can be removed with `scud hooks uninstall`.         |
| 4        | As an orchestrator, I want stuck/dead sessions to be auto-failed after timeout.                  | If Stop hook never fires (Claude killed, network drop, etc.), orchestratorâ€™s `scud next --stale` will reassign after 3 min of no heartbeat file. |
| 5        | As a developer, I want the hook to pull a short auto-summary from the live log if the agent didnâ€™t provide one. | `scud complete` called by the hook uses the last 5 lines of `.scud/live/<id>.log` as default summary.            |

#### 3. Minimal viable implementation (â‰¤ 150 LOC total)

**A. New CLI command**
```bash
scud hooks install        # writes the Stop hook (idempotent)
scud hooks uninstall      # removes it
scud hooks status         # shows if active
```

**B. Hook payload (written to .claude/settings.local.json)**
```json
{
  "hooks": {
    "Stop": [
      {
        "matcher": "Task|Subagent",
        "hooks": [
          {
            "type": "command",
            "command": "scud _internal_complete_from_hook",
            "run_in_background": false
          }
        ]
      }
    ]
  }
}
```

**C. Internal binary command (private, not exposed to LLM)**
```rust
// src/bin/scud-_internal_complete_from_hook.rs
// Only callable from Claude Code hook (never documented to LLM)
fn main() {
    let task_id = std::env::var("CLAUDE_TASK_ID")
        .or_else(|_| extract_from_last_prompt()) // fallback
        .expect("No task ID found");

    let summary = match std::env::var("CLAUDE_TASK_SUMMARY") {
        Ok(s) if !s.trim().is_empty() => s,
        _ => auto_summary_from_log(&task_id),
    };

    Command::new("scud")
        .args(["complete", &task_id, "--summary", &summary])
        .status()
        .expect("Failed to mark task complete");

    // Optional: remove claim + clean live log
    Command::new("scud").args(["unclaim", &task_id]).ok();
}
```

**D. Wave start auto-install**
```rust
if !matches.contains_id("no-enforce") {
    hooks::install_stop_hook().unwrap();
    println!("Bulletproof completes enabled âœ“");
}
```

#### 4. Exact user-facing flow (what people will actually type)

```bash
# One-time setup (per project)
scud init

# Start a parallel wave â€” automatically installs the enforcement hook
scud wave start auth-refactor --parallelism 4

# Fire four Claude Code sub-agents exactly as before
for task in $(scud next --ready --wave auth-refactor --limit 4); do
  claude-code run "Implement $task" &
done

# Every single sub-agent is now physically unable to end without:
# â†’ scud complete t-47 --summary "..."
# â†’ DAG is always accurate
# â†’ Orchestrator never has to guess
```

#### 5. Success metrics (post-release)

| Metric                            | Target      | How to measure                                 |
|-----------------------------------|-------------|------------------------------------------------|
| `scud complete` called rate       | 99.9%       | Count Stop hook invocations vs. manual completes |
| Abandoned tasks per wave          | â‰¤ 1         | `scud list --claimed --stale` after wave finish |
| Hook installation friction        | 0 manual steps | 100% of `wave start` calls succeed without --no-enforce |
| Extra latency added by hook       | â‰¤ 80 ms     | Benchmarked on 100 tasks                        |

#### 6. Future-proofing (already designed in)

- Heartbeats â†’ add a second hook (SessionStart + background loop) when we want full agent mode.
- Non-Claude agents â†’ same `scud run` wrapper from earlier PRD can call the same internal complete binary.
- Cursor / Amp â†’ theyâ€™ll get their own hook systems in 2026; the internal binary stays identical.

Ship this and SCUD instantly becomes the only public tool that can reliably run 5â€“20 parallel Claude Code agents with zero manual cleanup. This is the single highest-leverage feature you can add right now.
</file>

<file path="README.md">
# SCUD

**Sprint Cycle Unified Development** - Fast, AI-powered task management for building software

> **Version 1.17.0** - Stable release. Fast Rust CLI with SCG format, DAG-driven execution, and Claude Code hook integration.

A lightweight DAG-driven task management system with automatic completion enforcement via Claude Code hooks.

---

## Quick Start

### Install

**Using pnpm (recommended):**
```bash
pnpm add -g scud-task
cd your-project
scud init
scud hooks install  # Enable automatic task completion
```

**Using npm:**
```bash
npm install -g scud-task
cd your-project
scud init
scud hooks install
```

**Using Bun:**
```bash
# Bun blocks postinstall scripts by default, so use npm or pnpm
# Or manually run the postinstall after bun install:
bun install -g scud-task
cd ~/.bun/install/global/node_modules/scud-task
node bin/postinstall.js
```

### Basic Usage
```bash
# Create tasks manually or parse from a PRD
scud parse-prd docs/feature.md --tag my-feature

# Find and work on next ready task
scud next --tag my-feature
scud set-status 1 in-progress

# When done, mark complete (or let hooks do it automatically)
scud set-status 1 done
```

**Full guide:** [docs/guides/COMPLETE_GUIDE.md](docs/guides/COMPLETE_GUIDE.md)
**Orchestrator pattern:** [docs/orchestrator.md](docs/orchestrator.md)

---

## Usage Modes

SCUD offers two ways to work with AI assistants, each with different trade-offs:

### Mode 1: Direct CLI (Recommended)

**How it works:**
- Use SCUD CLI directly via bash for manual task management
- Hooks enforce automatic task completion when Claude Code sessions end
- DAG execution ensures tasks become ready when dependencies complete

**Setup:**
```bash
pnpm add -g scud-task   # or: npm install -g scud-task
cd your-project
scud init
scud hooks install      # Critical: enables automatic completion
```

**Pros:**
- âœ… Full file system access (can edit tasks.scg directly if needed)
- âœ… Automatic task completion via hooks (prevents forgotten updates)
- âœ… More flexible - can use any tool/command
- âœ… Better error messages (sees full CLI output)
- âœ… Can combine SCUD with other tools seamlessly
- âœ… DAG-driven execution (tasks ready when deps complete)

**Cons:**
- âŒ Requires bash/shell access
- âŒ Must learn CLI commands

**Best for:** Individual developers, orchestrator patterns, automated workflows

---

### Mode 2: MCP Server (Universal Protocol)

**How it works:**
- Lightweight TypeScript server wraps SCUD CLI
- Exposes 20 MCP tools + 3 resources via standardized protocol
- Works with any MCP-compatible client (Claude Desktop, Cursor, Claude Code, etc.)

**Setup:**
```bash
pnpm add -g scud-task scud-mcp   # or: npm install -g scud-task scud-mcp

# Configure your MCP client (example for Claude Desktop):
# ~/Library/Application Support/Claude/claude_desktop_config.json
{
  "mcpServers": {
    "scud": {
      "command": "scud-mcp",
      "env": {
        "ANTHROPIC_API_KEY": "sk-ant-..."
      }
    }
  }
}

# For Cursor/other clients, see scud-mcp/README.md for config
# Then use naturally: "Initialize SCUD and parse my PRD"
```

**Pros:**
- âœ… Structured protocol (well-defined tool schemas)
- âœ… Works across multiple AI clients (Claude Desktop, Cursor, etc.)
- âœ… Cleaner interface (named tools vs bash commands)
- âœ… Type-safe tool calls with validation
- âœ… Can add file access via MCP resources (extensible)

**Cons:**
- âŒ Requires MCP server installation (extra dependency)
- âŒ Less ad-hoc than direct bash (predefined tools only)
- âŒ Client must support MCP protocol

**Best for:** Multi-client usage, structured workflows, type-safe operations

---

### Which Mode Should You Use?

**Use Direct CLI if:**
- You want maximum flexibility (any bash command)
- You need DAG-driven execution with hooks
- You're building orchestrator patterns (parallel execution)
- You want zero external dependencies (just the CLI)
- You want automatic task completion enforcement

**Use MCP Server if:**
- You use multiple AI clients (Claude Desktop, Cursor, etc.)
- You want structured, type-safe tool calls
- You prefer protocol-based integration
- You want extensible architecture (can add custom tools/resources)

**Can you use both?** Yes! The MCP server wraps the CLI, so they're fully compatible. Both modes work in Claude Code, Cursor, and Claude Desktop.

---

## Core Concepts

### SCG Format

Tasks are stored in **SCG (SCUD Graph)** formatâ€”a token-efficient, human-readable text format that achieves ~75% token reduction compared to JSON. SCG explicitly represents the task dependency graph with sections for nodes, edges, and metadata. Inspired in part by Nikolai Mushegian's [JAMS spec](https://nikolai.fyi/jams/) ([GitHub](https://github.com/nmushegian/jams)).

```
@nodes
auth:1 | Design auth system | X | 13 | H
auth:1.1 | Implement JWT | D | 5 | H

@edges
auth:1.1 -> auth:1
```

**Full spec:** [docs/reference/SCG_FORMAT_SPEC.md](docs/reference/SCG_FORMAT_SPEC.md)

### DAG-Driven Execution
Tasks become ready when their dependencies complete. No manual phase management required.

```
Task 1 â”€â”€â”
         â”œâ”€â”€> Task 3 â”€â”€> Task 5
Task 2 â”€â”€â”˜      â”‚
                â””â”€â”€> Task 4
```

### Tags
Group related tasks together (e.g., `auth-system`, `payment-flow`). Each tag has its own task graph.

### Automatic Completion
Claude Code hooks enforce task completion when sessions end. Set `SCUD_TASK_ID` env var and the hook marks it done automatically.

### Parallel Execution
Use orchestrator patterns to spawn multiple Claude Code agents in parallel, each working on a ready task.

---

## Key Features

### Fast Rust CLI
- âš¡ **50x faster** than JavaScript alternatives
- ðŸŽ¯ **42x fewer tokens** (500 vs 21k)
- ðŸ“¦ **Single binary** - no dependencies

### Hook-Enforced Completion
- ðŸ”’ **Automatic task completion** via Claude Code hooks
- âœ… **Prevents forgotten updates** (solves 15% failure case)
- ðŸŽ¯ **Session-based** - marks done when Claude session ends

### DAG-Driven Execution
- ðŸ“Š **Dependency graphs** - tasks ready when deps complete
- ðŸ”€ **Parallel waves** - visualize concurrent work
- ðŸŽ¯ **Smart scheduling** - next command finds ready tasks

### Orchestrator Support
- ðŸ‘¥ **Parallel agents** - spawn multiple Claude instances
- ðŸ”’ **Task locking** - prevent conflicts
- ðŸ“Š **Session monitoring** - track active work

---

## Documentation

**Getting Started:**
- [Complete Guide](docs/guides/COMPLETE_GUIDE.md) - Comprehensive reference (25,000 words)
- [Orchestrator Pattern](docs/orchestrator.md) - Parallel execution guide
- [Migration Guide](docs/guides/MIGRATION.md) - Upgrading from BMAD-TM Lite

**Integration:**
- [MCP Server Guide](scud-mcp/README.md) - Model Context Protocol integration
- [Quick Reference](docs/reference/QUICK_REFERENCE.md) - Command cheat sheet

**Advanced:**
- [Parallel Features](docs/features/PARALLEL_FEATURES.md) - Task locking & orchestration

**Development:**
- [Development Logs](log_docs/) - Implementation details & history

---

## Commands

### Setup
```bash
scud init                          # Initialize SCUD
scud hooks install                 # Enable automatic completion
scud hooks status                  # Check hook status
```

### Core Commands (Instant)
```bash
scud tags                          # List all tags
scud list [--tag <tag>]           # List tasks
scud next [--tag <tag>]           # Find next ready task
scud set-status <id> <status>      # Update task
scud stats [--tag <tag>]          # Show statistics
scud waves [--tag <tag>]          # Show parallel waves
```

### AI Commands (Requires XAI_API_KEY)
```bash
scud parse-prd <file> --tag <tag>  # Parse PRD into tasks
scud analyze-complexity             # Analyze all tasks
scud expand --all                   # Break down complex tasks
```

Default: `grok-code-fast-1` via xAI. Configure with `scud config --provider <provider> --model <model>`.

### Orchestrator Commands
```bash
scud claim <id> --name <name>      # Claim task (lock)
scud release <id>                  # Release task lock
scud whois [--tag <tag>]          # See who's working on what
scud doctor [--tag <tag>]         # Check for stale locks
```

---

## Example Workflow

```bash
# 1. Initialize
scud init
scud hooks install

# 2. Create tasks from PRD
scud parse-prd docs/feature.md --tag auth-system
# Creates tasks with dependencies

# 3. View execution plan
scud waves --tag auth-system
# Shows which tasks can run in parallel

# 4. Work on next ready task
scud next --tag auth-system
# Returns: Task 1 is ready

# 5. Implement (manual or via orchestrator)
# Manual: Work on task, then mark done
scud set-status 1 done

# Or with orchestrator: Start Claude session with task ID
SCUD_TASK_ID=1 claude "Implement task 1"
# Hook auto-marks complete when session ends

# 6. Repeat until done
scud stats --tag auth-system
# Shows progress: 8/10 complete
```

See [docs/orchestrator.md](docs/orchestrator.md) for parallel execution patterns.

---

## Why SCUD?

**DAG-Driven:**
- Tasks become ready when dependencies complete
- No manual phase management
- Visualize parallel execution waves
- Smart scheduling finds ready work

**Hook-Enforced:**
- Automatic task completion
- Prevents forgotten status updates
- Session-based tracking
- Solves 15% agent failure case

**Fast & Simple:**
- Rust CLI is instant
- SCG format is human-readable and git-friendly
- Works offline (core commands)
- No vendor lock-in

**Orchestrator-Ready:**
- Spawn parallel Claude agents
- Task locking prevents conflicts
- Monitor active sessions
- Doctor command finds stale work

---

## Requirements

- **Node.js 16+** (for pnpm/npm package wrapper)
- **xAI API key** (for AI features only; core commands work offline)

```bash
export XAI_API_KEY=xai-...
```

Alternative providers: Anthropic (`ANTHROPIC_API_KEY`), OpenAI (`OPENAI_API_KEY`), OpenRouter (`OPENROUTER_API_KEY`). Configure with `scud config`.

---

## File Structure

```
.scud/
â”œâ”€â”€ tasks/tasks.scg           # All tasks in SCG format (see spec)
â”œâ”€â”€ config.toml               # Active tag and settings
â””â”€â”€ current-task              # Active task ID (for hooks)

.claude/
â””â”€â”€ settings.local.json       # Claude Code hooks config

docs/
â”œâ”€â”€ prd/                      # Product requirements
â””â”€â”€ epics/                    # Feature descriptions
```

---

## Development

```bash
# Build Rust CLI
cd scud-cli
cargo build --release

# The binary will be at:
# scud-cli/target/release/scud
```

---

## Contributing

Issues and PRs welcome at [github.com/pyrex41/scud](https://github.com/pyrex41/scud)

---

## License

MIT

---

## Learn More

- **Complete Guide:** [docs/guides/COMPLETE_GUIDE.md](docs/guides/COMPLETE_GUIDE.md)
- **Orchestrator Pattern:** [docs/orchestrator.md](docs/orchestrator.md)
- **Quick Reference:** [docs/reference/QUICK_REFERENCE.md](docs/reference/QUICK_REFERENCE.md)
- **Parallel Features:** [docs/features/PARALLEL_FEATURES.md](docs/features/PARALLEL_FEATURES.md)
- **Implementation Logs:** [log_docs/](log_docs/)

**Happy building!**
</file>

<file path="RELEASE.md">
# Release Process

This document describes how to release a new version of scud-task.

## Prerequisites

1. Ensure all changes are committed and pushed
2. Update version in `package.json` and `scud-cli/Cargo.toml`
3. Update CHANGELOG.md (if exists)

## Release Steps

### 1. Create a Git Tag

```bash
# For version 1.0.0
git tag v1.0.0
git push origin v1.0.0
```

### 2. GitHub Actions Builds Binaries

When you push a tag starting with `v`, GitHub Actions will automatically:
- Build Rust binaries for all platforms:
  - macOS x64
  - macOS ARM64 (Apple Silicon)
  - Linux x64
  - Linux ARM64
  - Windows x64
- Create a GitHub Release
- Upload all binaries as release assets

**Check progress:** https://github.com/pyrex41/scud/actions

### 3. Publish to npm

Once the GitHub Release is created and binaries are uploaded:

```bash
npm login              # If not already logged in
npm publish
```

The postinstall script will automatically download the appropriate binary for each user's platform.

## Manual Release (if needed)

If you need to trigger a release without a tag:

1. Go to: https://github.com/pyrex41/scud/actions/workflows/release.yml
2. Click "Run workflow"
3. Select the branch
4. Click "Run workflow"

## Testing Before Release

Test the installation locally:

```bash
# Pack without publishing
npm pack

# Install locally in another directory
cd /tmp
npm install -g /path/to/scud-task-1.0.0.tgz

# Test commands
scud help
scud init
scud tags
```

## Troubleshooting

### Binary Download Fails

If users can't download binaries, they can build from source:
```bash
cd node_modules/scud-task/scud-cli
cargo build --release
```

### GitHub Actions Build Fails

Check the Actions tab: https://github.com/pyrex41/scud/actions

Common issues:
- Rust compilation errors (check scud-cli/src code)
- Cross-compilation tools not installed (check workflow file)
- Missing permissions (check GitHub repo settings)

## Platform Support

Currently supported platforms:
- macOS (Intel & Apple Silicon)
- Linux (x64 & ARM64)
- Windows (x64)

To add more platforms, update `.github/workflows/release.yml` matrix.
</file>

<file path="RELEASING.md">
# Releasing SCUD

SCUD has **two packages** that must be released together with matching versions:

1. **npm**: `scud-task` (JavaScript wrapper, slash commands, install script)
2. **crates.io**: `scud-cli` (Rust CLI binary)

## Version Files to Update

Both files must have the same version number:

```
package.json          â†’ "version": "X.Y.Z"
scud-cli/Cargo.toml   â†’ version = "X.Y.Z"
```

## Release Checklist

1. **Update versions** in both files:
   - `package.json` (npm)
   - `scud-cli/Cargo.toml` (cargo/crates.io)

2. **Commit the version bump**:
   ```bash
   git add package.json scud-cli/Cargo.toml
   git commit -m "chore: bump version to X.Y.Z"
   ```

3. **Create and push tag**:
   ```bash
   git tag vX.Y.Z
   git push origin master
   git push origin vX.Y.Z
   ```

4. **CI/CD automatically publishes**:
   - GitHub Actions `Release` workflow triggers on tag push
   - Publishes to npm (scud-task)
   - Publishes to crates.io (scud-cli)
   - Creates GitHub Release

5. **Verify both packages**:
   ```bash
   npm view scud-task version      # Should show X.Y.Z
   cargo search scud-cli           # Should show X.Y.Z
   ```

## Common Mistakes

- **Forgetting to bump `package.json`**: npm publish fails with E403 "cannot publish over previously published version"
- **Forgetting to bump `Cargo.toml`**: crates.io publish fails, or `scud --version` shows old version
- **Version mismatch**: Keep both files in sync to avoid confusion

## GitHub Secrets Required

The release workflow requires these secrets in GitHub repository settings:

- `NPM_TOKEN` - npm access token for publishing scud-task
- `CARGO_TOKEN` - crates.io API token for publishing scud-cli

## Installing After Release

```bash
# npm (installs JS wrapper + slash commands)
pnpm add -g scud-task@latest

# cargo (installs Rust CLI)
cargo install scud-cli

# Or rebuild locally
cd scud-cli && cargo install --path .
```
</file>

</files>
